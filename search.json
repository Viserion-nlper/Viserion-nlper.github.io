[{"title":"赠尔只铃铛 一步一响 一步一想","url":"/2025/03/26/叮叮铛铛/","content":"\n\n\n\n\n\n飞鸟之外有月  \n\n新月之外有夜  \n\n我走在楼宇中  \n\n不打梆 不喊号  \n\n只在腰间佩两个梦  \n\n一步叮叮 一步铛铛  \n\n一步一想\n\n<img src=\"https://img.picgo.net/2025/03/27/WechatIMG66d894dee518349c90.jpg\" height=750/>\n\n摄于2025年3月27日晚\n\n\n","categories":["live"]},{"title":"【大模型】基于知识图谱的检索KAG-声明式shema","url":"/2025/03/26/KAG-声明式shema/","content":"\n# 声明式 schema\n\n20250326\n\n> 最近在解决RAG的系列问题，考虑了基于图的检索，以下是目前的思路总结：\n>\n> KAG对每个知识图谱节点都做了向量化，这样的话，就可以根据embedding的方式去匹配相关的向量了。\n>\n> 原来dbgpt是通过keywords方式（这种方式太呆了）。\n>\n> **向量化解决了语义相似度匹配。 **\n>\n> **图谱化解决了知识结构规范化。**\n>\n> 问题还存留在如何高效的构建知识图谱，原则上可以用llm批量自动抽取，然后人工精调。\n>\n> 还可以llm在抽取的时候加入一些schema的先验条件，这样的话抽取的时候就可以限定，提升准确率。\n>\n> **schema构建解决了知识图谱的质量和广度。**\n>\n> 更多细节待后面编辑分享\n\n\n\n### 关键字\n\n```markdown\nnamespace\n\nEntityType, ConceptType, EventType, ->, STD.*, Text, Float, Integer\n\ndesc, constraint, value, properties, relations, rule, hypernymPredicatem, autoRelate, spreadable, regular\n\nNotNull, MultiValue, Enum, Regular\n```\n\n> \\-> 用于表达类型的继承关系，A -> B\n>\n> STD.\\*表示以STD.开头的都是预留关键字，作标准类型名称\n\n### 基础句法\n\n类似YAML，以缩进作为作用域的表示。缩进建议使用4个空格（Tab符会被当做两个空格）\n\n*   **A(B): C**\n\n*   A为类型/属性的英文名\n\n*   B为类型/属性的中文名称\n\n*   C为取值\n\n*   **A(B)->P**\n\n*   A为类型的英文名\n\n*   B为类型的中文名称\n\n*   P为要继承自的父类型\n\n*   **namespace A**\n\n*   A表示项目前缀，在Schema文件的第一行必须出现。项目前缀会在Schema提交的时候自动拼接到实体类型名称的前面\n\n*   **\\[\\[...]]**\n\n*   规则脚本的定界符，仅用于rule的定义，类似于Python的\"\"\"用法\n\n### 约束\n\n*   声明式Schema脚本采用逐行解析的方式，定义上要遵循顺序原则，即父类型要在子类型之前定义、属性上使用的类型也需要在其所属类型定义之前先定义好\n*   属性id、name、description为内置属性，不需要再声明\n*   属性类型只支持以下几种\n\n```bash\n1、基本类型：Text(文本)、Integer(整型)、Float(浮点数)\n2、标准类型：STD.ChinaMobile(国内手机号)、STD.Email(电子邮箱)、STD.IdCardNo(身份证)、STD.MacAddress(MAC地址)、STD.Date(日期)、STD.ChinaTelCode(国内通讯号)、STD.Timestamp(时间戳)\n```\n\n*   属性英文名称首字母必须为小写字母，且只支持英文字母和数字\n*   关系属性类型只支持基本类型\n\n### 语法结构\n\n总共分类6层缩进，按缩进的多少依次为：\n\n*   第一层（无缩进）：定义类型、命名空间\n*   第二层：定义类型的元信息，比如描述、属性、关系等\n*   第三层：定义属性/关系的名称和类型\n*   第四层：定义属性/关系的元信息，比如约束、子属性、逻辑规则等\n*   第五层：定义子属性的名称和类型\n*   第六层：定义子属性的元信息，比如描述、约束\n\n```json\nnamespace DEFAULT\n\nTypeA(实体类型A): EntityType\n    desc: 实体类型描述\n    properties:\n        property1(属性1): STD.ChinaMobile\n            desc: 属性1的描述\n                constraint: NotNull, MultiValue\n            properties:\n                 property2(属性1的子属性): Text\n                     desc: 属性1的子属性，枚举约束\n                     constraint: NotNull, Enum=\"A,B,C\"\n                 property3(属性1的子属性): Text\n                     desc: 属性1的子属性，正则约束\n                     constraint: Regular=\"^abc[0-9]+$\"\n                 property4(属性4): Text\n                      rule: [[\n                        Define property4...\n                       ]]\n    relations:\n        relation1(关系1): TypeA\n            desc: 关系1的描述\n            properties:\n                 confidence(置信度): Float\n            rule: [[\n               Define relation1...\n            ]]\n\nTypeB(实体类型B) -> TypeA:\n    desc: 这是实体类型A的子类型\n```\n\n#### 定义实体类型\n\n    # 以下定义一个公司的实体类型\n    Company(公司): EntityType\n\n    # 以下是定义一个继承自公司的实体类型\n    ListedCompany(上市公司) -> Company:\n\n##### 定义属性和关系\n\n    Company(公司): EntityType\n        # 这里是公司的描述\n        desc: 公司的描述\n        properties:\n            # 这里定义属性\n            address(地址): Text\n                # 这里定义地址属性为非空约束，除此还可以定义MultiValue(多值，英文逗号分割)、Enum(枚举)和Regular(正则)\n                constraint: NotNull\n            industry(行业): Industry\n            # 每个类型会默认创建id、name和description属性，都是Text类型\n            # id(主键): Text\n            # name(名称): Text\n            # description(描述): Text\n        relations:\n            # 这里定义关系\n            subCompany(子公司): Company\n\n##### 定义子属性\n\n    Company(公司): EntityType\n        desc: 公司的描述\n        properties:\n            address(地址): Text\n              # 这里定义地址的子属性置信度\n              confidence(置信度): Float\n            industry(行业): Industry\n\n##### 定义谓词逻辑\n\n    Company(公司): EntityType\n        desc: 公司的描述\n        relations:\n            risk(风险关联): Company\n                # 这里定义关系的谓词逻辑，使用 [[ 和 ]] 作为逻辑规则的定界符\n                rule: [[\n                   Define (s:Comapny)-[p:risk]->(o:Company) {\n                        ... ...\n                   }\n                ]]\n\n如果是定义从实体类型到同个概念类型下的不同概念实例的逻辑，请在同个rule关键字里写多段Define的脚本。\n\n#### 定义概念类型\n\n    Industry(公司行业分类): ConceptType\n        # 这里定义概念的上下位关系谓词，默认为isA，可以指定isA和locateAt\n        hypernymPredicate: isA\n\n概念类型的关系只允许创建在7大类里定义的谓词，这里可以通过autoRelate一键创建7大类的所有关系：\n\n    Industry(公司行业分类): ConceptType\n        autoRelate: Industry\n\n#### 定义事件类型\n\n    CompanyRiskEvent(公司风险事件): EventType\n        properties:\n            # 这里定义事件类型的主体为公司，事件类型必须定义主体subject\n            subject: Company\n\n### **建模示例**\n\n    namespace Medical\n\n    Symptom(症状): EntityType\n\n    Drug(药品): EntityType\n\n    Indicator(医学指征): EntityType\n\n    BodyPart(人体部位): ConceptType\n        hypernymPredicate: isA\n\n    HospitalDepartment(医院科室): ConceptType\n        hypernymPredicate: isA\n\n    Disease(疾病): EntityType\n        properties:\n            complication(并发症): Disease\n                constraint: MultiValue\n\n            commonSymptom(常见症状): Symptom\n                constraint: MultiValue\n\n            applicableDrug(适用药品): Drug\n                constraint: MultiValue\n\n            department(就诊科室): HospitalDepartment\n                constraint: MultiValue\n\n            diseaseSite(发病部位): BodyPart\n                constraint: MultiValue\n\n        relations:\n            abnormal(异常指征): Indicator\n                properties:\n                    range(指标范围): Text\n                    color(颜色): Text\n                    shape(性状): Text\n\n","tags":["信息抽取","知识图谱","KAG"],"categories":["AI"]},{"title":"農曆​乙巳蛇年二月十六 清淡平常与卿白首永偕","url":"/2025/03/15/订婚宴留念/","content":"\n## 和我最爱的宝贝 订婚宴纪念\n\n### 当二进制代码成为情诗\n\n### 协议栈里藏满永恒算法\n\n### ​理工青年终于牵定了文艺小姐的一辈子\n\n### 農曆​乙巳蛇年二月十六 清淡平常与卿白首永偕\n\n![](https://img.picgo.net/2025/03/16/0e4a6a7a9547962466d55b8db4c31c2e983dbcdb151cb6.jpg)\n\n![](https://img.picgo.net/2025/03/16/0bd26f7d91e2e63e33e4b56a50b5ab7658c68a16dbec45.jpg)\n\n![](https://img.picgo.net/2025/03/16/6b880c204c866ce7fc42e9b8a6e68bc97625f9185453d7.jpg)\n\n![](https://img.picgo.net/2025/03/16/67fa3c22d1c556e36d1565f68480602575a0f766c01a7e.jpg)\n\n![](https://img.picgo.net/2025/03/16/89fa3b1d10172d8ba6c8502e354f6d61ba072e9ada395a.jpg)\n\n![](https://img.picgo.net/2025/03/16/b455cd86ff9d4a94faf195ce0bfcc6debdf714a07ce77b.jpg)\n\n![](https://img.picgo.net/2025/03/16/ce53e4b1d090c5a86b42255ac7a5d1c9dafcc8b536eb19.jpg)\n\n![](https://img.picgo.net/2025/03/16/d6d72aeadf3a2d291869692dc031428a02dde6b69eb4df.jpg)\n\n[![](https://img.picgo.net/2025/03/16/e7bb5e97070e54c9c0d195f97aaa31c6481c4d5227beae.jpg)](https://www.picgo.net/image/e7bb5e97070e54c9c0d195f97aaa31.WpQSBA)","tags":["摄影记录生活"],"categories":["photograph"]},{"title":"为什么在道德经中全文都没努力这个概念？","url":"/2025/03/10/道德经中没有努力/","content":"\n## 任凭世事浮沉 我自静若幽兰\n\n\n在《道德经》里，你几乎看不到“努力”这个词，因为老子认为，太过用力的人很难走得长远。\n\n就如《道德经》第二十三章说的那样：“飘风不终朝，骤雨不终日。”意思就是，不管风多猛，也不会一直刮到天亮；不论雨多大，也不会下个没完。\n\n真正能持久的，是那种细水长流般的温和力量。\n\n老子觉得，做事情最好的状态就是像呼吸一样自然，顺其自然地去做。\n\n不需要刻意使劲，也不需要被外界干扰，心里平静专注，不去想前因后果，也不怕结果如何。\n\n这就是“无心生大用”——做一件事情最好的境界，就是像呼吸一样自然，顺其自然的把事情给做好，结果自然不会差，不着力，不受力，顺其自然心无旁骛。\n\n不问因果，不惧结果。","tags":["生活随笔"],"categories":["live"]},{"title":"窗外小雨","url":"/2025/03/10/窗外小雨/","content":"\n##窗外小雨\n\n窗外的小雨落在谁的肩膀\n\n随着陌生的人远去\n\n像我爱的你无声无息\n\n消失在风里\n\n人说感情的事不值一提\n\n可我的泪怎么绝了堤\n\n是不是只有年轻的人才会这样\n\n许多年后铁石心肠\n\n也许有天我会变成讨厌的模样\n\n也许有天我会把你遗忘\n\n可当我经过所有人生的风霜\n\n那些故事又该对谁讲\n\n人说感情的事不值一提\n\n可我的泪怎么绝了堤\n\n是不是只有年轻的人才会这样\n\n许多年后铁石心肠\n\n也许有天我会变成讨厌的模样\n\n也许有天我会把你遗忘\n\n可当我经过所有人生的风霜\n\n那些故事又该对谁讲\n\n也许有天我会变成讨厌的模样\n\n也许有天我会把你遗忘\n\n可当我经过所有人生的风霜\n\n那些故事又该对谁讲\n\n那些故事早已无人讲","tags":["生活随笔"],"categories":["live"]},{"title":"No Title","url":"/2025/03/09/hello-world/","content":"1111"},{"title":"【前后端开发】Jave+ES重构搜房网","url":"/2023/09/26/ES重构搜房网/","content":"\n\n\n#  ES重构搜房网\n\n\n\n\n\n## 一、es相关技术\n\n- 强力技术组合技 = ElasticSearch + MySQL + Kafka\n\n  <img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uvhjpa9oj215a09ygma.jpg\" alt=\"image-20220704142142250\" style=\"zoom: 33%;\" />\n\n- 强强联合 = ElasticSearch + 百度地图\n\n  <img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uvi7pdfoj20xu0fumxu.jpg\" alt=\"image-20220704142238638\" style=\"zoom: 33%;\" />\n\n- ElasticSearch生产环境优化经验\n\n- 负载加安全 = ElasticSearch + Nginx\n\n  <img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uvjoo77uj212s08e74r.jpg\" alt=\"image-20220704142403633\" style=\"zoom:33%;\" />\n\n- 传说中的数据分析神器 ELK = ElasticSearch + Logstash + Kibana\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uvkx2hywj211809u3yr.jpg\" alt=\"image-20220704142514879\" style=\"zoom:33%;\" />\n\n- 基础核心技术框架 = SpringBoot\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uvllo1qfj20jq0ay0t5.jpg\" alt=\"image-20220704142552030\" style=\"zoom:33%;\" />\n\n- 数据库的常青树 = MySQL + Spring Data JPA\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uvmr00llj20zs0ak74x.jpg\" alt=\"image-20220704142700158\" style=\"zoom:33%;\" />\n\n- 前端核心技术框架 = thymeleaf + Bootstrap + jQuery\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uvo2191mj215q0eawfk.jpg\" alt=\"image-20220704142815132\" style=\"zoom:33%;\" />\n\n- 项目安全框架 = Spring Security\n- 图片上传 = 七牛云 + webUpload\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uvp9u8vhj20pm08iweq.jpg\" alt=\"image-20220704142925971\" style=\"zoom:33%;\" />\n\n- 免注册登陆 = 阿里短信\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uvprdcqoj20r40f00tz.jpg\" alt=\"image-20220704142953479\" style=\"zoom:33%;\" />\n\n\n\n\n\n### 1、系统架构\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uvqrifj8j20zo0lyae7.jpg\" alt=\"image-20220704143051510\" style=\"zoom: 50%;\" />\n\n### 2、课程收获\n\n- 了解一个中度复杂规模的应用开发流程\n- 掌握ElasticSearch的高级业务应用\n- 熟悉ES与其他技术框架的应用结合思路与技巧\n- 掌握ES的相关优化技巧及扩展应用\n- 熟悉完整的搜房网业务，提升技术应用能力\n\n\n\n## 二、技术选型介绍\n\n### 1、数据库技术选型介绍\n\n​\t**MySQL**：MySQL是当前最流行的关系型数据库，在互联网公司MySQL也是应用最多的关系型数据库。\n\n​\t**ElasticSearch**：ElasticSearch是基于Apache Lucene的开源搜索引擎。\n\n​\t**ElasticSearch Vs MySQL**：\n\n \t\t\t利用ES方便实现站内搜索引擎\n\n​\t\t\t 利用MySQL的事务特性做稳定的数据存储\n\n​\t\t\t 以MySQL做基础数据存储，结合ES实现站内搜索引擎\n\n\n\n## 三、需求分析\n\n1. ### 项目背景\n\n2. ### 目标用户\n\n3. ### 项目可行性\n\n## 四、数据库设计\n\n> ER图\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uwayv4qtj20zc0k0jsi.jpg\" alt=\"image-20220704145015972\" style=\"zoom: 50%;\" />\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3uwb97ci1j211m0m6ac3.jpg\" alt=\"image-20220704145033327\" style=\"zoom: 50%;\" />\n\n### 1、基础表介绍\n\n![image-20220729182048729](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h4nyvqn9ekj21e00owq8f.jpg)\n\n> 其中关于两个表之前的关联，最好使用逻辑上的外键连接，少用数据库中的外键连接，在分表分库的时候会依赖数据库的一些操作，有一定的依赖性。慎用数据库特性，分表的时候将会是一个灾难。\n\n### 2、表结构设计原则\n\n> ​\t减少中间表的设计\n>\n> ​\t保证表表之间没有耦合\n\n\n\n## 五、环境要求\n\nJava环境： JDK1.8\n\n构建工具： Maven\n\n常用IDE：（IDEA、Eclipse等）\n\n\n\n\n\n## 六、后端框架搭建\n\n- SpringBoot\n- Spring Data JPA + Hibernate\n\n\n\n### 1、新建config工具包，创建JpaConfig类\n\n![image-20220729183149365](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h4nz76jcydj20ia03rmxc.jpg)\n\n加入注解@Configuration。\n\n加入@EnableJpaRepositories让其可以扫描到我们的repository Dao类。\n\n加入@EnableTransactionManagement 允许事务管理。\n\n新建\n\n```java\n @Bean\n    /*建立数据源，并设定数据源配置的前缀，需要用到数据源mysql的用户名密码等，在properties文件中增加*/\n    @ConfigurationProperties(prefix = \"spring.datasource\")\n    public DataSource dataSource(){\n        return DataSourceBuilder.create().build();\n    }\n```\n\n在application.properties文件中增加mysql信息：\n\n```xml\nspring.datasource.driver-class-name=com.mysql.jdbc.Driver\nspring.datasource.url=jdbc:mysql://localhost:3306/xunwu?useSSL=false&allowPublicKeyRetrieval=true\nspring.datasource.username=root\nspring.datasource.password=123456789\n```\n\n\n\n另外设置实体类的管理工厂 LocalContainerEntityManagerFactoryBean，实例化Hibernate，因为jpa实现的是hibernate，所以要选择HibernateJpaVendorAdapter，然后设置jpaVendor.setGenerateDdl为false，设置其不会自动生成sql，因为要把sql权掌握在自己的手里。\n\n再=实例化实体映射管理工厂类LocalContainerEntityManagerFactoryBean。对该工厂类设置一些属性，setDataSource，setJpaVendorAdapter（jpaVendorAdapter），setPackagesToScan（实体类的包名），最后返回新建的实体类映射管理工厂bean。\n\n\n\n```java\n   @Bean\n    public LocalContainerEntityManagerFactoryBean entityManagerFactory() {\n        HibernateJpaVendorAdapter japVendor = new HibernateJpaVendorAdapter();\n        japVendor.setGenerateDdl(false);\n\n        LocalContainerEntityManagerFactoryBean entityManagerFactory = new LocalContainerEntityManagerFactoryBean();\n        entityManagerFactory.setDataSource(dataSource());\n        entityManagerFactory.setJpaVendorAdapter(japVendor);\n        entityManagerFactory.setPackagesToScan(\"com.zryy.soufangtest.entity\");\n        return entityManagerFactory;\n```\n\n再去新建一个com.zryy.soufangtest.entity实体类包。\n\n在JPAConfig类下我们添加了一个事务管理的注解，所以我们需要新建一个事务管理的类PlatformTransactionManager。传的参数是实体映射管理工厂EntityManagerFactory，在内部中实例化一下事务管理类JpaTransactionManager，把实体映射管理工厂当参数传进去。最后返回事物管理类TransactionManager。\n\n```xml\n  @Bean\n    public PlatformTransactionManager transactionManager(EntityManagerFactory entityManagerFactory) {\n        JpaTransactionManager transactionManager = new JpaTransactionManager();\n        transactionManager.setEntityManagerFactory(entityManagerFactory);\n        return transactionManager;\n    }\n```\n\n到这里我们的Jpa配置就设置完成了。\n\n```xml\n/**\n * Created by zhiqiang\n */\n/*允许事务管理*/\n@Configuration\n@EnableJpaRepositories(basePackages = \"com.zryy.soufangtest.repository\")\n@EnableTransactionManagement\npublic class JPAConfig {\n    @Bean\n    /*建立数据源，并设定数据源配置的前缀，需要用到数据源mysql的用户名密码等，在properties文件中增加*/\n    @ConfigurationProperties(prefix = \"spring.datasource\")\n    public DataSource dataSource(){\n        return DataSourceBuilder.create().build();\n    }\n\n\n    @Bean\n    public LocalContainerEntityManagerFactoryBean entityManagerFactory() {\n        HibernateJpaVendorAdapter japVendor = new HibernateJpaVendorAdapter();\n        japVendor.setGenerateDdl(false);\n\n        LocalContainerEntityManagerFactoryBean entityManagerFactory = new LocalContainerEntityManagerFactoryBean();\n        entityManagerFactory.setDataSource(dataSource());\n        entityManagerFactory.setJpaVendorAdapter(japVendor);\n        entityManagerFactory.setPackagesToScan(\"com.zryy.soufangtest.entity\");\n        return entityManagerFactory;\n    }\n\n    @Bean\n    public PlatformTransactionManager transactionManager(EntityManagerFactory entityManagerFactory) {\n        JpaTransactionManager transactionManager = new JpaTransactionManager();\n        transactionManager.setEntityManagerFactory(entityManagerFactory);\n        return transactionManager;\n    }\n}\n\n```\n\n另外在配置文件中我们还要增加两条：\n\n```properties\nspring.jpa.show-sql=true     #方便我们在开发过程中看到hibernate给我们建立的sql语句。\nspring.jpa.hibernate.ddl-auto = validate\n```\n\n正常情况下我们的日志级别是info，但是我们的日志打印级别是debug级别，把sql级别打印调为debug这样才可以正常输出。\n\n```properties\nlogging.level.org.hibernate.SQL = debug\n```\n\n运行测试mvn，如果提示“No Spring Session store is configured: Set the spring.session.store-type property”，则将设置spring会话存储的类型\n\n```properties\nspring.session.store-type = hash_map\t\n```\n\n如果打开localhost:8080后有验证，则可以设置\n\n```properties\nsecurity.basic.enabled=false\n```\n\n测试在main函数中增加一个@RestController \n\n\n\n## 七、集成单元测试\n\n### 1、在spring中使用JUnit来进行测试。\n\n### 2、如何利用H2内存数据库解耦mysql来进行测试。\n\n\n\n> 依赖\n\n```xml\n <!-- jpa相关依赖 -->\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-data-jpa</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>com.h2database</groupId>\n            <artifactId>h2</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <scope>runtime</scope>\n        </dependency>\n <!-- 测试依赖 -->\n  <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n```\n\n建数据表，存入sql数据。\n\n或者新建entity包，新建实体类。\n\n```java\n@Entity\n@Table(name = \"user\")\npublic class User implements UserDetails {\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n\n    private String name;\n\n    private String password;\n\n    private String email;\n\n    @Column(name = \"phone_number\")\n    private String phoneNumber;\n```\n\n其中因为要兼顾hibernate和h2所以主键的自增方式不能写为GenerationType.auto，修改为Identity\n\n在实体类中定义的变量参数一般都是驼峰的，而在数据库中一般定义的是下划线的，所以添加column注解。给他的orm-Mapping做一个映射。另外我们定义的实体类首字母是大写的，但是在sql中表是小写的，所以也要添加@Table（name =“ ”）注解。\n\n\n\n再定义userRepository 也就是jpa操作类，是一个接口，并且实现Crud Repository<xx,xx>。  第一个是我们自己定义的实体类，第二个参数。\n\n我们在单元测试中写一个单独的测试类去继承系统给的测试类\n\n```\npublic class UserRepositoryTest extends SoufangTestApplicationTests {\n    @Autowired\n    private UserRepository userRepository;\n\n    @Test\n    public void testFindOne() {\n        User user = userRepository.findOne(1L);\n        Assert.assertEquals(\"wali\",  user.getName());\n    }\n}\n```\n\n在实际开发过程中，会脱离mysq进行测试，而使用h2内存数据库。\n\n 做一下配置分离。新建：application-test.properties  /   application-dev-properties\n\n然后在在application.properties文件中添加spring.profiles.active = dev 进行激活。\n\n可以把一些通用的配置放在application.properties中，一些个性化的放在单独的文件中。\n\n在-test配置文件中加入\n\n```properties\nspring.datasource.driver-class-name=org.h2.Driver\t\n```\n\n如果上面步骤直接执行的话，并不会走测试配置，需要配置一个注解@ActiveProfiles(\"test\")， /*加这个注解就会走application-test.properties*/\n\n在resources文件夹下新建db文件夹，里面包含两个文件，一个是结构体，另外一个是表数据文件。\n\n然后在-test.properties中指定目录：\n\n```properties\nspring.datasource.schema=classpath:db/schema.sql\nspring.datasource.data=classpath:db/data.sql\n```\n\n再运行测试文件就ok通过了。\n\n## 八、前端集成\n\n### 1、集成thymeleaf及基本用法\n\n添加依赖：\n\n```xml\n <!-- 前端模板 thymeleaf 依赖 -->\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-thymeleaf</artifactId>\n        </dependency>\n\n        <!-- https://mvnrepository.com/artifact/org.thymeleaf/thymeleaf -->\n        <dependency>\n            <groupId>org.thymeleaf</groupId>\n            <artifactId>thymeleaf</artifactId>\n            <version>${thymeleaf.version}</version>\n        </dependency>\n\n        <dependency>\n            <groupId>org.thymeleaf</groupId>\n            <artifactId>thymeleaf-spring4</artifactId>\n            <version>3.0.2.RELEASE</version>\n        </dependency>\n```\n\n注意thymeleaf的版本做个一个覆盖，在properties标签下进行对springboot所依赖的版本进行了覆盖：\n\n```xml\n  <!-- thymeleaf 覆盖parent 选择自己的版本 -->\n        <thymeleaf.version>3.0.3.RELEASE</thymeleaf.version>\n        <thymeleaf-layout-dialect.version>2.1.1</thymeleaf-layout-dialect.version>\n        <thymeleaf-extras-springsecurity4.version>3.0.2.RELEASE</thymeleaf-extras-springsecurity4.version>\n```\n\n并进行相关的配置：\n\n新建WebMvcConfig类 ，并且对其继承WebMvcConfigurerAdapter类，并实现ApplicationContextAware，实现的该接口可以帮助我们获取spring的一个上下文。\n\n先私有化一个类，把这个对象持久化一下。\n\n```java\nprivate ApplicationContext applicationContext\n```\n\n然后在覆盖的set方法里面，把这个变量给set进去，赋值。\n\n\n\n然后对模板资源进行解析**（模版资源解析器）**\n\n```java\n@Bean\npublic SpringResourceTemplateResolver templateResolver(){\n\t#新建一个\n\tSpringResourceTemplateResolver templateRosolver = new SpringResourceTemplateResolver();\n\t#然后设置spring的application上下文\n\ttemplateRolver.setApplicationContext(this.applicationContext);\n\treturn templateRolver;\n}\n```\n\n另外一个呢，还有一个thymeleaf**标准方言解释器。**\n\n```java\n@Bean\npublic SpringTemplateEngine templateEngine(){\n\tSpringTemplateEngine templateEngine = new SpringTemplateEngine();\n\ttemplateEngine.setTemplateResolver(templateResolver)  //上面刚设置的模版资源解析器\n\t//并且设置支持spring EL表达式\n\ttemplateEngine.setEnableSpringELCompiler(true);\n\t\n\t\n\treturn templateEngine；\n}\n```\n\n  在上面还定义springSecurity方言，这里不做设置，在下文中会进行介绍。\n\n除此之外还有一个**视图解析器。**\n\n```java\n@Bean\npublic ThymeleafViewResolver viewResolver(templateEngine){\n\tThymeleafViewResolver viewResolver = new ThymeleafViewResolver();\n  viewResolver.setTemplateEngine(templateEngine());\n  return vierResolver;\n}\n```\n\n到目前为止thymeleaf的搜索引擎基本搭建完毕。\n\n\n\n另外呢还需要在配置文件中进行一些其他的配置，因为模版内容是经常进行变化的，thymeleaf在运行中默认是开启缓存的，会使得修改的内容不能够得到及时的反馈。可以设置其不能进行缓存。\n\n```properties\nspring.thymeleaf.cache = false\n```\n\n另外还有一个通用的配置：\n\n```properties\nspring.thymeleaf.mode =HTML\n```\n\nthymeleaf默认的是HTML5，不过现在已经废弃了，这个是必备的，用HTML。\n\n\n\n另外还可以在配置文件中配置thymeleaf模版的前缀、后缀。\n\n```properties\nspring.thymeleaf.suffix = .html. \nspring.thymeleaf.prefix=classpath:/templates/\n```\n\n\n\n> 小技巧：可以通过命令行mvn spring-boot:run方式进行启动\n>\n> 当然也可以通过IDEA的快捷方式来启动：\n>\n> Command line：  clean  package spring-boot:run -Dmaven.test.skip=true   \n>\n> //-Dmaven.test.skip=true 是跳过测试类\n\n有时候启动程序先加载再编译时间速度非常缓慢。可以使用SpringBoot自带热加载开发工具：\n\n```xml\n <!-- SpringBoot自带热加载开发工具 -->\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n            <scope>runtime</scope>\n        </dependency>\n```\n\n还要在IDEA中修改一些配置才可以生效，注意设置好之后需要重启IDEA才可以生效，这里不做介绍更多自行查阅。\n\n如果提示：\n\n```java\nCould not open ServletContext resource [/index]\n```\n\n那么就在模版资源解析器中添加配置注解：\n\n```java\n@ConfigurationProperties(prefix = \"spring.thymeleaf\")\n```\n\n这样就可以了。\n\n\n\n新建一个controller添加测试：\n\n```java\n @GetMapping(\"/index2\")\n    public String index2(Model model){\n        model.addAttribute(\"name\",\"zhiqiang\");\n        return \"index2\";\n    }\n```\n\n并且新建一个html页面：\n\n```html\n<!DOCTYPE html>\n<html xmlns:th=\"http://www.thymeleaf.org\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Title</title>\n</head>\n<body>\n<h1>Hello ,zhiqiang</h1>\n\n<span th:text=\"${name}\"></span>\n</body>\n</html>\n```\n\n![image-20220802111018198](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h4s8x24xf9j20ay061dfv.jpg)\n\n\n\n另外，加载一些静态资源：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h4s8xqywzoj20l80qkmyc.jpg\" alt=\"image-20220802111059762\" style=\"zoom: 50%;\" />\n\n按照该目录结构进行部署，并且在WebMvcConfig中需要单独进行配置：\n\n```java\n/**\n * 静态资源加载配置\n */\n@Override\npublic void addResourceHandlers(ResourceHandlerRegistry registry) {\n    registry.addResourceHandler(\"/static/**\").addResourceLocations(\"classpath:/static/\");\n\n}\n```\n\n增加处理路径和静态资源绝对路径。意味着我们只要在静态资源里面加入/static/**前缀就会到classpath：/static/路径下找到相关的静态资源文件。\n\n### 2、集成Bootstrap\n\n### 3、集成jQuery\n\n\n\n## 九、架构设计与分层\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h4sf3g2rvpj21480u044k.jpg\" alt=\"image-20220802144402131\" style=\"zoom:50%;\" />\n\n### 结构分层\n\n经典的三层架构：\n\n​\t表示层 ——web\n\n​\t业务逻辑层——service\n\n​\t数据层——entity、repository\n\n## 十、API结构设计\n\n### RestfulApi的结构设计\n\n### 自定义API返回的标准\n\n- code 自定义请求状态编码\n- message自定义请求响应信息描述\n- data请求目标数据\n\n新建一个base文件包，存储一些基础的结构体，比如：ApiResponse\n\n另外我们在class ApiResponse定义了一个内部允许类 Status\n\n```java\npublic class ApiResponse{\n\tprivate int code;\n\tprivate String message;\n\tprivate Object data;\n\tprivate boolean more;\n\t\n\tpublic enum Status{\n\t\tSUCESS(200,\"ok\"),\n\t\tBAD_REQUEST(400, \"Bad Request\"),\n    INTERNAL_SERVER_ERROR(500,\"Unknown Internal Error\" ),\n    NOT_VALID_PARAM(40006,\"Operation not supported\"),\n    NOT_LOGIN(50000,\"Not Login\");\n    \n    private int code;\n    private String standardMessage;\n    //建立构造器\n    Status(int code, String standardMessage) {\n      this.code = code;\n      this.standardMessage = standardMessage;\n    }\n    //设置get set方法\n\t}\n}\n```\n\n\n\n\n\n并且可以创建一个构造器，而有的api只是想确认一下状态，这时候可以创建一个空的构造器\n\n```java\npublic ApiResponse(int code, String message, Object data) {\n        this.code = code;\n        this.message = message;\n        this.data = data;\n    }\n\n    public ApiResponse() {\n        this.code = Status.SUCCESS.getCode();\n        this.message = Status.SUCCESS.getStandardMessage();\n    }\n```\n\n以及设计三个静态方法类：\n\n```java\npublic static ApiResponse ofMessage(int code ,String message){\n        return new ApiResponse(code,message,null);\n    }\n\npublic static ApiResponse ofSuccess(Object data){\n\treturn new ApiResponse(Status.SUCCESS.getCode(), Status.SUCCESS.getStandardMessage(), data);\n}\n\npublic  static ApiResponse ofStatus(Status status){\n\treturn new ApiResponse(status.getCode(),status.getStandardMessage(),null    );\n}\n```\n\n测试：\n\n```java\n @GetMapping(\"/get\")\n    @ResponseBody\n    public ApiResponse apiget(){\n        return ApiResponse.ofMessage(200,\"成功了\");\n    }\n```\n\n返回结果如下：\n\n![image-20220802172936208](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h4sjvoqsvdj20a107974i.jpg)\n\n\n\n## 十一、异常拦截器\n\n### 1、首先，为什么要添加异常拦截器？\n\n会有很多我们想象不到的情况，比如说用户页面访问接口异常，用户访问不存在的页面 ，用户的权限不如等等。\n\n### 2、我们主要实现两个异常拦截器：\n\n#### \t1、页面异常拦截器    404 403 500页面等。\n\n#### \t2、API异常拦截器\t 同样是拦截404 403 500等情况。 \n\n\n\n比如我们输入localhost:8000/xxx就会弹出 Whitelabel Error Page页面，这个是springboot自带的一个页面。\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h4tggx3534j214c0b4aaw.jpg\" alt=\"image-20220803121706517\" style=\"zoom:50%;\" />\n\n在配置文件中修改并进行优化：\n\n```properties\nserver.error.whitelabel.enabled=false\n```\n\n\n\n并且在base包中新建一个AppErrorController类 继承ErrorController类。\n\n设置一个静态变量ERROR_PATH = \"/error\"\n\n并且覆盖类 getErrorPath（）方法类，return ERROR_PATH。\n\n并且将errorAttributes新建个内部变量存储起来。\n\n```java\n@Autowired\npublic AppErrorController(ErrorAttributes errorAttributes){\n    this.errorAttributes = errorAttributes;\n}\n```\n\n并且新建一个Web页面错误处理类errorhandler\n\n上面添加注解@RequestMapping（value 接收 定义的Error_path， 并且produces= “text/html“) \n\nWeb页面错误处理类errorhandler接收的参数是HttpServerRequest 和HttpServerletResponse\n\n{\n\n​\t获取到response的状态\n\n​\tint status = response.getStatus()\n​\t然后switch进行判断\n\n​\t并return ”404“到该页面，等等\n\n​\t如果什么都没匹配到，就返回index页面。\n\n}\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h4tlyyvi5yj21mp0u0acc.jpg\" alt=\"image-20220803152731553\" style=\"zoom: 33%;\" />\n\n因为在方法上面使用的是requestMapping，所以也要在类上面增加@controller注解。\n\n\n\n\n\n以上定义的是web页面的拦截处理，下面定义api的拦截处理：\n\n除web页面外的错误处理，比如Json/xml等，那么就不需要像web页面拦截器一样声明produces = \"xxx“了，因为是除了web页面以外的信息。\n\n```java\n@RequestMapping(value = ERROR_PATH)\n//需要返回的是结构体，所以定义@ResponseBody\n@ResponseBody\n//只需要传入一个request参数即可 参数：HttpServletRequest request\npublic ApiResponse errorApiHandler(HttpServletRequest request){\n  然后需要定义一个RequestAttributes实例，把类参数传入进去。\n\tRequestAttributes requestAttributes = new ServletRequestAttributes(request)\n    需要获取到请求返回的状态\n    Map<String,Object> attr = this.errorAttributes.getErrorAttributes(requestAttributes, 第二个参数是includeStackTrance，取值为false)\n    使用一个map来接收。\n    想要获取到状态需要单独写一个函数getStatus（）\n    private int getStatus(HttpServletRequest request){\n      Integer status = (Integer) request.getAttribute(\"javax.servlet.error.status_code\");\n      if(status != null){\n        return status;\n      }\n\n      return 500;\n  \t}\n  \n  \t然后返回函数中，使用\n    int status = getStatus(request);获取到状态。\n    获取到状态后，直接使用定义好的ApiResponse.ofMessage(status, 并且在返回信息这里使用getOrDefault方法，\t\t\tString.valueOf(attr.getOrDefault(\"message\",\"error\")))\n}\n\n```\n\n> 如果正确请求的话，就会直接进入到controller中对应RequestMapping下的函数中去。\n>\n> 如果错误请求的话，内部会检测到匹配不到的错误请求path，然后在AppErrorController中赋值给ERROR_PATH，然后通过this.errorAttributes.getErrorAttributes(requestAttributes,false);去获取到javax.servlet.error.status_code对应的状态码，以及对应的error_message。最后包装在我们定义好的ApiResponse.ofMessage中去。\n\n\n\n## 十二、功能性页面开发\n\n### 403:权限限制性页面\n\n### 404:Not found 提示页面\n\n### 500:异常服务提示页面\n\n\n\n> 另外，springdevTools是在整个项目进行监听的，如果在开发前端的过程中引发一些热加载其实是没有必要的，因为我们设置了thymeleaf的缓存cache为false，只需要一行配置即可生效，\n>\n> spirng.devtools.restart.exclude=templates/** ,static/**\n\n这样对静态资源的修改就不会引发热加载了\n\n\n\n当thymeleaf出现乱码的情况下，只需要在templateResolver中进行设置就可以。\n\n```java\ntemplateResolver.setCharacterEncoding(\"UTF-8\")\n```\n\n这样就解决了thymeleaf前端乱码的问题了。\n\n\n\n## 十三、后台管理模块\n\n### 业务与功能分析：\n\n​\t为了方便网站运营人员**管理租房网站**的房源信息，就需要有后台管理系统。开发一个后台管理模块来管理租房网站的数据、人员信息等等\n\n\n\n首先在webcontroller下面新建一个admin的文件夹，并且新建一个AdminController控制类，新建两个getMapping方法。\n\n```java\n@GetMapping(\"/admin/center\")\npublic String adminCenterPage(){\n    return \"admin/center\";\n}\n\n@GetMapping(\"/admin/welcome\")\npublic String welcomePage(){\n    return \"admin/welcome\";\n}\n```\n\n并且把admin/center页面添加进去。\n\n## 十四、后台登录功能模块实现\n\n在后台管理页面上有一个展示目前登录账户名称，就需要一个登录管理模块，那么就需要查询数据库，就需要用到hibernate，所以需要在WebMvcConfig中SpringTemplateEngine（tyhmeleaf标准方言解释器）中增加支持springSecurity方言。\n\n```java\n // 支持SpringSecurity方言\n        SpringSecurityDialect securityDialect = new SpringSecurityDialect();\n        templateEngine.addDialect(securityDialect);\n```\n\n\n\n并且在pom.xml中加入web SpringSecurity依赖和thymeleafSecurity依赖，并且制定thymeleafsecurity依赖的固定版本3.0.2 。\n\n```xml\n<!-- SpringSecurity 依赖 -->\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-web</artifactId>\n</dependency>\n\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-security</artifactId>\n</dependency>\n\n <!-- Thymeleaf方言支持SpringSecurity 依赖-->\n<dependency>\n  <groupId>org.thymeleaf.extras</groupId>\n  <artifactId>thymeleaf-extras-springsecurity4</artifactId>\n  <version>${thymeleaf-extras-springsecurity4.version}</version>\n</dependency>\n\n<properties>\n\t<thymeleaf-extras-springsecurity4.version>3.0.2.RELEASE</thymeleaf-extras-springsecurity4.version>\n</properties>\n```\n\n并且新建一个类，WebSecurityConfig 继承WebSecurityConfigurerAdapter\n\n并且在类上面添加两个注解：\n\n@EnableWebSecurity\n\n@EnableGlobalMethodSecurity\n\n再然后复写一下继承类的方法：configure（接收的参数是HttpSecurity http）的。\n\n里面就是http权限控制的内容了，比如设置页面的权限、api的角色权限等等。\n\n 即增加：\n\nhttp.authorizeRequests().antMatchers(\"/admin/login\").permitAll();\n\n\n\n```java\n@EnableWebSecurity\n@EnableGlobalMethodSecurity\npublic class WebSecurityConfig extends WebSecurityConfigurerAdapter {\n    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n\n        //资源访问权限\n        http.authorizeRequests()\n                .antMatchers(\"/admin/login\").permitAll()  //管理员登录入口\n                .antMatchers(\"/static/**\").permitAll()      //静态资源访问\n                .antMatchers(\"/user/login\").permitAll()     //用户登录入口\n                .antMatchers(\"/admin/**\").hasRole(\"ADMIN\")\n                .antMatchers(\"/user/**\").hasAnyRole(\"ADMIN\",\"USER\")\n                .antMatchers(\"/admin/user/**\").hasAnyRole(\"ADMIN\",\"USER\")\n                .and()\n                .formLogin()\n                .loginProcessingUrl(\"/login\")\n                .defaultSuccessUrl(\"/admin/center\",true).permitAll()\n                .failureHandler(authFailHandler())\n                .and()\n                .logout()\n                .logoutUrl(\"/logout\")\n                .logoutSuccessUrl(\"/logout/page\")\n                .deleteCookies(\"JSESSIONID\")\n                .invalidateHttpSession(true)\n                .and()\n                .exceptionHandling()\n                .authenticationEntryPoint(urlEntryPoint())\n                //无权访问的跳转页面\n                .accessDeniedPage(\"/403\");\n\n\n        http.csrf().disable();//常用的防御策略 关闭\n        http.headers().frameOptions().sameOrigin(); //常用的默认开启同源策略\n    }\n```\n\n在我们设置了loginProcessingUrl(\"/login\")后，springSecurity默认的登录界面是这样的：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h4uk1qiza0j20ls09o0sz.jpg\" alt=\"image-20220804110630147\" style=\"zoom:50%;\" />\n\n所以我们需要在controller中去配置/admin/login函数并且return /admin/login\n\n另外我们还需要去关闭两个设置：\n\nhttp.csrf().disable();   //csrf是一个防御策略，为了方便开发我们这里关闭\n\nhttp.headers().frameOptions().sameOrigin()。 h-ui是使用iframe开发的，所以我们要设置同源策略。\n\n\n\n\n\n并在还需要另外设置自定义认证策略：\n\n```java\n//自定义认证策略\n @Autowired\n    public void configGlobal(AuthenticationManagerBuilder auth){\n      //在最开始的时候可以定义一个基于内存验证的用户名和密码。//以and结尾。\n      auth.inMemoryAuthentication().withUser(\"admin\").password(\"admin\").roles(\"ADMIN\").and();\n    }\n```\n\n测试admin/login，登录成功之后默认跳转到首页。\n\n但是呢我们的认证数据是从内存中定义的，这里我们需要从数据库中进行替换认证的数据及逻辑。\n\n\n\n\n\n另外呢需要单独新建一个security的包，负责存放一些关于安全认证的代码。\n\n然后在securiyty包里AuthProvider（自定义认证实现）实现AuthenticationProvider，然后实现两个覆盖类：authenticate和supports。\n\n其中supports我们设置默认返回true，支持所有的权限认证，在authenticate中去实现我们对用户的一些详细的认证。\n\n在athenticate方法中authentication.getName()   getCredentials()   \t去获取用户名和输入的密码。\n\n\n\n这时候需要新建一个service包用来存放接口 IUserService，新建一个findUserByName方法，\n\n然后新建一个实现类UserServiceImpl 实现IUserService，然后实现findUserByName方法，并且新建一个UserRepository来查询数据库。\n\n但是UserRepository虽然继承了CrudRepository但是并没有findByName，需要在UserRepository中去定义。\n\n```java\npublic interface UserRepository extends CrudRepository<User,Long>{\n\n    User findByName(String username);\n}\n```\n\n然后这时候才可以在UserServiceImpl中使用repository的findByName()\n\n这样的话我们就可以在AuthProvider中使用UserService了，需要@autowired，注入之后就可以使用其从数据库中查出我们的用户名和密码来了。\n\n\n\n获取到用户名后，**需要添加一个if判断，如果为null的话直接throw一个**\tAuthenticationCredentialsNotFoundException(\"authError\")错误。\n\n然后就可以验证inputPassword和数据库里的密码做比对了/。\n\n```java\nuser.getPassword().equals(inputPassword)\n```\n\n但是这样的话就容易在逻辑代码中暴露用户的密码了，所以要使用md5加密。\n\n```java\nprivate final Md5PasswordEncoder passwordEncoder = new Md5PasswordEncoder()\n```\n\n```java\n//使用md5来对password进行隐藏式的验证\n\nif(this.passwordEncoder.isPasswordValid(user.getPassword(),inputPassword,user.getId())){\n            return new UsernamePasswordAuthenticationToken(user, null, user.getAuthorities());\n        }\n\n//if判断条件中输入参数1用户数据库密码，获取到的密码，加盐（这里使用userid进行加盐）\n//然后如果通过了就return 一个UsernamePasswordAuthenticationToken（name）这里参入的参数user是有要求的，需要User类去实现UserDetails方法，然后把方法都实现掉。而且return都返回一个true。\n\n @Transient\n    private List<GrantedAuthority> authorityList;\n            @Override\n                public boolean isAccountNonExpired() {\n                    return true;\n                }\n\n                @Override\n                public boolean isAccountNonLocked() {\n                    return true;\n                }\n\n                @Override\n                public boolean isCredentialsNonExpired() {\n                    return true;\n                }\n\n                @Override\n                public boolean isEnabled() {\n                    return true;\n                }\n//并且给User添加一个新的属性：/*jpa会验证这个字段，但是在mysql中并没有这个字段，这个是在security中进行单独设置的，添加这个注解让其透明*/\n//另外一个\n//\n@Override\n    public Collection<? extends GrantedAuthority> getAuthorities() {\n        return this.authorityList;\n    }\n//虽然我们在getAuthorities中return了这个authrityList，但是并没有对这个List进行包装。\n//所以需要我们新建role 实体类。以及roleRepository。\n//并且在UserServiceImpl中通过roleRepository去查询相关的role角色，并且新建一个ArrayList，存放在这个ArrayList中，把这个ArrayList存放在user属性中，最后返回出来，这样我们定义的authorityList属性也填充完毕了。\n//特别注意的是这个字段要加一个@Transient注解\n//@Transient\n    //private List<GrantedAuthority> authorityList;\n\nthrow new BadCredentialsException(\"authError\");\n```\n\n并且在WebSecurityConfig类的configGlobal()方法中我们之前定义的是在内存中设定一个用户admin/admin\n\n现在因为我们自己设定了一个AuthProvider类，所以呢我们就先建立一个@bean\n\n```java\n@Bean\n    public AuthProvider authProvider() {\n        return new AuthProvider();\n    }\n```\n\n然后在configGlobal中auth.authenticationProvider(authProvider()).eraseCredentials(true);  //并且设置擦除密码验证。\n\n\n\n## 十五、验证失败逻辑处理\n\nauthProvider实现了自定义登录功能。下面实现基于springSecurity的权限控制。\n\n比如说我们要根据不同的请求去做不同的跳转页面控制，比如普通用户登录就跳转到普通用户的登录界面，管理员就跳转到管理员登录的登录页面。\n\n 我们新建LoginAuthFailHandler并且继承simpleUrlAuthenticationFailureHandler\n\n\n\n并且定义一个属性 ，LoginUrlEntryPoint 用来跳转到需要跳转的url并附带一些其他信息。\n\n并且实现一个覆盖类onAuthenticationFailure\n\n里面方法写this.urlEntryPoint.determineUrlToUseForThisRequest(request,response,exception)，并且返回的是targetUrl。\n\n接下来对我们获取到的targetUrl进行处理：\n\n```java\ntargetUrl += “？” +exception.getMessage()\n```\n\n然后再用父类的一个方法，跳转到targetUrl。\n\n```java\nsuper.setDefaultFailureUrl(targetUrl);\n```\n\n执行父级的跳转逻辑：\n\n```java\nsuper.onAuthenticationFailure(request,response,exception);\n```\n\n这个时候呢，验证失败处理器LoginAuthFailHandler就配置完成了，但是还要设置另外一个地方，那就是WebSecurityConfig中设置failureHandler（authFailHandler（））\n\nauthFailHandler是新建的一个Bean类\n\n```java\n@Bean\npublic LoginAuthFailHandler authFailHandler(){\n\treturn new LoginAuthFailHandler(urlEntryPoint())\n}\n```\n\n\n\n\n\n## 十六、房源信息管理模块\n\n### 业务与功能分析：\n\n​\t已经搭建了后台管理模块的框架，另外还需要完善网站的房源信息管理子模块，那么接下来就要实现房源信息的增删改查。\n\n\n\n### 实现目标：\n\n- ​\t新增房源\n- ​    房源信息管理（查、改、删）\n- ​    房源审核（一些完善性的工作）\n\n## 十七、基于七牛云的图片上传（上传到本地）\n\n图片上传功能：\n\n- ​\t上传到本地\n- ​    上传到七牛云\n\n七牛依赖：\n\n```xml\n<dependency>\n\t<groupId>com.qiniu</groupId>\n\t<artifacId>qiniu-java-sdk</artifactId>\n\t<version>[7.2.0, 7.2.99]</version>\n</dependency>\n```\n\n前端使用的是百度开源的webuploader。\n\n后段需要在AdminController中定义：\n\n```java\n@PostMapping(value = \"admin/upload/photo\", consumes = MediaType.MULTIPART_FROM_DATA_VALUE)\n@ResponseBody\npublic ApiResponse uploadPhoto(@RequestParam(\"file\") MultipartFile file){\n\t\treturn ApiResponse.ofSuccess(null)\n}\n\n```\n\n\n\n另外新建一个文件上传配置类\n\n```java\n@Configuration\n@ConditionalOnClass({Servlet.class, StandardServletMultipartResolver.class, MultipartConfigElement.class})\n@ConditionalOnProperty(prefix = \"spring.http.multipart\", name = \"enabled\", matchIfMissing = true)\n@EnableConfigurationProperties(MultipartProperties.class)\npublic class WebFileUploadConfig {\n}\n```\n\n\n\n另外呢还要去properties类中设置mutipartProperties  multipart config\n\n```java\nspring.http.multipart.enabled=true\nspring.http.multipart.location=/Users/zhiqiang/Downloads/zq/有趣的项目/soufang-test/tmp\nspring.http.multipart.file-size-threshold=5MB\nspring.http.multipart.max-file-size=20MB\n```\n\n然后继续完善类里面的配置，因为我们让springboot自动配置了MultipartProperties.class类，所以我们要定义一个MultipartProperties 变量。\n\n然后新建一个WebFileUploadConfig类去注入这个属性\n\n```java\n  private final MultipartProperties multipartProperties;\n\n    public WebFileUploadConfig(MultipartProperties multipartProperties, MultipartProperties multipartProperties1){\n        this.multipartProperties = multipartProperties;\n    }\n```\n\n另外还需要创建一个上传配置类：\n\n```java\n    /**\n     * 上传配置\n     */\n    @Bean\n    @ConditionalOnMissingBean\n    public MultipartConfigElement multipartConfigElement(){\n        return this.multipartProperties.createMultipartConfig();\n    }\n}\n```\n\n另外还需要创建一个注册解析器：\n\n```java\n /**\n     * 注册解析器\n     */\n    @Bean(name = DispatcherServlet.MULTIPART_RESOLVER_BEAN_NAME)\n    @ConditionalOnMissingBean(MultipartResolver.class)\n    public StandardServletMultipartResolver multipartResolver(){\n        StandardServletMultipartResolver multipartResolver = new StandardServletMultipartResolver();\n        multipartResolver.setResolveLazily(this.multipartProperties.isResolveLazily());\n\n        return multipartResolver;\n\n    }\n```\n\n创建完毕后需要再完善一下webUpload的接口。\n\n```java\n @PostMapping(value = \"admin/upload/photo\", consumes = MediaType.MULTIPART_FORM_DATA_VALUE)\n    @ResponseBody\n    public ApiResponse uploadPhoto(@RequestParam(\"file\") MultipartFile file){\n\n        if (file.isEmpty()){\n            return ApiResponse.ofStatus(ApiResponse.Status.NOT_VALID_PARAM);\n        }\n\n        String fileName = file.getOriginalFilename();\n        File target  = new File(\"/Users/zhiqiang/Downloads/zq/tmp/\" +fileName);\n\n        try {\n            file.transferTo(target);\n        } catch (IOException e) {\n            e.printStackTrace();\n            return ApiResponse.ofStatus(ApiResponse.Status.INTERNAL_SERVER_ERROR);\n        }\n        return ApiResponse.ofSuccess(null);\n    }\n```\n\n这样图片上传就到了我们的本地/Users/zhiqiang/Downloads/zq/tmp/了。\n\n\n\n## 十八、基于七牛云的图片上传（上传到七牛云）\n\n <img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h51nur7z9yj20m30ld769.jpg\" alt=\"image-20220810143709446\"  />\n\n\n\n配置七牛云服务器直传，需要设置一个Bean类去配置qiniuConfig\n\n```java\n /**\n     * 为七牛云配置华东机房zone0\n     * @return\n     */\n    @Bean\n    public com.qiniu.storage.Configuration qiniuConfig(){\n        return new com.qiniu.storage.Configuration(Zone.zone1());\n    }\n```\n\n\n\n并且设置一个七牛云上传工具管理类，把上面设置的配置类当作参数传入进去。\n\n另外七牛云还需要配置AccessKey， SecretKey和Bucket\n\n另外，七牛云新建对象存储后会有30天的临时测试域名。\n\n![image-20220810152446672](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h51p89puj6j215w0cm0ts.jpg)\n\n\n\n并且在properties文件中定义：\n\n```java\n# qiniu config\nqiniu.AccessKey=FQ_nf-hE1Bu7ZE-ffCgkSxQnbUGXhBGgT0UVwP-J\nqiniu.SecretKey=mPHwqW5mYIp3Wgdj2vFvWAATN8NBgwVo0rHduz44\nqiniu.Bucket=soufang-zryy\nqiniu.cdn.prefix=http://rge19896q.hb-bkt.clouddn.com/\n```\n\n然后在WebFileUploadConfig文件中以注解的方式注入：\n\n```java\n@Value(\"${qiniu.AccessKey}\")\nprivate String accessKey;\n\n@Value(\"${qiniu.SecretKey}\")\nprivate String secretKey;\n```\n\n\n\n然后利用这两个变量来生成认证信息：\n\n```java\n/**\n * 认证信息实例\n */\n\n@Bean\npublic Auth auth(){\n    return Auth.create(accessKey,secretKey);\n}\n```\n\n并且构建七牛云空间管理实例，把认证类auth当作参数传入，并且把qiniuConfig传入：\n\n```java\n/**\n * 构建七牛空间管理实例\n */\n@Bean\npublic BucketManager bucketManager(){\n    return new BucketManager(auth(),qiniuConfig());\n}\n```\n\n\n\n这样配置文件弄好了，就可以去设计业务了。从设计业务角度来讲，从顶层由上自下设计是合适的。从开发流程来说，从数据库建表开始进行开始是最为合适的。\n\n新建IQiNiuService接口：\n\n``` java\npublic interface  IQiNiuService {\n\n    //文件形式上传\n    Response uploadFile(File file) throws QiniuException;\n\n    //文件流形式上传\n    Response uploadFile(InputStream inputStream) throws QiniuException;\n\n    //删除文件\n    Response delete(String key) throws QiniuException;\n}\n```\n\n然后新建QiNiuServiceImpl实现类\n\n```java\n@Service\npublic class QiNiuServiceImpl implements IQiNiuService, InitializingBean {\n\n    @Autowired\n    private UploadManager uploadManager;\n\n    @Autowired\n    private BucketManager bucketManager;\n\n    @Autowired\n    private Auth auth;\n\n    @Value(\"${qiniu.Bucket}\")\n    private String bucket;\n\n\n    private StringMap putPolicy;\n\n    @Override\n    public Response uploadFile(File file) throws QiniuException {\n        Response response  = this.uploadManager.put(file,null,getUploadToken());\n        int retry = 0;\n        while(response.needRetry() && retry < 3){\n            response  = this.uploadManager.put(file,null,getUploadToken());\n            retry++;\n        }\n\n        return response;\n    }\n\n    @Override\n    public Response uploadFile(InputStream inputStream) throws QiniuException {\n        return null;\n    }\n\n    @Override\n    public Response delete(String key) throws QiniuException {\n        return null;\n    }\n\n    @Override\n    public void afterPropertiesSet() throws Exception {\n        this.putPolicy = new StringMap();\n        putPolicy.put(\"returnBody\", \"{\\\"key\\\":\\\"$(key)\\\",\\\"hash\\\":\\\"$(etag)\\\",\\\"bucket\\\":\\\"$(bucket)\\\",\\\"width\\\":$(imageInfo.width), \\\"height\\\":${imageInfo.height}}\");\n    }\n\n    /**\n     * 获取上传凭证\n     * @return\n     */\n    private String getUploadToken(){\n         return this.auth.uploadToken(bucket,null,3600, putPolicy);\n    }\n```\n\n其中自动配置@Autowired upLoadManager类，\n\nuploadManager.put传的参数是file、null、以及return this.auth.uploadToken(bucket,null,3600, putPolicy);\n\n其中InitializingBean、afterPropertiesSet当一个类实现这个接口之后，Spring启动后，初始化Bean时，若该Bean实现InitialzingBean接口，会自动调用afterPropertiesSet()方法，完成一些用户自定义的初始化操作。\n\nputPolicy是七牛云固定的returnBody的格式。\n\n\n\n最后我们建立一个测试类并且继承之前定义好的SoufangtestApplicationTests\n\n```java\npublic class QiNiuServiceTests extends SoufangTestApplicationTests {\n\n    @Autowired\n    private IQiNiuService qiNiuService;\n\n    @Test\n    public void testUploadFile(){\n        String fileName = \"/Users/zhiqiang/Downloads/zq/tmp/664328b2a21a76bd8de7c629eeff7e.jpg\";\n\n\n        File file = new File(fileName);\n        Assert.assertTrue(file.exists());\n\n        try {\n            Response response = qiNiuService.uploadFile(file);\n            Assert.assertTrue(response.isOK());\n        } catch (QiniuException e) {\n            e.printStackTrace();\n        }\n    }\n\n}\n```\n\n 测试通过，成功上传到七牛云上面。\n\n![image-20220811141223495](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h52sr9r1c0j21gg0u0wia.jpg)\n\n\n\n\n\n> 另外除了uploadFile外，我们还定义了uploadInputStream。\n\n```java\n@PostMapping(value = \"admin/upload/photo\", consumes = MediaType.MULTIPART_FORM_DATA_VALUE)\n@ResponseBody\npublic ApiResponse uploadPhoto(@RequestParam(\"file\") MultipartFile file) throws IOException {\n\n    if (file.isEmpty()){\n        return ApiResponse.ofStatus(ApiResponse.Status.NOT_VALID_PARAM);\n    }\n\n    String fileName = file.getOriginalFilename();\n\n\n    try {\n        InputStream inputStream = file.getInputStream();\n        Response response = iQiNiuService.uploadFile(inputStream);\n        if (response.isOK()){\n            return ApiResponse.ofSuccess();\n        }else{\n            return ApiResponse.ofMessage(response.statusCode,response.getInfo());\n        }\n\n    }\n    catch (IOException e){\n        return ApiResponse.ofStatus(ApiResponse.Status.INTERNAL_SERVER_ERROR);\n    }\n\n}\n```\n\n 不过在return ApiResponse.ofSuccess的时候，需要定义一个dto来接收，这时候新建一个QiNiuPutRet类，并且定义：\n\n```java\npublic final class QiNiuPutRet {\n    public String key;\n    public String hash;\n    public String bucket;\n    public int width;\n    public int height;\n\n\n    @Override\n    public String toString() {\n        return \"QiNiuPutRet{\" +\n                \"key='\" + key + '\\'' +\n                \", hash='\" + hash + '\\'' +\n                \", bucket='\" + bucket + '\\'' +\n                \", width=\" + width +\n                \", height=\" + height +\n                '}';\n    }\n}\n```\n\n\n\n而且在WebFileUploadConfig类中定义一个Gson解析json\n\n``` java\n@Bean\npublic Gson gson(){\n    return new Gson();\n}\n```\n\n这样呢，就可以在controller中去自动注入Gson了\n\nAdminController\t————》\n\n```java\n@Autowired\nprivate Gson gson;\n\ngson.fromJson(response.bodyString(), QiNiuPutRet.class);\n```\n\n gson.fromJson需要输入两个参数一个是string类型的json数据，另外一个是对应变量的类。所以需要定义一个dto类QiNiuPutRet.class\n\n 另外呢还需要定义一个catch去抓取QiNiu的异常。\n\n``` java\ntry {\n    InputStream inputStream = file.getInputStream();\n    Response response = iQiNiuService.uploadFile(inputStream);\n    if (response.isOK()){\n        QiNiuPutRet ret = gson.fromJson(response.bodyString(), QiNiuPutRet.class);\n        return ApiResponse.ofSuccess(ret);\n    }else{\n        return ApiResponse.ofMessage(response.statusCode,response.getInfo());\n    }\n\n}\ncatch (QiniuException e){\n    Response response = e.response; \n    return ApiResponse.ofMessage(response.statusCode,response.bodyString());\n}\ncatch (IOException e){\n    return ApiResponse.ofStatus(ApiResponse.Status.INTERNAL_SERVER_ERROR);\n}\n```\n\n\n\n\n\n在我们编写完以文件流方式上传图片后，获取到了文件的key：FnJXb0TSb-ImeX1PzxIRo9wc1AkX\n\n然后我们就去实现delete操作：\n\n 但是这里需要注意的是delete是操作的BucketManager，而上传使用的是uploadManager\n\n``` java\n@Override\npublic Response delete(String key) throws QiniuException {\n    Response response = bucketManager.delete(this.bucket, key);\n    int retry = 0;\n    while (response.needRetry() && retry++ < 3) {\n        response = bucketManager.delete(bucket, key);\n    }\n    return response;\n}\n```\n\n 然后编辑测试类：\n\n``` java\n@Test\npublic void deleteFile() throws QiniuException {\n    String key = \"FnJXb0TSb-ImeX1PzxIRo9wc1AkX\";\n    try {\n        Response delete = qiNiuService.delete(key);\n        Assert.assertTrue(delete.isOK());\n    }catch (QiniuException e){\n        e.printStackTrace();\n    }\n\n}\n```\n\n最终的QiNiuServiceImpl类的实现代码为：\n\n``` java\n@Service\npublic class QiNiuServiceImpl implements IQiNiuService, InitializingBean {\n\n    @Autowired\n    private UploadManager uploadManager;\n\n    @Autowired\n    private BucketManager bucketManager;\n\n    @Autowired\n    private Auth auth;\n\n    @Value(\"${qiniu.Bucket}\")\n    private String bucket;\n\n\n    private StringMap putPolicy;\n\n    @Override\n    public Response uploadFile(File file) throws QiniuException {\n        Response response  = this.uploadManager.put(file,null,getUploadToken());\n        int retry = 0;\n        while(response.needRetry() && retry < 3){\n            response  = this.uploadManager.put(file,null,getUploadToken());\n            retry++;\n        }\n\n        return response;\n    }\n\n    @Override\n    public Response uploadFile(InputStream inputStream) throws QiniuException {\n        Response response  = this.uploadManager.put(inputStream,null,getUploadToken(),null,null);\n        int retry = 0;\n        while(response.needRetry() && retry < 3){\n            response  = this.uploadManager.put(inputStream,null,getUploadToken(),null,null);\n            retry++;\n        }\n        return response;\n    }\n\n    @Override\n    public Response delete(String key) throws QiniuException {\n        Response deleteResponse = bucketManager.delete(this.bucket, key);\n        int retry = 0;\n        while(deleteResponse.needRetry() && retry++ <3 ){\n            deleteResponse = bucketManager.delete(bucket,key);\n        }\n\n        return deleteResponse;\n    }\n\n    @Override\n    public void afterPropertiesSet() throws Exception {\n        this.putPolicy = new StringMap();\n        putPolicy.put(\"returnBody\", \"{\\\"key\\\":\\\"$(key)\\\",\\\"hash\\\":\\\"$(etag)\\\",\\\"bucket\\\":\\\"$(bucket)\\\",\\\"width\\\":$(imageInfo.width), \\\"height\\\":${imageInfo.height}}\");\n    }\n\n    /**\n     * 获取上传凭证\n     * @return\n     */\n    private String getUploadToken(){\n         return this.auth.uploadToken(bucket,null,3600, putPolicy);\n    }\n}\n```\n\n## 十九、新增房源信息功能(1)\n\n在新增房源这里我们设定了支持城市下拉的选择：\n\n![image-20220812094729522](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h53qpzhcenj218u0emabu.jpg)\n\n![image-20220812094810146](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h53qqn8xc5j21z20luwgu.jpg)\n\n我们从下到上开始创建：\n\n新建一个controller类。HouseController\n\n并且新建实体类：\n\n```java\npackage com.zryy.soufangtest.entity;\n\nimport javax.persistence.*;\n\n@Entity\n@Table(name = \"support_address\")\npublic class SupportAddress {\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n\n    //上一级行政单位\n    @Column(name = \"belong_to\")\n    private String belongTo;\n\n    //英文名称\n    @Column(name = \"en_name\")\n    private String enName;\n\n    //中文名称\n    @Column(name = \"cn_name\")\n    private String cnName;\n\n    //行政级别\n    private String level;\n\n\n    public Long getId() {\n        return id;\n    }\n\n    public void setId(Long id) {\n        this.id = id;\n    }\n\n    public String getBelongTo() {\n        return belongTo;\n    }\n\n    public void setBelongTo(String belongTo) {\n        this.belongTo = belongTo;\n    }\n\n    public String getEnName() {\n        return enName;\n    }\n\n    public void setEnName(String enName) {\n        this.enName = enName;\n    }\n\n    public String getCnName() {\n        return cnName;\n    }\n\n    public void setCnName(String cnName) {\n        this.cnName = cnName;\n    }\n\n    public String getLevel() {\n        return level;\n    }\n\n    public void setLevel(String level) {\n        this.level = level;\n    }\n\n    /**\n     * 行政级别定义\n     */\n    public enum Level{\n        CITY(\"city\"),\n        REGION(\"region\");\n\n        private String value;\n\n        //构造函数\n        Level(String value){\n            this.value = value;\n        }\n\n        public String getvalue(){\n            return value;\n        }\n        public static Level of(String value){\n            for (Level level : Level.values()) {\n                if(level.getvalue().equals(value)){\n                    return level;\n                }\n            }\n            throw new IllegalArgumentException();\n        }\n\n\n    }\n\n\n}\n```\n\n另外呢，要新建Repository接口SupportAddressRepository去继承CrudRepository<SupportAddress,Long>\n\n第一个参数是实体类，第二个参数是id的类型\n\n```java\npublic interface SupportAddressRepository extends CrudRepository<SupportAddress,Long> {\n}\n```\n\n\n\n然后我们去定义controller：\n\n```java\n@Controller\npublic class HouseController {\n\n    @GetMapping(\"address/support/cities\")\n    @ResponseBody\n    public ApiResponse getSupportCities(){\n        return ApiResponse.ofSuccess(null);\n    }\n}\n```\n\n既然我们定义了support/cities路径所以我们的dao就需要一个获取所有城市列表的接口：\n\n```java\npublic interface SupportAddressRepository extends CrudRepository<SupportAddress,Long> {\n\n    /**\n     * 获取所有对应行政级别的信息\n     */\n    List<SupportAddress> findAllByLevel(String level);\n}\n```\n\n然后再去定义service层：\n\n新建IAddressService接口\n\n```java\npublic interface IAddressService(){\n\tList<SupportAddress> findAllCities();\n}\n```\n\n**虽然我们可以这样定义接口， 但是不建议，我们的dao和返回的dto（也就是传输到前端的类型）尽量要有一个数据的隔阂。也就是对数据进行保护，并不是所有的数据都可以被前端获取到。所以我们要定义一个中间的转换对象，所以我们新建一个新的类型SupportAddressDTO**\n\n```java\npublic class SupportAddressDTO {\n    private Long id;\n\n    @JsonProperty(value = \"belong_to\")\n    private String belongTo;\n\n\n    @JsonProperty(value = \"en_name\")\n    private String enName;\n\n    @JsonProperty(value = \"cn_name\")\n    private String cnName;\n\n    private String level;\n\n    public Long getId() {\n        return id;\n    }\n\n    public void setId(Long id) {\n        this.id = id;\n    }\n\n    public String getBelongTo() {\n        return belongTo;\n    }\n\n    public void setBelongTo(String belongTo) {\n        this.belongTo = belongTo;\n    }\n\n    public String getEnName() {\n        return enName;\n    }\n\n    public void setEnName(String enName) {\n        this.enName = enName;\n    }\n\n    public String getCnName() {\n        return cnName;\n    }\n\n    public void setCnName(String cnName) {\n        this.cnName = cnName;\n    }\n\n    public String getLevel() {\n        return level;\n    }\n\n    public void setLevel(String level) {\n        this.level = level;\n    }\n}\n```\n\n**这样呢我们就需要在IAddressService这里去修改返回list的类型了，由SupportAddress转换成了SupportAddressDTO：**\n\n```java\npublic interface IAddressService(){\n\tList<SupportAddressDTO> findAllCities();\n}\n```\n\n然后新建一个service实现类，AddressServiceImpl 实现IAddressService：\n\n```java\npublic ServiceMultiResult<SupportAddressDTO> findAllCities() {\n    List<SupportAddress> addresses = supportAddressRepository.findAllByLevel(SupportAddress.Level.CITY.getValue());\n    List<SupportAddressDTO> addressDTOS = new ArrayList<>();\n    for (SupportAddress supportAddress : addresses) {\n        SupportAddressDTO target = modelMapper.map(supportAddress, SupportAddressDTO.class);\n        addressDTOS.add(target);\n    }\n\n    return new ServiceMultiResult<>(addressDTOS.size(), addressDTOS);\n}\n```\n\n​\t在上面的函数中 我们使supportAddressRepository去查询所有的地址后，返回了supportAddress的list，然后新建了一个List<SupportAddressDTO>的列表，进行for循环，然后通过建立的modelMapper进行映射。\n\n​\t在我们对SupportAddress类型转换为SupportAddressDto类型的时候在webMvcConfig中定义了一个Bean Util，modelMapper是一个复制bean的。\n\n```java\n/**\n * Bean Util\n */\n@Bean\npublic ModelMapper modelMapper(){\n    return new ModelMapper();\n}\n```\n\n\n\n\n\n这样呢我们就存在一个问题，虽然这样我们虽然返回了一个List列表（findAllCities函数中），\n\n```java\npublic List<SupportAddressDTO> findAllCities(){}\n```\n\n但是其他接口也有返回一个supportAddress列表呢？比如说分页，所以说我们要定义这个返回的列表，设置所有的返回都有一个list，同时呢还要有一个表示数据总集的字段（result），下面去定义一个通用的结构：\n\nServiceMultiResult\n\n我们定义了total、List<T> result 、以及getResultSize方法作为该类的返回结构。\n\n```java\n/**\n * 通用多结果Service返回结构\n */\npublic class ServiceMultiResult<T> {\n    private long total;\n    private List<T> result;\n\n    public ServiceMultiResult(long total, List<T> result) {\n        this.total = total;\n        this.result = result;\n    }\n\n    public long getTotal() {\n        return total;\n    }\n\n    public void setTotal(long total) {\n        this.total = total;\n    }\n\n    public List<T> getResult() {\n        return result;\n    }\n\n    public void setResult(List<T> result) {\n        this.result = result;\n    }\n\n    public int getResultSize() {\n        if (this.result == null) {\n            return 0;\n        }\n        return this.result.size();\n    }\n}\n```\n\n这样呢我们在AddressServiceImpl中定义的返回就不用List了，而是使用我们自己定义好的ServiceMultiResult了。\n\n修改前：\n\n```java\npublic List<SupportAddressDTO> findAllCities(){}\n```\n\n修改后：\n\n```java\npublic ServiceMultiResult<SupportAddressDTO> findAllCities() {\n  \n  //省略具体操作\n  \n   return new ServiceMultiResult<>(addressDTO.size(), addressDTOS)\n}\n```\n\n上面传递的两个参数是因为我们定义的ServiceMultiResult需要传入两个类变量，而第二个变量需要传入的是一个List<T>类型。\n\n在设计完service层后，我们去定义controller层：\n\n```java\n@Controller\npublic class HouseController {\n\n    @Autowired\n    private IAddressService addressService;\n\n    @GetMapping(\"address/support/cities\")\n    @ResponseBody\n    public ApiResponse getSupportCities(){\n        ServiceMultiResult<SupportAddressDTO> result = addressService.findAllCities();\n        if (result.getResultSize() == 0){\n            return ApiResponse.ofSuccess(ApiResponse.Status.NOT_FOUND);\n        }\n        return ApiResponse.ofSuccess(result.getResult());\n\n    }\n}\n```\n\n![image-20220815164459062](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h57jnatbypj20th0eignw.jpg)\n\n在查询的时候，我们是根据ByLevel进行查询的，传递的是定义好的Enum类型SupportAddress.Level.CITY来进行条件检索的。\n\n```java\nList<SupportAddress> addresses = supportAddressRepository.findAllByLevel(SupportAddress.Level.CITY.getValue());\n```\n\n> 另外还有区县、地铁站、地铁线路的接口实现，这里不做过多说明，只列举相关代码。\n\n```java\n@Controller\npublic class HouseController {\n\n    @Autowired\n    private IAddressService addressService;\n\n\n    /**\n     * 获取支持城市列表\n     *\n     * @return\n     */\n    @GetMapping(\"address/support/cities\")\n    @ResponseBody\n    public ApiResponse getSupportCities() {\n        ServiceMultiResult<SupportAddressDTO> result = addressService.findAllCities();\n        if (result.getResultSize() == 0) {\n            return ApiResponse.ofSuccess(ApiResponse.Status.NOT_FOUND);\n        }\n        return ApiResponse.ofSuccess(result.getResult());\n\n    }\n\n\n    /**\n     * 获取对应城市支持区域列表\n     *\n     * @param cityEnName\n     * @return\n     */\n    @GetMapping(\"address/support/regions\")\n    @ResponseBody\n    public ApiResponse getSupportRegions(@RequestParam(name = \"city_name\") String cityEnName) {\n        ServiceMultiResult<SupportAddressDTO> addressResult = addressService.findAllRegionsByCityName(cityEnName);\n        if (addressResult.getResult() == null || addressResult.getTotal() < 1) {\n            return ApiResponse.ofStatus(ApiResponse.Status.NOT_FOUND);\n        }\n        return ApiResponse.ofSuccess(addressResult.getResult());\n    }\n\n    /**\n     * 获取具体城市所支持的地铁线路\n     * @param cityEnName\n     * @return\n     */\n    @GetMapping(\"address/support/subway/line\")\n    @ResponseBody\n    public ApiResponse getSupportSubwayLine(@RequestParam(name = \"city_name\") String cityEnName) {\n        List<SubwayDTO> subways = addressService.findAllSubwayByCity(cityEnName);\n        if (subways.isEmpty()) {\n            return ApiResponse.ofStatus(ApiResponse.Status.NOT_FOUND);\n        }\n\n        return ApiResponse.ofSuccess(subways);\n    }\n\n    /**\n     * 获取对应地铁线路所支持的地铁站点\n     * @param subwayId\n     * @return\n     */\n    @GetMapping(\"address/support/subway/station\")\n    @ResponseBody\n    public ApiResponse getSupportSubwayStation(@RequestParam(name = \"subway_id\") Long subwayId) {\n        List<SubwayStationDTO> stationDTOS = addressService.findAllStationBySubway(subwayId);\n        if (stationDTOS.isEmpty()) {\n            return ApiResponse.ofStatus(ApiResponse.Status.NOT_FOUND);\n        }\n\n        return ApiResponse.ofSuccess(stationDTOS);\n    }\n}\n```\n\n> 从总体上来说，关于其他（地铁站、地铁线等等）的查询以及展示都是类似思想，无非就是通过前端联结查询，当选择城市的时候，会传递城市名称并请求地铁站api，然后再前端并联渲染出来，不过需要注意的地方是我们定义了DTO类，并非原来的Entity实体类直接返回出来，而是定义了其他的一些的返回类，并非原来的List，定义了其他的一些返回类定义了一些关于ResultSize等等之类的信息，非常方便获取。\n>\n\n## 二十、新增房源信息功能(2)\n\n我们新建了实体类：\n\n​\tHouse\n\n​\tHouseDetail\n\n​\tHousePicture\n\n​\tHouseSubscribe\n\n​\tHouseTag\n\n然后我们新建新的HouseRepository、HouseDetailRepository、HousePictureRepository、HouseTagRepository\n\nRepository创建完了，我们新建一个IService接口：\n\n```java\npublic interface IHouseService {\n\tsave()\n}\n```\n\n在Service接口中我们去定义一个save房源类，可是它返回什么类型呢？\n\n我们考虑像之前定义的ServiceMultiResult类一样，去定义这个返回类。\n\n\n\n```java\n/**\n * 服务接口通用结构\n */\npublic class ServiceResult<T> {\n    private boolean success;\n    private String message;\n    private T result;\n\n    public ServiceResult(boolean success) {\n        this.success = success;\n    }\n\n    public ServiceResult(boolean success, String message) {\n        this.success = success;\n        this.message = message;\n    }\n\n    public ServiceResult(boolean success, String message, T result) {\n        this.success = success;\n        this.message = message;\n        this.result = result;\n    }\n\n    public boolean isSuccess() {\n        return success;\n    }\n\n    public void setSuccess(boolean success) {\n        this.success = success;\n    }\n\n    public String getMessage() {\n        return message;\n    }\n\n    public void setMessage(String message) {\n        this.message = message;\n    }\n\n    public T getResult() {\n        return result;\n    }\n\n    public void setResult(T result) {\n        this.result = result;\n    }\n}\n```\n\n这样定义完返回的类类型后，重新去service中去设置save方法:\n\n```java\npublic interface IHouseService {\n    ServiceResult<> save();\n}\n```\n\n但是，ServiceResult我们定义的是一个范型类T，直接设置成House Entity的话就会暴露，所以我们需要重新去定义一个HouseDTO类，并且附带定义了HostDetailDTO类、HousePictrueDTO类。\n\n这样呢service中的save方法就可以设置了：\n\n```java\npublic interface IHouseService {\n    ServiceResult<HouseDTO> save();\n}\n```\n\n但是这里save方法接收的参数类是什么？实质上是前端页面传过来的form表单，这里我们也重新定义一个类去接收这个表单一一对应。\n\n```java\npublic class HouseForm {\n    private Long id;\n\n    @NotNull(message = \"大标题不允许为空!\")\n    @Size(min = 1, max = 30, message = \"标题长度必须在1~30之间\")\n    private String title;\n\n    @NotNull(message = \"必须选中一个城市\")\n    @Size(min = 1, message = \"非法的城市\")\n    private String cityEnName;\n\n    @NotNull(message = \"必须选中一个地区\")\n    @Size(min = 1, message = \"非法的地区\")\n    private String regionEnName;\n\n    @NotNull(message = \"必须填写街道\")\n    @Size(min = 1, message = \"非法的街道\")\n    private String street;\n\n    @NotNull(message = \"必须填写小区\")\n    private String district;\n\n    @NotNull(message = \"详细地址不允许为空!\")\n    @Size(min = 1, max = 30, message = \"详细地址长度必须在1~30之间\")\n    private String detailAddress;\n\n    @NotNull(message = \"必须填写卧室数量\")\n    @Min(value = 1, message = \"非法的卧室数量\")\n    private Integer room;\n\n    private int parlour;\n\n    @NotNull(message = \"必须填写所属楼层\")\n    private Integer floor;\n\n    @NotNull(message = \"必须填写总楼层\")\n    private Integer totalFloor;\n\n    @NotNull(message = \"必须填写房屋朝向\")\n    private Integer direction;\n\n    @NotNull(message = \"必须填写建筑起始时间\")\n    @Min(value = 1900, message = \"非法的建筑起始时间\")\n    private Integer buildYear;\n\n    @NotNull(message = \"必须填写面积\")\n    @Min(value = 1)\n    private Integer area;\n\n    @NotNull(message = \"必须填写租赁价格\")\n    @Min(value = 1)\n    private Integer price;\n\n    @NotNull(message = \"必须选中一个租赁方式\")\n    @Min(value = 0)\n    @Max(value = 1)\n    private Integer rentWay;\n\n    private Long subwayLineId;\n\n    private Long subwayStationId;\n\n    private int distanceToSubway = -1;\n\n    private String layoutDesc;\n\n    private String roundService;\n\n    private String traffic;\n\n    @Size(max = 255)\n    private String description;\n\n    private String cover;\n\n    private List<String> tags;\n\n    private List<PhotoForm> photos;\n\n    public Long getId() {\n        return id;\n    }\n\n    public void setId(Long id) {\n        this.id = id;\n    }\n\n    public String getTitle() {\n        return title;\n    }\n\n    public void setTitle(String title) {\n        this.title = title;\n    }\n\n    public String getCityEnName() {\n        return cityEnName;\n    }\n\n    public void setCityEnName(String cityEnName) {\n        this.cityEnName = cityEnName;\n    }\n\n    public String getRegionEnName() {\n        return regionEnName;\n    }\n\n    public void setRegionEnName(String regionEnName) {\n        this.regionEnName = regionEnName;\n    }\n\n    public String getStreet() {\n        return street;\n    }\n\n    public void setStreet(String street) {\n        this.street = street;\n    }\n\n    public String getDistrict() {\n        return district;\n    }\n\n    public void setDistrict(String district) {\n        this.district = district;\n    }\n\n    public String getDetailAddress() {\n        return detailAddress;\n    }\n\n    public void setDetailAddress(String detailAddress) {\n        this.detailAddress = detailAddress;\n    }\n\n    public Integer getRoom() {\n        return room;\n    }\n\n    public void setRoom(Integer room) {\n        this.room = room;\n    }\n\n    public int getParlour() {\n        return parlour;\n    }\n\n    public void setParlour(int parlour) {\n        this.parlour = parlour;\n    }\n\n    public Integer getFloor() {\n        return floor;\n    }\n\n    public void setFloor(Integer floor) {\n        this.floor = floor;\n    }\n\n    public Integer getTotalFloor() {\n        return totalFloor;\n    }\n\n    public void setTotalFloor(Integer totalFloor) {\n        this.totalFloor = totalFloor;\n    }\n\n    public Integer getDirection() {\n        return direction;\n    }\n\n    public void setDirection(Integer direction) {\n        this.direction = direction;\n    }\n\n    public Integer getBuildYear() {\n        return buildYear;\n    }\n\n    public void setBuildYear(Integer buildYear) {\n        this.buildYear = buildYear;\n    }\n\n    public Integer getArea() {\n        return area;\n    }\n\n    public void setArea(Integer area) {\n        this.area = area;\n    }\n\n    public Integer getPrice() {\n        return price;\n    }\n\n    public void setPrice(Integer price) {\n        this.price = price;\n    }\n\n    public Integer getRentWay() {\n        return rentWay;\n    }\n\n    public void setRentWay(Integer rentWay) {\n        this.rentWay = rentWay;\n    }\n\n    public Long getSubwayLineId() {\n        return subwayLineId;\n    }\n\n    public void setSubwayLineId(Long subwayLineId) {\n        this.subwayLineId = subwayLineId;\n    }\n\n    public Long getSubwayStationId() {\n        return subwayStationId;\n    }\n\n    public void setSubwayStationId(Long subwayStationId) {\n        this.subwayStationId = subwayStationId;\n    }\n\n    public int getDistanceToSubway() {\n        return distanceToSubway;\n    }\n\n    public void setDistanceToSubway(int distanceToSubway) {\n        this.distanceToSubway = distanceToSubway;\n    }\n\n    public String getLayoutDesc() {\n        return layoutDesc;\n    }\n\n    public void setLayoutDesc(String layoutDesc) {\n        this.layoutDesc = layoutDesc;\n    }\n\n    public String getRoundService() {\n        return roundService;\n    }\n\n    public void setRoundService(String roundService) {\n        this.roundService = roundService;\n    }\n\n    public String getTraffic() {\n        return traffic;\n    }\n\n    public void setTraffic(String traffic) {\n        this.traffic = traffic;\n    }\n\n    public String getDescription() {\n        return description;\n    }\n\n    public void setDescription(String description) {\n        this.description = description;\n    }\n\n    public String getCover() {\n        return cover;\n    }\n\n    public void setCover(String cover) {\n        this.cover = cover;\n    }\n\n    public List<String> getTags() {\n        return tags;\n    }\n\n    public void setTags(List<String> tags) {\n        this.tags = tags;\n    }\n\n    public List<PhotoForm> getPhotos() {\n        return photos;\n    }\n\n    public void setPhotos(List<PhotoForm> photos) {\n        this.photos = photos;\n    }\n\n    @Override\n    public String toString() {\n        return \"HouseForm{\" +\n                \"id=\" + id +\n                \", title='\" + title + '\\'' +\n                \", cityEnName='\" + cityEnName + '\\'' +\n                \", regionEnName='\" + regionEnName + '\\'' +\n                \", district='\" + district + '\\'' +\n                \", detailAddress='\" + detailAddress + '\\'' +\n                \", room=\" + room +\n                \", parlour=\" + parlour +\n                \", floor=\" + floor +\n                \", totalFloor=\" + totalFloor +\n                \", direction=\" + direction +\n                \", buildYear=\" + buildYear +\n                \", area=\" + area +\n                \", price=\" + price +\n                \", rentWay=\" + rentWay +\n                \", subwayLineId=\" + subwayLineId +\n                \", subwayStationId=\" + subwayStationId +\n                \", distanceToSubway=\" + distanceToSubway +\n                \", layoutDesc='\" + layoutDesc + '\\'' +\n                \", roundService='\" + roundService + '\\'' +\n                \", traffic='\" + traffic + '\\'' +\n                \", description='\" + description + '\\'' +\n                \", cover='\" + cover + '\\'' +\n                \", photos=\" + photos +\n                '}';\n    }\n}\n```\n\n现在就变成了：\n\n```java\npublic interface IHouseService {\n    ServiceResult<HouseDTO> save(HouseForm);\n}\n```\n\n\n\nservice编写完毕后，房源信息的添加应该是在admin下，所以我们在adminController下编写controller类：\n\n```java\n@PostMapping(\"admin/add/house\")\n@ResponseBody\npublic ApiResponse addHouse(@Valid @ModelAttribute(\"form-house-add\") HouseForm houseForm, BindingResult bindingResult){\n    if (bindingResult.hasErrors()){\n        return new ApiResponse(HttpStatus.BAD_REQUEST.value(),bindingResult.getAllErrors().get(0).getDefaultMessage(),null);\n    }\n\n    if(houseForm.getPhotos() ==null || houseForm.getCover() ==null){\n        return ApiResponse.ofMessage(HttpStatus.BAD_REQUEST.value(), \"必须上传图片\");\n    }\n    \n  \t//对传入的地址表单进行校验\n    Map<SupportAddress.Level, SupportAddressDTO> cityAndRegionMap = addressService.findCityAndRegion(houseForm.getCityEnName(), houseForm.getRegionEnName());\n    if(cityAndRegionMap.keySet().size() != 2){\n      return ApiResponse.ofStatus(ApiResponse.Status.NOT_VALID_PARAM);\n    }\n\t\t\n  //HouseService的处理\n  //对定义的houseForm进行操作\n        ServiceResult<HouseDTO> result = houseService.save(houseForm);\n        if(result.isSuccess()){\n            return ApiResponse.ofSuccess(result.getResult());\n        }\n        return ApiResponse.ofSuccess(ApiResponse.Status.NOT_VALID_PARAM);\n\n}\n```\n\n我们接口的参数名称是form-house-add，并使用ModelAttribute注解来使用，另外使用@Valid来自动的对表单进行验证。\n\n**另外我们定义了一个BindingResult类：作用：用于对前端穿进来的参数进行校验，省去了大量的逻辑判断操作，一开始传入的参数没有使用@Validated 修饰，结果绑定不起作用，参数校验不成功，加上此注解即可生效。\n所以BingdingResult是要与@Validated同时使用的。**\n\nbindingResult类如果entity类校验错误的话，错误信息就会绑定到bindingResult类上面去。\n\n> 关于bindingResult的详细内容需要后期补充\n\n\n\n另外在编辑完毕对传入的地址表单进行验证后（使用的是IAddressService）\n\n我们之前在上面定义的IHouseService，虽然没有定义HouseServiceImpl但是我们是面向接口编程，所以我们先假设使用HouseService执行。\n\n\n\n然后我们编辑了对定义的houseForm进行操作后，开始实现IHouseSevice的实现类：\n\n```java\n//1.第一步 定义一个House实体类\nHouse house = new House();\nhouse.setParlour(houseForm.getParlour());\n```\n\n当然这里我们可以去设置house.set(  houseForm.getXXXX)\n\n不过如果我们有很多个变量的话这样写就会变的很麻烦很繁琐。\n\n**这样可以直接使用我们之前用过的ModelMapper映射过去。**\n\n```java\nmodelMapper.map(houseForm, house);\n```\n\n```java\n//另外还需要创建一些其他的变量\nDate now = new Date();\nhouse.setCreateTime(now);\nhouse.setLastUpdateTime(now);\n\n//设置当前登陆用户的id\nhouse.setAdminId(LoginUserUtil.getUserId());\n\n\nhouseRepository.save(house);\nHouseDetail houseDetail = new HouseDetail();\n\nServiceResult<HouseDTO> subwayValidtionResult = wrapperSubwayDetailInfo(houseDetail, houseForm);\n```\n\n另外呢不只是house的信息，还有houseDetail的信息需要（围绕houseDetail做一些属性的设置，将地铁以及地铁站的信息设置进去。），所以我们定义了一个W rapperSubwayDetailInfo类去得到关于地铁的一些信息设置成houseDetail的一些属性。\n\n```java\n/**\n * 围绕houseDetail做一些属性的设置，将地铁以及地铁站的信息设置进去。\n * @param houseDetail\n * @param houseForm\n * @return\n */\nprivate ServiceResult<HouseDTO> wrapperSubwayDetailInfo(HouseDetail houseDetail, HouseForm houseForm){\n    Subway subwayInfo = subwayRepository.findOne(houseForm.getSubwayLineId());\n    if (subwayInfo == null){\n        return new ServiceResult<>(false,\"Not valid subway line!\");\n    }\n\n    SubwayStation subwayStationInfo = subwayStationRepository.findOne(houseForm.getSubwayStationId());\n    if(subwayStationInfo == null || subwayInfo.getId() != subwayStationInfo.getSubwayId()){\n        return new ServiceResult<>(false,\"Not valid subway staition\");\n    }\n\n    houseDetail.setSubwayLineId(subwayInfo.getId());\n    houseDetail.setSubwayLineName(subwayInfo.getName());\n\n    houseDetail.setSubwayStationId(subwayStationInfo.getSubwayId());\n    houseDetail.setSubwayStationName(subwayStationInfo.getName());\n\n    houseDetail.setDescription(houseForm.getDescription());\n    houseDetail.setDetailAddress(houseForm.getDetailAddress());\n    houseDetail.setLayoutDesc(houseForm.getLayoutDesc());\n    houseDetail.setRentWay(houseForm.getRentWay());\n    houseDetail.setRoundService(houseForm.getRoundService());\n    houseDetail.setTraffic(houseDetail.getTraffic());\n    return null;\n}\n```\n\n> 上面虽然返回了null，但是houseDetail的属性是添加进去了的。\n\n然后houseDetail.setHouseId(house.getId())\n\n使用houseDetailRepository存储到数据库。\n\n\n\n接下来就是housePictrue的实现包装类generatePictures了：\n\n```java\nprivate List<HousePicture> generatePictures(HouseForm houseForm, Long houseId){\n    List<HousePicture> pictures = new ArrayList<>();\n    if(houseForm.getPhotos() ==null || houseForm.getPhotos().isEmpty()){\n        return pictures;\n    }\n    for (PhotoForm photo : houseForm.getPhotos()) {\n        HousePicture housePicture = new HousePicture();\n        housePicture.setHouseId(houseId);\n        housePicture.setCdnPrefix(cdnPrefix);\n        housePicture.setPath(photo.getPath());\n        housePicture.setWidth(photo.getWidth());\n        housePicture.setHeight(photo.getHeight());\n        pictures.add(housePicture);\n    }\n    return pictures;\n\n}\n```\n\n其中通过houseForm前端传递的参数获取到photos，然后循环，新建一个HousePicture的List放进去，最后返回出来。然后：\n\n```java\nIterable<HousePicture> housePictures = housePictureRepository.save(pictures);\n```\n\n\n\n 另外返回的都是存储着实体类的List，我们要对其转换成DTO类：\n\n```java\n//使用modelMapper映射到DTO\nHouseDTO houseDTO = modelMapper.map(house,HouseDTO.class);\nHouseDetailDTO houseDetailDTO = modelMapper.map(houseDetail, HouseDetailDTO.class);\n\nhouseDTO.setHouseDetail(houseDetailDTO);\nList<HousePictureDTO> pictureDTOS  = new ArrayList<>();\nhousePictures.forEach(housePicture -> pictureDTOS.add(modelMapper.map(housePicture,HousePictureDTO.class)));\nhouseDTO.setPictures(pictureDTOS);\nhouseDTO.setCover(this.cdnPrefix + houseDTO.getCover());\n```\n\n其中使用到了peoperties文件中定义的cdnPrefix变量\n\n```java\n@Value(\"${qiniu.cdn.prefix}\")\nprivate String cdnPrefix;\n```\n\n另外呢还需要得到前端点击标记的标签，然后存储在houseTagRespository中，最后都在houseDTO类中去定义类型。\n\n最后的最后；\n\nreturn 一个我们定义好的ServiceResult类：\n\n```java\nreturn new ServiceResult<HouseDTO>(true,null,houseDTO);\n```\n\n\n\n**测试插入数据：**\n\n![image-20220816205011825](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h58wcr1rl6j21410doq5v.jpg)\n\n\n\n## 二十一、房源信息浏览功能(1)\n\n### 1）基础开发\n\n### 2）分页实现\n\n### 3）多维度排序\n\n\n\n![image-20220817112443451](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h59lmo8693j21it0u0gop.jpg)\n\n每次刷新后会出现页面重复情况，这种情况出现的原因在于：每次项目呢session会存在内存里面，每次重启呢项目内存就会丢失，这样session就过期了，这样为了保证每次登陆都不受此情况烦恼，所以**提出了使用redis来保存我们的对话信息**。\n\n> 我们怎么去实现呢？\n\n\n\n### 1、加一个RedisSessionConfig配置\n\n```java\n@Configuration\n@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 86400)\npublic class RedisSessionConfig {\n\t\n}\n```\n\n添加配置类注解、\n\n添加EnableRedisHttpSession，让Springboot自动为我们配置redisSession服务。\n\n并且设置其生效时间为一天86400秒（(maxInactiveIntervalInSeconds = 86400)）\n\n```java\n@Configuration\n@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 86400)\npublic class RedisSessionConfig {\n    @Bean\n    public RedisTemplate<String,String> redisTemplate(RedisConnectionFactory factory){\n        return new StringRedisTemplate(factory);\n    }\n}\n```\n\n当然也可以自己进行手动的配置：\n\n```properties\n# redis config\nspring.redis.database=0\nspring.redis.host=localhost\nspring.redis.port=6379\nspring.redis.pool.min-idle=1\nspring.redis.timeout=3000\n```\n\n另外呢之前存储缓存在hash_map下也就是在本机内存中，现在需要修改成：\n\n```properties\n# session会话存储类型\nspring.session.store-type = redis\n```\n\n\n\n但是在上面写localhost就出错了，应该调整为127.0.0.1\n\n```properties\n# redis config\nspring.redis.database=0\nspring.redis.host=127.0.0.1\nspring.redis.port=6379\nspring.redis.pool.min-idle=1\nspring.redis.timeout=3000\n```\n\n然后这样呢在登录的时候就会将session自动的存储到redis中了：\n\n![image-20220817142440167](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h59qtvrkmvj20mp043mxn.jpg)\n\n但是这些session是怎么被写进redis中的呢？什么时候写进去的呢？\n\n猜想是在RedisSessionConfig的时候后台自动对session进行的操作。\n\n另外在pom.xml 中添加的依赖是：\n\n```xml\n<dependency>\n    <groupId>org.springframework.session</groupId>\n    <artifactId>spring-session-data-redis</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.session</groupId>\n    <artifactId>spring-session</artifactId>\n</dependency>\n```\n\n> 关于spring-session的更多详细文章见：\n>\n> https://www.cnblogs.com/54chensongxia/p/12096493.html\n\n\n\n在房屋列表页面中我们使用的dataTables这个插件，对请求的接口有一些特殊的要求。\n\n```java\n@PostMapping(\"admin/houses\")\n@ResponseBody\npublic ApiResponse\n```\n\n  但是我们就不能用ApiResponse了，因为dataTables有一个固定的格式。\n\n![image-20220817171008284](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h59vm230kbj21wj0u043e.jpg)\n\n所以我们需要单独给它定制一个格式类：\n\n```java\n/**\n * Datatables响应结构\n */\npublic class ApiDataTableResponse extends ApiResponse{\n    private int draw;\n    private Long recordsTotal;\n    private Long recordsFiltered;\n\n    public ApiDataTableResponse(int code, String message, Object data, int draw, Long recordsTotal, Long recordsFiltered) {\n        super(code, message, data);\n    }\n\n    public int getDraw() {\n        return draw;\n    }\n\n    public void setDraw(int draw) {\n        this.draw = draw;\n    }\n\n    public Long getRecordsTotal() {\n        return recordsTotal;\n    }\n\n    public void setRecordsTotal(Long recordsTotal) {\n        this.recordsTotal = recordsTotal;\n    }\n\n    public Long getRecordsFiltered() {\n        return recordsFiltered;\n    }\n\n    public void setRecordsFiltered(Long recordsFiltered) {\n        this.recordsFiltered = recordsFiltered;\n    }\n}\n```\n\n然后就可以在controller中进行定义了：\n\n```java\n@PostMapping(\"admin/houses\")\n@ResponseBody\npublic ApiDataTableResponse houses(){\n    \n}\n```\n\n 但是我们需要去定义一个dataTables 的表单类：\n\n```java\npublic class DataTableSearch {\n\n    /**\n     * Datatables要求回显字段\n     */\n    private int draw;\n\n    /**\n     * Datatables规定分页字段\n     */\n    private int start;\n    private int length;\n\n    private Integer status;\n\n    @DateTimeFormat(pattern = \"yyyy-MM-dd\")\n    private Date createTimeMin;\n    @DateTimeFormat(pattern = \"yyyy-MM-dd\")\n    private Date createTimeMax;\n\n    private String city;\n    private String title;\n    private String direction;\n    private String orderBy;\n\n    public int getDraw() {\n        return draw;\n    }\n\n    public void setDraw(int draw) {\n        this.draw = draw;\n    }\n\n    public int getStart() {\n        return start;\n    }\n\n    public void setStart(int start) {\n        this.start = start;\n    }\n\n    public int getLength() {\n        return length;\n    }\n\n    public void setLength(int length) {\n        this.length = length;\n    }\n\n    public Integer getStatus() {\n        return status;\n    }\n\n    public void setStatus(Integer status) {\n        this.status = status;\n    }\n\n    public Date getCreateTimeMin() {\n        return createTimeMin;\n    }\n\n    public void setCreateTimeMin(Date createTimeMin) {\n        this.createTimeMin = createTimeMin;\n    }\n\n    public Date getCreateTimeMax() {\n        return createTimeMax;\n    }\n\n    public void setCreateTimeMax(Date createTimeMax) {\n        this.createTimeMax = createTimeMax;\n    }\n\n    public String getCity() {\n        return city;\n    }\n\n    public void setCity(String city) {\n        this.city = city;\n    }\n\n    public String getTitle() {\n        return title;\n    }\n\n    public void setTitle(String title) {\n        this.title = title;\n    }\n\n    public String getDirection() {\n        return direction;\n    }\n\n    public void setDirection(String direction) {\n        this.direction = direction;\n    }\n\n    public String getOrderBy() {\n        return orderBy;\n    }\n\n    public void setOrderBy(String orderBy) {\n        this.orderBy = orderBy;\n    }\n}\n```\n\n其中，关于start、length都定义的是int类型，但是关于status定义的是Integer类型，这里定义了一个小技巧，Integer当为空的时候，代表返回所有的状态码。\n\n```java\n@DateTimeFormat(pattern = \"yyyy-MM-dd\")\nprivate Date createTimeMin;\n```\n\nDateTimeFormat去定义了字段的格式校验。\n\n\n\n定义好了返回类之后，返回到controller中，我们定义@ModelAttribut固定化返回的结构DataTableSearch.\n\n```java\n@PostMapping(\"admin/houses\")\n@ResponseBody\npublic ApiDataTableResponse houses(@ModelAttribute DataTableSearch SearchBody){\n        return null;\n}\n```\n\n我们先暂时返回null。\n\n这时候我们去定义service：\n\n```java\nServiceMultiResult<HouseDTO> adminQuery(DataTableSearch searchBody);\n```\n\n在serviceImpl中定义：\n\n```java\n@Override\npublic ServiceMultiResult<HouseDTO> adminQuery(DataTableSearch searchBody) {\n    List<HouseDTO> houseDTOS = new ArrayList<>();\n    Iterable<House> houses = houseRepository.findAll();\n    houses.forEach(house -> {\n        HouseDTO houseDTO = modelMapper.map(house,HouseDTO.class);\n        houseDTO.setCover(this.cdnPrefix + house.getCover());\n        houseDTOS.add(houseDTO);\n    });\n    return new ServiceMultiResult<>(houseDTOS.size(),houseDTOS);\n}\n```\n\n上面我们只定义了findAll，并没有用到searchBody做一些比较详细的查找，会在后续中进行查找。\n\n\n\nserviceImpl实现了之后我们就返回去去整理controller。\n\n```java\nServiceMultiResult<HouseDTO> result = houseService.adminQuery(SearchBody);\n```\n\n```java\n@PostMapping(\"admin/houses\")\n@ResponseBody\npublic ApiDataTableResponse houses(@ModelAttribute DataTableSearch SearchBody){}\n```\n\n因为我们返回的是ApiDataTableRespose类型，所以我们需要做一个封装:\n\n```java\nApiDataTableResponse apiDataTableResponse = new ApiDataTableResponse(ApiResponse.Status.SUCCESS);\n```\n\n然后给ApiDataTableResponese去定义一个构造函数：\n\n```java\npublic ApiDataTableResponse(ApiResponse.Status status){\n    this(status.getCode(),status.getStandardMessage(),null);\n}\n```\n\n\n\n最后我们定义的controller如下：\n\n```java\n@PostMapping(\"admin/houses\")\n@ResponseBody\npublic ApiDataTableResponse houses(@ModelAttribute DataTableSearch searchBody){\n    ServiceMultiResult<HouseDTO> result = houseService.adminQuery(searchBody);\n    ApiDataTableResponse response = new ApiDataTableResponse(ApiResponse.Status.SUCCESS);\n    response.setData(result.getResult());\n    response.setRecordsFiltered(result.getTotal());\n    response.setRecordsTotal(result.getTotal());\n\n    //这个字段需要回显，所以我们从searchBody中获取，防伪的一个验证\n    response.setDraw(searchBody.getDraw());\n    return response;\n}\n```\n\n返回的结果界面如下：\n\n\n\n![image-20220822173105778](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5fodvfaa7j21qs0u0zpj.jpg)\t\n\n\n\n但是这里我们方法是findAll，另外分页是在前端做的，如果数据量很多的话，很容易就会请求接口的时候把内存撑爆。所以我们不仅要有前端的分页还得要有后端的分页。\n\n## 二十二、房源信息浏览功能(2)\n\n我们来实现分页，实现基本的分页和基本的排序，这时候就用到了我们controller中的参数searchBody了。\n\n之前我们定义的事House Repository继承的是CrudRepository，使用的是findAll方法，用到分页所以我们就不能继承这个了。\n\n```java\npublic interface HouseRepository extends PagingAndSortingRepository<House,Long> {\n}\n```\n\n所以我们重构了Repository方法后，又重新定义了ServiceImpl方法类：\n\n```java\n@Override\npublic ServiceMultiResult<HouseDTO> adminQuery(DataTableSearch searchBody) {\n    List<HouseDTO> houseDTOS = new ArrayList<>();\n    //新建排序类\n    Sort sort = new Sort(Sort.Direction.fromString(searchBody.getDirection()),searchBody.getOrderBy());\n    int page = searchBody.getStart() / searchBody.getLength() ;\n\n    Pageable pageable = new PageRequest(page,searchBody.getLength(),sort);\n    Page<House> houses = houseRepository.findAll(pageable);\n\n    //以前的方法就不能用了 Iterable<House> houses = houseRepository.findAll();\n    houses.forEach(house -> {\n        HouseDTO houseDTO = modelMapper.map(house,HouseDTO.class);\n        houseDTO.setCover(this.cdnPrefix + house.getCover());\n        houseDTOS.add(houseDTO);\n    });\n    return new ServiceMultiResult<>(houses.getTotalElements(),houseDTOS);\n}\n```\n\n 首先要新建一个排序类：\n\nnew Sort() ，里面需要传的第一个参数是排序的方式（正排还是倒排），需要用Sort.Direction.fromString来进行转换。第二个参数是根据什么字段来进行排序，也就是GetOrderBy，\n\n\n\n然后再去定义page，固定的公式就是：\n\n```java\nint page = searchBody.getStart() / searchBody.getLength() ;\n```\n\n然后新建一个Pageable类，需要传递的参数就是page，searchBody.getLength()，sort类，然后把返回的pageable类使用PagingAndSortingRepository的findAll(**Pageable**)重构方法。\n\n另外呢需要调整的地方还有：\n\n```java\nreturn new ServiceMultiResult<>(houses.getTotalElements(),houseDTOS);\n```\n\n之前是houseDTOS.size()，现在是返回结果houses.getTotalElements()。**这里因为使用的是findAll，所以getTotalElements返回的是查询的所有的数量总和**\n\n------------\n\n另外发现：在前端页面中，配置都定义在了：\n\n```javascript\n// 【关键】数据显示控制 服务器分页\nvar table = $('#data-table').DataTable({\n    \"order\": [[7, \"desc\"]],//默认创建时间排序\n    \"pageLength\": 3, // 配置单页显示条数\n    \"paging\": true, // 关闭本地分页\n    \"lengthChange\": false, // 不允许用户改变表格每页显示的记录数\n    \"searching\": false, // 不允许Datatables开启本地搜索\n    \"ordering\": true, // 启用Datatables排序\n    \"info\": true, // 表格左边显示搜索信息\n    \"autoWidth\": true, // 自动计算表格宽度\n    \"stateSave\": false, // 允许表格缓存Datatables，以便下次恢复之前的状态\n    \"retrieve\": true, // 如果已经初始化了，则继续使用之前的Datatables实例\n    \"processing\": true, // 显示正在处理的状态\n    \"serverSide\": true, // 服务器模式，数据由服务器掌控\n    \"pagingType\": \"simple_numbers\", // 翻页显示: 上一页和下一页两个按钮，加上页数按钮\n    \"language\": {\n        \"sProcessing\": \"处理中...\",\n        \"sLengthMenu\": \"显示 _MENU_ 项结果\",\n        \"sZeroRecords\": \"没有匹配结果\",\n        \"sInfo\": \"显示第 _START_ 至 _END_ 项结果，共 _TOTAL_ 项\",\n        \"sInfoEmpty\": \"显示第 0 至 0 项结果，共 0 项\",\n        \"sInfoFiltered\": \"(由 _MAX_ 项结果过滤)\",\n        \"sInfoPostFix\": \"\",\n        \"sUrl\": \"\",\n        \"sEmptyTable\": \"未搜索到数据\",\n        \"sLoadingRecords\": \"载入中...\",\n        \"sInfoThousands\": \",\",\n        \"oPaginate\": {\n```\n\n这里，所以在前端发送ajax请求的时候就会把这些预设好的参数传到服务器url。\n\n![image-20220823145625654](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5gpgt741zj20u60e3mzd.jpg)\n\n定义的Sort类，实际传入的值是：createTime: DESC\n\n定义的Pageable类，实际传入的值是：Page request [number: 0, size 3, sort: createTime: DESC]\n\n> 这里对于前端的技术代码只做了浏览，并未对其进行详细阅读，至于说熟练掌握，那还需要更多时间去了解。\n\n\n\n这里的翻页，就是使用服务器来完成的。然后使用前端html按钮来发送请求。\n\n![image-20220823151109688](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5gpw3v4g4j22c00tqwjm.jpg)\n\n## 二十三、房源信息浏览功能(3)\n\n\n\n![image-20220823151240457](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5gpxohml2j21rk0pejvq.jpg)\n\n目前的筛选条件等等只能通过表格的字段进行升序、降序的筛选，而不能通过上面城市、房源、创建时间、标题等进行搜索筛选，本节就解决这个问题。\n\n所以我们在repository类中增加：\n\n```java\npublic interface HouseRepository extends PagingAndSortingRepository<House,Long>, JpaSpecificationExecutor<House> {\n}\n```\n\n这样不仅可以实现分页，也可以实现对指定内容的查询。\n\n然后需要在ServiceImpl中定义一个Specification。\n\n> spring data jpa为我们提供了JpaSpecificationExecutor接口，只要简单实现toPredicate方法就可以实现复杂的查询。JpaSpecification查询的关键在于怎么构建Predicates。\n\n```java\n@Override\npublic ServiceMultiResult<HouseDTO> adminQuery(DataTableSearch searchBody) {\n    List<HouseDTO> houseDTOS = new ArrayList<>();\n    //新建排序类\n    Sort sort = new Sort(Sort.Direction.fromString(searchBody.getDirection()),searchBody.getOrderBy());\n    int page = searchBody.getStart() / searchBody.getLength() ;\n\n    Pageable pageable = new PageRequest(page,searchBody.getLength(),sort);\n\n\n    Specification<House> specification = new Specification<House>() {\n        @Override\n        public Predicate toPredicate(Root root, CriteriaQuery query, CriteriaBuilder cb) {\n            return null;\n        }\n    }\n```\n\n这里我们使用java8的lambda形式来定义：\n\n``` java\n@Override\npublic ServiceMultiResult<HouseDTO> adminQuery(DataTableSearch searchBody) {\n    List<HouseDTO> houseDTOS = new ArrayList<>();\n    //新建排序类\n    Sort sort = new Sort(Sort.Direction.fromString(searchBody.getDirection()),searchBody.getOrderBy());\n    int page = searchBody.getStart() / searchBody.getLength() ;\n\n    Pageable pageable = new PageRequest(page,searchBody.getLength(),sort);\n\n    Specification<House> specification = (root, query,cb) ->{\n        Predicate predicate = cb.equal((root.get(\"adminId\"),LoginUserUtil.getUserId()));\n        predicate = cb.and(predicate,cb.notEqual(root.get(\"status\"), HouseStatus.DELETED.getValue()));\n\n        if(searchBody.getCity() !=null){\n            predicate = cb.and(predicate, cb.equal(root.get(\"cityEnName\"), searchBody.getCity()));\n        }\n\n        if (searchBody.getStatus() !=null){\n            predicate = cb.and(predicate, cb.equal(root.get(\"status\"),searchBody.getStatus()));\n        }\n\n        if(searchBody.getCreateTimeMin() !=null){\n            predicate = cb.and(predicate,cb.greaterThanOrEqualTo(root.get(\"createTime\"),searchBody.getCreateTimeMin()));\n        }\n\n        if(searchBody.getCreateTimeMax() !=null){\n            predicate = cb.and(predicate,cb.lessThanOrEqualTo(root.get(\"createTime\"),searchBody.getCreateTimeMax()));\n        }\n\n        if(searchBody.getTitle() !=null){\n            predicate = cb.and(predicate, cb.like(root.get(\"title\"),\"%\"+searchBody.getTitle()+\"%\"));\n        }\n\n        return predicate;\n\n    };\n```\n\n> root.get(\"country\")，对应着实体类的相关属性country，root.get属性对应的实体类，我认为从使用该spec的Repository对应的实体类相对应，比如此处的playerRepo对应的实体类PlayerEntity，实际上如果属性对不上，运行时会报错。root.get取得相应实体的操作字段。\n>\n> 而criteriaBuilder.equal和criteriaBuilder.and，则是用来构建复杂查询\n>\n> criteriaBuilder.add 或者 criteriaBuilder.or等方法的返回值也为Predicate对象。\n\n如果添加一些搜索条件找不到的话，显示结果图如下所示：\n\n![image-20220823172508134](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5gtriepryj22bc0kcad6.jpg)\n\n## 二十四、编辑功能实现1\n\n```java\n/**\n * 房源信息编辑页\n */\n\n@GetMapping(\"admin/house/edit\")\npublic String houseEditPage(@RequestParam(value = \"id\") Long id, Model model){\n\n    if(id == null || id <1){\n        return \"404\";\n    }\n\n    return \"admin/house-edit\";\n}\n```\n\n在controller层中新家房源信息编辑代码，然后在serviceImpl中去新建一个service接口\n\n\n\n```java\n/**\n * 服务接口通用结构\n */\npublic class ServiceResult<T> {\n    private boolean success;\n    private String message;\n    private T result;\n\n    public ServiceResult(boolean success) {\n        this.success = success;\n    }\n\n    public ServiceResult(boolean success, String message) {\n        this.success = success;\n        this.message = message;\n    }\n\n    public ServiceResult(boolean success, String message, T result) {\n        this.success = success;\n        this.message = message;\n        this.result = result;\n    }\n\n    public boolean isSuccess() {\n        return success;\n    }\n\n    public void setSuccess(boolean success) {\n        this.success = success;\n    }\n\n    public String getMessage() {\n        return message;\n    }\n\n    public void setMessage(String message) {\n        this.message = message;\n    }\n\n    public T getResult() {\n        return result;\n    }\n\n    public void setResult(T result) {\n        this.result = result;\n    }\n\n    public static <T> ServiceResult<T> success() {\n        return new ServiceResult<>(true);\n    }\n\n    public static <T> ServiceResult<T> of(T result) {\n        ServiceResult<T> serviceResult = new ServiceResult<>(true);\n        serviceResult.setResult(result);\n        return serviceResult;\n    }\n\n    public static <T> ServiceResult<T> notFound() {\n        return new ServiceResult<>(false, Message.NOT_FOUND.getValue());\n    }\n\n    public enum Message {\n        NOT_FOUND(\"Not Found Resource!\"),\n        NOT_LOGIN(\"User not login!\");\n\n        private String value;\n\n        Message(String value) {\n            this.value = value;\n        }\n\n        public String getValue() {\n            return value;\n        }\n    }\n}\n```\n\n在serviceImpl中去定义：\n\n```java\n@Override\npublic ServiceResult<HouseDTO> findCompleteOne(Long id) {\n    House house = houseRepository.findOne(id);\n\n    if(house ==null){\n        return ServiceResult.notFound();\n    }\n\n    HouseDetail detail;\n    \n    List<HouseTag> tags;\n    \n    return null;\n}\n```\n\n因为需要获取到完整的house信息，上面代码部分我们不仅要查询house，还要查询houseDetail、housetag（注意这里HouseTag是List类型）\n\n\n\n```java\n@Override\npublic ServiceResult<HouseDTO> findCompleteOne(Long id) {\n    House house = houseRepository.findOne(id);\n\n    if(house ==null){\n        return ServiceResult.notFound();\n    }\n\n    HouseDetail detail = houseDetailRepository.findOne(id);\n    List<HousePicture> housePictures = housePictureRepository.findAllByhouseId(id);\n\n\n    //需要把do转换成dto\n    HouseDetailDTO houseDetailDTO =  modelMapper.map(detail,HouseDetailDTO.class);\n    List<HousePictureDTO> pictureDTOS = new ArrayList<>();\n    for (HousePicture housePicture : housePictures) {\n        HousePictureDTO housePictureDTO  = modelMapper.map(housePicture,HousePictureDTO.class);\n        pictureDTOS.add(housePictureDTO);\n    }\n    List<HouseTag> tags = houseTagRepository.findAllById(id);\n\n    List<String> tagList = new ArrayList<>();\n    for (HouseTag tag : tags) {\n        tagList.add(tag.getName());\n\n    }\n\n    HouseDTO result = modelMapper.map(house, HouseDTO.class);\n    result.setHouseDetail(houseDetailDTO);\n    result.setPictures(pictureDTOS);\n    result.setTags(tagList);\n    ServiceResult<HouseDTO> serviceResult = ServiceResult.of(result);\n    return serviceResult;\n}\n```\n\n都是一些查询的操作，没有什么好纪录的了，不过有一个类倒是需要纪录一下：\n\n```java\npackage com.zryy.soufangtest.service;\n\n/**\n * 服务接口通用结构\n */\npublic class ServiceResult<T> {\n    private boolean success;\n    private String message;\n    private T result;\n\n    public ServiceResult(boolean success) {\n        this.success = success;\n    }\n\n    public ServiceResult(boolean success, String message) {\n        this.success = success;\n        this.message = message;\n    }\n\n    public ServiceResult(boolean success, String message, T result) {\n        this.success = success;\n        this.message = message;\n        this.result = result;\n    }\n\n    public boolean isSuccess() {\n        return success;\n    }\n\n    public void setSuccess(boolean success) {\n        this.success = success;\n    }\n\n    public String getMessage() {\n        return message;\n    }\n\n    public void setMessage(String message) {\n        this.message = message;\n    }\n\n    public T getResult() {\n        return result;\n    }\n\n    public void setResult(T result) {\n        this.result = result;\n    }\n\n    public static <T> ServiceResult<T> success() {\n        return new ServiceResult<>(true);\n    }\n\n    public static <T> ServiceResult<T> of(T result) {\n        ServiceResult<T> serviceResult = new ServiceResult<>(true);\n        serviceResult.setResult(result);\n        return serviceResult;\n    }\n\n    public static <T> ServiceResult<T> notFound() {\n        return new ServiceResult<>(false, Message.NOT_FOUND.getValue());\n    }\n\n    public enum Message {\n        NOT_FOUND(\"Not Found Resource!\"),\n        NOT_LOGIN(\"User not login!\");\n\n        private String value;\n\n        Message(String value) {\n            this.value = value;\n        }\n\n        public String getValue() {\n            return value;\n        }\n    }\n}\n```\n\n**这个封装好的返回类的of方法**\n\n```java\npublic static <T> ServiceResult<T> of(T result) {\n        ServiceResult<T> serviceResult = new ServiceResult<>(true);\n        serviceResult.setResult(result);\n        return serviceResult;\n    }\n```\n\n**ServiceResult<T>这里不是很理解，为什么后面加一个范型类T**\n\n\n\n最后测试，点击编辑按钮，返回页面：\n\n![image-20220824204543117](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5i56k81zbj21o90u0n05.jpg)\n\n![image-20220824204607629](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5i56yb8igj21rz0u0q72.jpg)\n\n\n\n但是我们点击更新的时候会报如下错误：\n\n![image-20220824205928885](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5i5ku36fmj21km0rw0v3.jpg)\n\n**我们重新定义@GetMapping(\"admin/house/edit\")**\n\n**使用post请求，即重载**\n\n```java\n@PostMapping(\"admin/house/edit\")\n@ResponseBody\npublic ApiResponse saveHouse(@Valid @ModelAttribute(\"form-house-edit\") HouseForm houseForm, BindingResult bindingResult){\n\n}\n```\n\n我们定义了@valid验证注解 并设置model参数为form-house-edit，还有设定一个绑定的变量参数bindingResult。\n\n并在IHouseService中定义updae接口：\n\n```java\n/**\n * 修改\n * @param houseForm\n * @return\n */\nServiceResult update(HouseForm houseForm);\n```\n\n因为是修改更新，所以我们不同于add一样需要验证，所以我们不需要进行返回参数。\n\n```java\n@Override\n@Transactional\npublic ServiceResult<HouseDTO> update(HouseForm houseForm) {\n    House house = houseRepository.findOne(houseForm.getId());\n    if(house == null){\n        return ServiceResult.notFound();\n    }\n    HouseDetail houseDetail = houseDetailRepository.findOne(houseForm.getId());\n    if(houseDetail == null){\n        return ServiceResult.notFound();\n    }\n\n    //围绕houseDetail做一些属性的设置，将地铁以及地铁站的信息设置进去。\n    ServiceResult wrapperResult =  wrapperDetailInfo(houseDetail,houseForm);\n    //因为wraperResult正常返回的就是null，如果不返回null，就直接返回，说明有问题。\n    if (wrapperResult != null){\n        return wrapperResult;\n    }\n\n    //另外只要是有save、update、delete操作都要在接口上加上@Transactional事务操作，另外有jpa的原因，如果不加事务，是不会让这段代码去运行的\n    houseDetailRepository.save(houseDetail);\n\n    //另外呢除了Detail，Picture也是需要进行保存的\n    List<HousePicture> pictures = generatePictures(houseForm, houseDetail.getHouseId());\n    housePictureRepository.save(pictures);\n\n    //因为有时候cover并不能很及时的返回，所以这里设置重新加载一次设置，保存的时候不要丢掉数据\n    if(houseForm.getCover() == null){\n        houseForm.setCover(house.getCover());\n    }\n\n    //然后把houseform内容映射到house中。为什么？是因为只有houseform增加了setCover吗？那为什么不直接去增加house呢？\n    //原来是houseForm是最新的数据，而houseDetail存储的是根据id查询出来的历史的未修改的地铁相关数据。\n    //在上面houseDetail这里已经将修改好的地铁详情信息存储了，这里再对主体的house信息进行entity的映射。当然map并非一对一，是有对应的变量就对应过去。有些对应不上的（detail类的信息）就不会映射过去。\n    modelMapper.map(houseForm,house);\n    //另外有个字段需要重新设置一下\n    house.setLastUpdateTime(new Date());\n    houseRepository.save(house);\n\n\n\n    return ServiceResult.success();\n}\n```\n\n最后去编辑controller层：\n\n```java\n/**\n * 编辑接口，更新按钮的重载\n */\n\n@PostMapping(\"admin/house/edit\")\n@ResponseBody\npublic ApiResponse saveHouse(@Valid @ModelAttribute(\"form-house-edit\") HouseForm houseForm, BindingResult bindingResult){\n    //首先要判断绑定的参数有没有问题\n    if(bindingResult.hasErrors()){\n        return new ApiResponse(HttpStatus.BAD_REQUEST.value(), bindingResult.getAllErrors().get(0).getDefaultMessage(),null);\n    }\n\n    Map<SupportAddress.Level, SupportAddressDTO> cityAndRegion = addressService.findCityAndRegion(houseForm.getCityEnName(), houseForm.getRegionEnName());\n    //如果返回的地址城市city和Regin两个，就没有问题\n    if(cityAndRegion.size() !=2 ){\n        return ApiResponse.ofSuccess(ApiResponse.Status.NOT_VALID_PARAM);\n    }\n\n    //对houseForm表单进行更新操作\n    ServiceResult result = houseService.update(houseForm);\n    if(result.isSuccess()){\n        //更新成功并不需要返回什么数据\n        return ApiResponse.ofSuccess(null);\n    }\n    //如果不成功的话就返回bad——request\n    ApiResponse apiResponse = ApiResponse.ofStatus(ApiResponse.Status.BAD_REQUEST);\n    apiResponse.setMessage(result.getMessage());\n    return apiResponse;\n}\n```\n\n## 二十五、编辑功能实现2\n\n\n\n虽然上节中我们实现了更新的操作，但是对于房源标签修改，移除图片等操作还没有具体实现。\n\n```java\n/**\n * 移除图片\n * @param id\n * @return\n */\nServiceResult removePhoto(Long id );\nServiceResult addTag(Long houseId, String tag);\nServiceResult updateCover(Long coverId, Long targetId);\n```\n\n然后在serviceImpl实现：\n\n```java\n@Override\npublic ServiceResult removePhoto(Long id) {\n    return null;\n}\n```\n\n暂列返回为null。\n\n然后定义修改封面接口、移除图片接口、增加标签接口的实现：\n\n```java\n/**\n * 移除图片接口\n * @param id\n * @return\n */\n@DeleteMapping(\"admin/house/photo\")\n@ResponseBody\npublic ApiResponse removeHousePhoto(@RequestParam(value = \"id\") Long id) {\n    ServiceResult result = this.houseService.removePhoto(id);\n\n    if (result.isSuccess()) {\n        return ApiResponse.ofStatus(ApiResponse.Status.SUCCESS);\n    } else {\n        return ApiResponse.ofMessage(HttpStatus.BAD_REQUEST.value(), result.getMessage());\n    }\n}\n```\n\n```java\n/**\n * 修改封面接口\n * @param coverId\n * @param targetId\n * @return\n */\n@PostMapping(\"admin/house/cover\")\n@ResponseBody\npublic ApiResponse updateCover(@RequestParam(value = \"cover_id\") Long coverId,\n                               @RequestParam(value = \"target_id\") Long targetId) {\n    ServiceResult result = this.houseService.updateCover(coverId, targetId);\n\n    if (result.isSuccess()) {\n        return ApiResponse.ofStatus(ApiResponse.Status.SUCCESS);\n    } else {\n        return ApiResponse.ofMessage(HttpStatus.BAD_REQUEST.value(), result.getMessage());\n    }\n}\n```\n\n```java\n/**\n * 增加标签接口\n * @param houseId\n * @param tag\n * @return\n */\n@PostMapping(\"admin/house/tag\")\n@ResponseBody\npublic ApiResponse addHouseTag(@RequestParam(value = \"house_id\") Long houseId,\n                               @RequestParam(value = \"tag\") String tag) {\n    if (houseId < 1 || Strings.isNullOrEmpty(tag)) {\n        return ApiResponse.ofStatus(ApiResponse.Status.BAD_REQUEST);\n    }\n\n    ServiceResult result = this.houseService.addTag(houseId, tag);\n    if (result.isSuccess()) {\n        return ApiResponse.ofStatus(ApiResponse.Status.SUCCESS);\n    } else {\n        return ApiResponse.ofMessage(HttpStatus.BAD_REQUEST.value(), result.getMessage());\n    }\n}\n```\n\n```java\n/**\n * 移除标签接口\n * @param houseId\n * @param tag\n * @return\n */\n@DeleteMapping(\"admin/house/tag\")\n@ResponseBody\npublic ApiResponse removeHouseTag(@RequestParam(value = \"house_id\") Long houseId,\n                                  @RequestParam(value = \"tag\") String tag) {\n    if (houseId < 1 || Strings.isNullOrEmpty(tag)) {\n        return ApiResponse.ofStatus(ApiResponse.Status.BAD_REQUEST);\n    }\n\n    ServiceResult result = this.houseService.removeTag(houseId, tag);\n    if (result.isSuccess()) {\n        return ApiResponse.ofStatus(ApiResponse.Status.SUCCESS);\n    } else {\n        return ApiResponse.ofMessage(HttpStatus.BAD_REQUEST.value(), result.getMessage());\n    }\n}\n```\n\n然后就到service实现类中去实现移除图片具体逻辑：\n\n```java\n@Override\npublic ServiceResult removePhoto(Long id) {\n    //首先要去判断photo的id，万一有人冒充该接口删除了一些不该删除的照片就会导致错误\n    HousePicture picture = housePictureRepository.findOne(id);\n    if(picture == null){\n        return ServiceResult.notFound();\n    }\n    Response response = null;\n    try {\n        response = iQiNiuService.delete(picture.getPath());\n        //也要判断七牛返回的删除接口是否是ok的状态\n        if(response.isOK()){\n            return ServiceResult.success();\n        } else {\n           //如果删除异常的话，也返回七牛发给我们的error信息\n           return new ServiceResult(false,response.error);\n        }\n    } catch (QiniuException e) {\n        e.printStackTrace();\n        return new ServiceResult(false,e.getMessage());\n    }\n\n}\n```\n\n> 另外发现一个问题缺陷\n\n在房源编辑的时候，上传完毕图片，在封面设置那里是一个错误的视图。所以找了半天，发现在upload.js文件下有一个设置封面图片的地方，需要拼接src+Photopath，需要修改成在七牛云申请的域名。\n\n```js\n$(\"#upload-cover-container\").append(\n    '<div style=\"float: left; margin: 2px; padding: 2px; border: 1px dashed; width:' +\n    ' 120px; height: 100px;\">' +\n    '<span><img src=\"http://rge19896q.hb-bkt.clouddn.com//' +\n    photo_path + '?imageView2/1/w/100/h/100\" title=\"待选封面\" />' +\n    '<input style=\"margin-left: 5px;\" type=\"radio\" name=\"cover\" value=\"' +\n    photo_path + '\"/></span></div>'\n);\t\t\n```\n\n才可以正常显示，显示成功的图如下所示：\n\n![image-20220825162919557](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5j3e1z84cj20u70ratah.jpg)\n\nservice实现类中去实现移除图片具体逻辑后再去实现updateCover的具体逻辑：\n\n```java\n@Override\n@Transactional\npublic ServiceResult updateCover(Long coverId, Long targetId) {\n    HousePicture cover = housePictureRepository.findOne(coverId);\n    if (cover == null) {\n        return ServiceResult.notFound();\n    }\n    houseRepository.updateCover(targetId, cover.getPath());\n    return null;\n}\n```\n\n然后在repository中去定义增删改查的语句，注意：\n\n要增加@Modify注解，告诉其这是一个修改的方法。\n\n要增加@Query注解，自定义一些crud的方法：\n\n```java\npublic interface HouseRepository extends PagingAndSortingRepository<House,Long>, JpaSpecificationExecutor<House> {\n    @Modifying\n    @Query(value =\"update House as house set house.cover = :cover where house.id = :id\" )\n    void updateCover(@Param(value = \"id\") Long id, @Param(value = \"cover\") String cover);\n}\n```\n\n**其中关于jpa的语法需要更多的去学习了解**\n\n\n\n然后返回写serviceImpl类：\n\n``` java\n@Override\n@Transactional\npublic ServiceResult updateCover(Long coverId, Long targetId) {\n    HousePicture cover = housePictureRepository.findOne(coverId);\n    if (cover == null) {\n        return ServiceResult.notFound();\n    }\n    houseRepository.updateCover(targetId, cover.getPath());\n    return ServiceResult.success();\n}\n```\n\n因为是update操作，所以不需要返回值。但是这里似乎并没有添加什么校验。\n\n效果图：\n\n![image-20220825172443129](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5j4zosgsqj21ki0u0gnz.jpg)\n\n![image-20220825172455963](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5j4zwf075j22580r8jvc.jpg)\n\n另外呢还需要添加增加标签和删除标签serviceImpl\n\n```java\n@Override\n@Transactional\npublic ServiceResult addTag(Long houseId, String tag) {\n    House house = houseRepository.findOne(houseId);\n    if (house == null) {\n        return ServiceResult.notFound();\n    }\n\n    HouseTag houseTag = houseTagRepository.findByNameAndHouseId(tag, houseId);\n    if (houseTag != null) {\n        return new ServiceResult(false, \"标签已存在\");\n    }\n\n    houseTagRepository.save(new HouseTag(houseId, tag));\n    return ServiceResult.success();\n}\n```\n\n```java\n@Override\n@Transactional\npublic ServiceResult removeTag(Long houseId, String tag) {\n    House house = houseRepository.findOne(houseId);\n    if (house == null) {\n        return ServiceResult.notFound();\n    }\n\n    HouseTag houseTag = houseTagRepository.findByNameAndHouseId(tag, houseId);\n    if (houseTag == null) {\n        return new ServiceResult(false, \"标签不存在\");\n    }\n\n    houseTagRepository.delete(houseTag.getId());\n    return ServiceResult.success();\n}\n```\n\n## 二十六、审核功能实现\n\n```java\n/**\n * 审核接口\n * @param id\n * @param operation\n * @return\n */\n@PostMapping(\"admin/house/operate/{id}/{operation}\")\n@ResponseBody\npublic ApiResponse operateHouse(@PathVariable(value = \"id\") Long id , @PathVariable(value = \"operation\") int operation){\n\n}\n```\n\n我们先设定审核通过这个简单功能测试：\n\n```java\n/**\n * 审核接口\n * @param id\n * @param operation\n * @return\n */\n@PutMapping(\"admin/house/operate/{id}/{operation}\")\n@ResponseBody\npublic ApiResponse operateHouse(@PathVariable(value = \"id\") Long id , @PathVariable(value = \"operation\") int operation){\n    if(id <=1  ){\n        return ApiResponse.ofStatus(ApiResponse.Status.NOT_VALID_PARAM);\n    }\n    if(operation ==1){\n        this.houseService.updateStatus(id, HouseStatus.PASSES.getValue());\n        return ApiResponse.ofStatus(ApiResponse.Status.SUCCESS);\n    }\n    return ApiResponse.ofStatus(ApiResponse.Status.BAD_REQUEST);\n}\n```\n\n> 在编辑的时候把@PutMapping写成了@PostMapping，导致了错误\n\n原因在于：@PutMapping和@PostMapping的区别？\n\n**如果执行添加操作, 后面的添加请求不会覆盖前面的请求, 所以使用@Postmapping**\n\n**如果执行修改操作, 后面的修改请求会把前面的请求给覆盖掉, 所以使用@PutMapping**\n\n\n\n然后我们在serviceImpl类中去实现updateStatus。\n\n因为在controller层中我们对传过来的id进行了校验。\n\n```java\nif(id <=1  ){\n    return ApiResponse.ofStatus(ApiResponse.Status.NOT_VALID_PARAM);\n}\n```\n\n我们也只是对id做个简单的校验，而没有对查询出来的house进行校验。\n\n所以我们在serviceImpl层进行了校验：\n\n```java\n@Override\n@Transactional\npublic ServiceResult updateStatus(Long id, int status) {\n    House house = houseRepository.findOne(id);\n    if(house == null){\n        return ServiceResult.notFound();\n    }\n\n    if(house.getStatus() == status){\n        return new ServiceResult(false,\"状态没有发生变化！\");\n    }\n    if (house.getStatus() == HouseStatus.RENTED.getValue()) {\n        return new ServiceResult(false,\"已出租的房屋不允许发生状态改变！\");\n    }\n    if(house.getStatus() == HouseStatus.DELETED.getValue()){\n        return new ServiceResult(false,\"已删除的房屋不允许操作！\");\n    }\n\n    houseRepository.updateStatus();\n    return null;\n}\n```\n\n当然了，在update之前我们要做一些其他的校验，比如说状态是否发生了改变？是否已出租房屋？是否已删除房屋？等等\n\n然后在repository中执行数据层的操作：\n\n```java\n@Modifying\n@Query(value = \"update House as house set house.status = :status where house.id = :id\")\nvoid updateStatus(@Param(value =\"id\") Long id, @Param(value = \"status\") int status);\n```\n\n同样的，在repositpory中并非简单的增删查操作，所以需要单独去编写@Modifying注解，和@Query注解去自定义JPA的语句。\n\n然后都编辑好了之后，就可以在serviceImpl中完善了：\n\n``` java\n@Override\n@Transactional\npublic ServiceResult updateStatus(Long id, int status) {\n    House house = houseRepository.findOne(id);\n    if(house == null){\n        return ServiceResult.notFound();\n    }\n\n    if(house.getStatus() == status){\n        return new ServiceResult(false,\"状态没有发生变化！\");\n    }\n    if (house.getStatus() == HouseStatus.RENTED.getValue()) {\n        return new ServiceResult(false,\"已出租的房屋不允许发生状态改变！\");\n    }\n    if(house.getStatus() == HouseStatus.DELETED.getValue()){\n        return new ServiceResult(false,\"已删除的房屋不允许操作！\");\n    }\n\n    houseRepository.updateStatus(id,status);\n    return ServiceResult.success();\n}\n```\n\n最后实现的审核：发布效果如下所示：\n\n![image-20220826150416712](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5k6k0gwpcj22600rigpt.jpg)\n\n\n\n\n\n当然了，在controller中我们不能定义operation=1、2、3、4这样的操作，所以要去定义一些通用的类库：\n\n```java\n@PutMapping(\"admin/house/operate/{id}/{operation}\")\n@ResponseBody\npublic ApiResponse operateHouse(@PathVariable(value = \"id\") Long id , @PathVariable(value = \"operation\") int operation){\n    if(id <=1  ){\n        return ApiResponse.ofStatus(ApiResponse.Status.NOT_VALID_PARAM);\n    }\n    if(operation ==1){\n        this.houseService.updateStatus(id, HouseStatus.PASSES.getValue());\n        return ApiResponse.ofStatus(ApiResponse.Status.SUCCESS);\n    }\n    return ApiResponse.ofStatus(ApiResponse.Status.BAD_REQUEST);\n}\n```\n\n\n\n\n\n```java\n/**\n * 房屋操作状态常量定义\n */\npublic class HouseOperation {\n    public static final int PASS = 1;  //通过审核\n\n    public static final int PULL_OUT = 0; //下架，重新审核\n\n    public static final int DELETE = 3; //逻辑删除\n    \n    public static final int RENT = 4; //出租\n\n}\n```\n\n定义好之后，就修改验证代码：\n\n```java\nif(operation == HouseOperation.PASS){\n    this.houseService.updateStatus(id, HouseStatus.PASSES.getValue());\n    return ApiResponse.ofStatus(ApiResponse.Status.SUCCESS);\n}\n```\n\n那既然这样，我们也可以换成swich( ) case的形式了：\n\n```java\n    /**\n     * 审核接口\n     * @param id\n     * @param operation\n     * @return\n     */\n    @PutMapping(\"admin/house/operate/{id}/{operation}\")\n    @ResponseBody\n    public ApiResponse operateHouse(@PathVariable(value = \"id\") Long id , @PathVariable(value = \"operation\") int operation){\n        if(id <=1  ){\n            return ApiResponse.ofStatus(ApiResponse.Status.NOT_VALID_PARAM);\n        }\n//        if(operation == HouseOperation.PASS){\n//            this.houseService.updateStatus(id, HouseStatus.PASSES.getValue());\n//            return ApiResponse.ofStatus(ApiResponse.Status.SUCCESS);\n//        }\n        ServiceResult result;\n        switch(operation){\n            case HouseOperation.PASS:\n                result = this.houseService.updateStatus(id, operation);\n                break;\n            case HouseOperation.PULL_OUT:\n                result = this.houseService.updateStatus(id, operation);\n                break;\n            case HouseOperation.DELETE:\n                result = this.houseService.updateStatus(id, operation);\n                break;\n            case HouseOperation.RENT:\n                result = this.houseService.updateStatus(id, operation);\n                break;\n\n            default:\n                return ApiResponse.ofStatus(ApiResponse.Status.BAD_REQUEST);\n        }\n        if(result == null){\n            return ApiResponse.ofSuccess(null);\n        }\n\n        return ApiResponse.ofMessage(HttpStatus.BAD_REQUEST.value(), result.getMessage());\n    }\n}\n```\n\n然而在我们设置PASS=1、PULL_OUT=2、DELETE=3、RENT=4的时候，发现点击上架按钮，前端显示的是已出租。所以找前端发现是这样设计的逻辑：\n\n```js\n   targets: 8,\n        render: function (data, type, row, meta) {\n            var html = '';\n            if (data === 0) {\n                html = '<td class=\"td-status\"><span class=\"label label-danger radius\">待审核</span></td>';\n            } else if (data === 1) {\n                html = '<td class=\"td-status\"><span class=\"label label-success radius\">已发布</span></td>';\n            } else if (data === 2) {\n                html = '<td class=\"td-status\"><span class=\"label label-warning radius\">已出租</span></td>';\n            } else {\n                html = '<td class=\"td-status\"><span class=\"label label-danger radius\">未知状态</span></td>';\n            }\n            return html;\n        }\n```\n\n然后在house-list.js中我们又调整了：\n\n```js\n/*房源-下架*/\nfunction house_stop(obj, id) {\n    layer.confirm('确认要下架吗？', function (index) {\n        $.ajax({\n            type: 'PUT',\n            url: '/admin/house/operate/' + id + '/' + '0',\n            success: function (data) {\n                if (data.code === 200) {\n                    $(obj).parents(\"tr\").find(\".td-manage\").prepend('<a style=\"text-decoration:none\"' +\n                        ' onClick=\"house_pass(this,id)\" href=\"javascript:;\" title=\"发布\"><i' +\n                        ' class=\"Hui-iconfont\">&#xe603;</i></a>');\n                    $(obj).parents(\"tr\").find(\".td-status\").html('<span class=\"label label-defaunt radius\">已下架</span>');\n                    $(obj).remove();\n                    layer.msg('已下架!', {icon: 5, time: 1000});\n                    reloadTable();\n                } else {\n                    layer.msg('下架失败！' + data.message, {icon: 5, time: 1000});\n                }\n\n            },\n            error: function (jqXHR, textStatus, errorThrown) {\n                console.log(jqXHR);\n                layer.msg('下架失败！' + jqXHR.responseText, {icon: 5, time: 3000});\n            }\n        });\n    });\n}\n```\n\n这里对下架的按钮修改为发送/admin/house/operate/' + id + '/' + '0' 的请求，也就是重新设置为状态**待审核**\n\n现在点击下架按钮就变为状态：待审核，而不是已出租的状态了。\n\n![image-20220826171519957](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5kac8a1tsj21b10gpq4h.jpg)\n\n## 二十七、基础功能分析\n\n### 1、业务与功能分析\n\n- 作为一个租房网站，要有基本的房源浏览功能，让用户在我们的网站可以浏览房源信息，并在找到目标房源后，能够查看房源的详细信息，这一章节就要实现租房网站的核心基础功能。\n\n### 2、实现目标\n\n- 网站基本布局\n- 房源信息浏览\n- 房源信息详情页\n\n## 二十八、基础功能实现\n\n### 1、房源信息浏览功能\n\n- 默认排序实现\n- 其他维度实现\n\n\n\n\n\n首先我们定义form结构体类：\n\n```java\npackage com.zryy.soufangtest.web.form;\n\n/**\n * 租房请求参数结构体\n *\n */\npublic class RentSearch {\n    private String cityEnName;\n    private String regionEnName;\n    private String priceBlock;\n    private String areaBlock;\n    private int room;\n    private int direction;\n    private String keywords;\n    private int rentWay = -1;\n    private String orderBy = \"lastUpdateTime\";\n    private String orderDirection = \"desc\";\n\n    public int getDirection() {\n        return direction;\n    }\n\n    public void setDirection(int direction) {\n        this.direction = direction;\n    }\n\n    private int start = 0;\n\n    private int size = 5;\n\n    public String getCityEnName() {\n        return cityEnName;\n    }\n\n    public void setCityEnName(String cityEnName) {\n        this.cityEnName = cityEnName;\n    }\n\n    public int getStart() {\n        return start > 0 ? start : 0;\n    }\n\n    public void setStart(int start) {\n        this.start = start;\n    }\n\n    public int getSize() {\n        if (this.size < 1) {\n            return 5;\n        } else if (this.size > 100) {\n            return 100;\n        } else {\n            return this.size;\n        }\n    }\n\n    public void setSize(int size) {\n        this.size = size;\n    }\n\n    public String getRegionEnName() {\n        return regionEnName;\n    }\n\n    public void setRegionEnName(String regionEnName) {\n        this.regionEnName = regionEnName;\n    }\n\n    public String getPriceBlock() {\n        return priceBlock;\n    }\n\n    public void setPriceBlock(String priceBlock) {\n        this.priceBlock = priceBlock;\n    }\n\n    public String getAreaBlock() {\n        return areaBlock;\n    }\n\n    public void setAreaBlock(String areaBlock) {\n        this.areaBlock = areaBlock;\n    }\n\n    public int getRoom() {\n        return room;\n    }\n\n    public void setRoom(int room) {\n        this.room = room;\n    }\n\n    public String getKeywords() {\n        return keywords;\n    }\n\n    public void setKeywords(String keywords) {\n        this.keywords = keywords;\n    }\n\n    public int getRentWay() {\n        if (rentWay > -2 && rentWay < 2) {\n            return rentWay;\n        } else {\n            return -1;\n        }\n    }\n\n    public void setRentWay(int rentWay) {\n        this.rentWay = rentWay;\n    }\n\n    public String getOrderBy() {\n        return orderBy;\n    }\n\n    public void setOrderBy(String orderBy) {\n        this.orderBy = orderBy;\n    }\n\n    public String getOrderDirection() {\n        return orderDirection;\n    }\n\n    public void setOrderDirection(String orderDirection) {\n        this.orderDirection = orderDirection;\n    }\n\n    @Override\n    public String toString() {\n        return \"RentSearch {\" +\n                \"cityEnName='\" + cityEnName + '\\'' +\n                \", regionEnName='\" + regionEnName + '\\'' +\n                \", priceBlock='\" + priceBlock + '\\'' +\n                \", areaBlock='\" + areaBlock + '\\'' +\n                \", room=\" + room +\n                \", direction=\" + direction +\n                \", keywords='\" + keywords + '\\'' +\n                \", rentWay=\" + rentWay +\n                \", orderBy='\" + orderBy + '\\'' +\n                \", orderDirection='\" + orderDirection + '\\'' +\n                \", start=\" + start +\n                \", size=\" + size +\n                '}';\n    }\n}\n```\n\n\n\n\n\n然后定义HouseController类：\n\n```java\npackage com.zryy.soufangtest.web.controller.house;\n\nimport com.zryy.soufangtest.base.ApiResponse;\nimport com.zryy.soufangtest.base.RentValueBLock;\nimport com.zryy.soufangtest.service.ServiceMultiResult;\nimport com.zryy.soufangtest.service.ServiceResult;\nimport com.zryy.soufangtest.service.house.IAddressService;\nimport com.zryy.soufangtest.service.house.IHouseService;\nimport com.zryy.soufangtest.web.dto.HouseDTO;\nimport com.zryy.soufangtest.web.dto.SubwayDTO;\nimport com.zryy.soufangtest.web.dto.SubwayStationDTO;\nimport com.zryy.soufangtest.web.dto.SupportAddressDTO;\nimport com.zryy.soufangtest.web.form.RentSearch;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Controller;\nimport org.springframework.ui.Model;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.ModelAttribute;\nimport org.springframework.web.bind.annotation.RequestParam;\nimport org.springframework.web.bind.annotation.ResponseBody;\nimport org.springframework.web.servlet.mvc.support.RedirectAttributes;\n\nimport javax.servlet.http.HttpSession;\nimport java.util.ArrayList;\nimport java.util.List;\n\n@Controller\npublic class HouseController {\n\n    @Autowired\n    private IAddressService addressService;\n\n    @Autowired\n    private IHouseService houseService;\n\n    /**\n     * 获取支持城市列表\n     *\n     * @return\n     */\n    @GetMapping(\"address/support/cities\")\n    @ResponseBody\n    public ApiResponse getSupportCities() {\n        ServiceMultiResult<SupportAddressDTO> result = addressService.findAllCities();\n        if (result.getResultSize() == 0) {\n            return ApiResponse.ofSuccess(ApiResponse.Status.NOT_FOUND);\n        }\n        return ApiResponse.ofSuccess(result.getResult());\n\n    }\n\n\n    /**\n     * 获取对应城市支持区域列表\n     *\n     * @param cityEnName\n     * @return\n     */\n    @GetMapping(\"address/support/regions\")\n    @ResponseBody\n    public ApiResponse getSupportRegions(@RequestParam(name = \"city_name\") String cityEnName) {\n        ServiceMultiResult<SupportAddressDTO> addressResult = addressService.findAllRegionsByCityName(cityEnName);\n        if (addressResult.getResult() == null || addressResult.getTotal() < 1) {\n            return ApiResponse.ofStatus(ApiResponse.Status.NOT_FOUND);\n        }\n        return ApiResponse.ofSuccess(addressResult.getResult());\n    }\n\n    /**\n     * 获取具体城市所支持的地铁线路\n     * @param cityEnName\n     * @return\n     */\n    @GetMapping(\"address/support/subway/line\")\n    @ResponseBody\n    public ApiResponse getSupportSubwayLine(@RequestParam(name = \"city_name\") String cityEnName) {\n        List<SubwayDTO> subways = addressService.findAllSubwayByCity(cityEnName);\n        if (subways.isEmpty()) {\n            return ApiResponse.ofStatus(ApiResponse.Status.NOT_FOUND);\n        }\n\n        return ApiResponse.ofSuccess(subways);\n    }\n\n    /**\n     * 获取对应地铁线路所支持的地铁站点\n     * @param subwayId\n     * @return\n     */\n    @GetMapping(\"address/support/subway/station\")\n    @ResponseBody\n    public ApiResponse getSupportSubwayStation(@RequestParam(name = \"subway_id\") Long subwayId) {\n        List<SubwayStationDTO> stationDTOS = addressService.findAllStationBySubway(subwayId);\n        if (stationDTOS.isEmpty()) {\n            return ApiResponse.ofStatus(ApiResponse.Status.NOT_FOUND);\n        }\n\n        return ApiResponse.ofSuccess(stationDTOS);\n    }\n\n    @GetMapping(\"rent/house\")\n    public String rentHousePage(@ModelAttribute RentSearch rentSearch,\n                                Model model, HttpSession httpSession,\n                                RedirectAttributes redirectAttributes){\n        //表单中先验证有没有城市\n        if(rentSearch.getCityEnName() ==null){\n            //从session中获取城市\n            String cityEnNameInSession = (String) httpSession.getAttribute(\"cityEnName\");\n            if(cityEnNameInSession == null){\n                redirectAttributes.addAttribute(\"msg\",\"must_chose_city\");\n                return \"redirect:/index\";\n            }else{\n                //从session里面读出来并设置到form表单中\n                rentSearch.setCityEnName(cityEnNameInSession);\n            }\n        }else{\n            httpSession.setAttribute(\"cityEnName\",rentSearch.getCityEnName());\n        }\n\n        ServiceResult<SupportAddressDTO> city = addressService.queryCity(rentSearch.getCityEnName());\n\n        if( !city.isSuccess()){\n            redirectAttributes.addAttribute(\"msg\",\"must_chose_city\");\n            return \"redirect:/index\";\n        }\n\n        model.addAttribute(\"currentCity\", city.getResult());\n\n        ServiceMultiResult<SupportAddressDTO> addressResult = addressService.findAllRegionsByCityName(rentSearch.getCityEnName());\n        if(addressResult.getTotal() < 1 ||addressResult.getResult() == null){\n            redirectAttributes.addAttribute(\"msg\",\"must_chose_city\");\n            return \"redirect:/index\";\n        }\n\n\n        ServiceMultiResult<HouseDTO> serviceMultiResult = houseService.query(rentSearch);\n\n\n        //设置一些页面的信息\n//        model.addAttribute(\"total\",serviceMultiResult.getTotal());\n        model.addAttribute(\"total\",10);\n        model.addAttribute(\"houses\",new ArrayList<>());\n\n        //如果用户没有设置区域，那么就默认选择所有区域的进行展示\n        if(rentSearch.getRegionEnName() ==null){\n            rentSearch.setRegionEnName(\"*\");\n        }\n\n        //每次查完都会把这个放在searchbody中防止用户刷新页面后，为其重新加载搜索结果\n        //在前端的逻辑是，我们把后端发送的serachbody数据读取好了后传入到js中然后拼接后再传到后端整理成一个url重新请求\n        model.addAttribute(\"searchBody\",rentSearch);\n\n        //区域信息\n        model.addAttribute(\"regions\", addressResult.getResult());\n\n        //并且设置价格区间，对于区间正式开发中要存放在数据库里面的，这里我们先简单固化在内存中。\n        model.addAttribute(\"priceBlocks\", RentValueBLock.PRICE_BLOCK);\n\n        //设置面积区间\n        model.addAttribute(\"areaBlocks\",RentValueBLock.AREA_BLOCK);\n\n        //另外还要设置当前的搜索条件，不能说翻了页之后就不带着这个搜索条件了\n        model.addAttribute(\"currentPriceBlock\",RentValueBLock.matchPrice(rentSearch.getPriceBlock()));\n\n        model.addAttribute(\"currentAreaBlock\",RentValueBLock.matchArea(rentSearch.getAreaBlock()));\n\n        return \"rent-list\";\n    }\n}\n\n```\n\n**然后我们去新建一个区间范围类，参数，以及方法：**\n\n```java\npackage com.zryy.soufangtest.base;\n\n\nimport com.google.common.collect.ImmutableMap;\n\nimport java.util.Map;\n\n/**\n * 带区间的常用数值定义\n */\npublic class RentValueBLock {\n    /**\n     * 价格区间定义\n     */\n    public static final Map<String, RentValueBLock> PRICE_BLOCK;\n\n    /**\n     * 面积区间定义\n     */\n    public static final Map<String, RentValueBLock> AREA_BLOCK;\n\n    /**\n     * 无限制区间\n     */\n    public static final RentValueBLock ALL = new RentValueBLock(\"*\",-1,-1);\n\n    static {\n        PRICE_BLOCK = ImmutableMap.<String, RentValueBLock>builder()\n                //设置最小值-1到1000区间范围\n                .put(\"*-1000\",new RentValueBLock(\"*-1000\",-1,1000))\n                //设置1000到3000的区间范围\n                .put(\"1000-3000\", new RentValueBLock(\"1000-3000\",1000,3000))\n                //设置3000到无限制的区间范围\n                .put(\"3000-*\", new RentValueBLock(\"3000-*\",3000,-1))\n                .build();\n\n        AREA_BLOCK = ImmutableMap.<String, RentValueBLock>builder()\n                //设置最小值-1到1000区间范围\n                .put(\"*-30\",new RentValueBLock(\"*-30\",-1,30))\n                //设置1000到3000的区间范围\n                .put(\"30-50\", new RentValueBLock(\"30-50\",30,50))\n                //设置3000到无限制的区间范围\n                .put(\"50-*\", new RentValueBLock(\"50-*\",50,-1))\n                .build();\n    }\n\n    private String key;\n    private int min;\n    private int max;\n\n    public RentValueBLock(String key, int min, int max) {\n        this.key = key;\n        this.min = min;\n        this.max = max;\n    }\n\n\n    //因为是javaBean所以需要定义一些get、set方法\n    public String getKey() {\n        return key;\n    }\n\n    public void setKey(String key) {\n        this.key = key;\n    }\n\n    public int getMin() {\n        return min;\n    }\n\n    public void setMin(int min) {\n        this.min = min;\n    }\n\n    public int getMax() {\n        return max;\n    }\n\n    public void setMax(int max) {\n        this.max = max;\n    }\n\n    //然后需要写一些方法执行\n    public static RentValueBLock matchPrice(String key){\n        //首先通过key来获取到区间\n        RentValueBLock block = PRICE_BLOCK.get(key);\n        //判断区间是否存在，如果不存在，那么就返回一个全部的ALL区间\n        if(block == null){\n            return ALL;\n        }\n        return block;\n    }\n    //面积区间匹配方法\n    public static RentValueBLock matchArea(String key){\n        //首先通过key来获取到区间\n        RentValueBLock block = AREA_BLOCK.get(key);\n        //判断区间是否存在，如果不存在，那么就返回一个全部的ALL区间\n        if(block == null){\n            return ALL;\n        }\n        return block;\n    }\n    //当然还可以写一些其他的区间筛选办法\n}\n```\n\n然后返回到controller中去编辑范围区间的代码：\n\n```java\n//并且设置价格区间，对于区间正式开发中要存放在数据库里面的，这里我们先简单固化在内存中。\nmodel.addAttribute(\"priceBlocks\", RentValueBLock.PRICE_BLOCK);\n\n//设置面积区间\nmodel.addAttribute(\"areaBlocks\",RentValueBLock.AREA_BLOCK);\n\n//另外还要设置当前的搜索条件，不能说翻了页之后就不带着这个搜索条件了\nmodel.addAttribute(\"currentPriceBlock\",RentValueBLock.matchPrice(rentSearch.getPriceBlock()));\n\nmodel.addAttribute(\"currentAreaBlock\",RentValueBLock.matchArea(rentSearch.getAreaBlock()));\n\nreturn \"rent-list\";\n```\n\n因为我们的houseServiceImpl中的query并未具体实现，所以先测试一下其他代码，结果如下：\n\n![image-20220830185856406](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5oztbly0aj21kg0u0why.jpg)\n\n其中model.addAttribute(\"total\",10);设置了一个固定的值，原来是model.addAttribute(\"total\",serviceMultiResult.getTotal());\n\n因为结果是空的，所以getTotal的话会报错。暂时先设定一个静态值。\n\n\n\n\n\n> 注意这里的一个细节\n\n```java\nif(addressResult.getTotal() < 1 ||addressResult.getResult() == null){\n    redirectAttributes.addAttribute(\"msg\",\"must_chose_city\");\n    return \"redirect:/index\";\n}\n```\n\n当判断条件成立的时候，那么url链接地址显示的是：http://localhost:8080/index?msg=must_chose_city\n\n也就是redirect到了index.html页，然后添加了msg参数。重新请求url。\n\n\n\n\n\n上面我们定义的query是一个空的，所以现在我们去定义具体的serviceImpl实现类：\n\n```java\n@Override\npublic ServiceMultiResult<HouseDTO> query(RentSearch rentSearch) {\n    //定义排序字段类\n    Sort sort = new Sort(Sort.Direction.DESC, \"lastUpdateTime\");\n\n    //定义页面\n    int page = rentSearch.getStart() / rentSearch.getSize();\n\n    //* @param page 页面开始的序号\n    //  * @param size 页面需要返回的数量\n    //  * @param sort sort类\n    Pageable pageable = new PageRequest(page, rentSearch.getSize(), sort);\n\n    //然后还需要定义特殊条件，比如逻辑删除的数据不被查询出来，根据状态、城市名称等\n    Specification<House> specification = (root, criteriaQuery, criteriaBuilder) ->{\n        Predicate predicate = criteriaBuilder.equal(root.get(\"status\"), HouseStatus.PASSES.getValue());\n        criteriaBuilder.and(predicate,criteriaBuilder.equal(root.get(\"cityEnName\"),rentSearch.getCityEnName()));\n\n        return predicate;\n    };\n\n    //这里我们使用的JPA的findAll，传递的参数第一个是特殊查询变量，第二个是分页\n    Page<House> houses = houseRepository.findAll(specification, pageable);\n    //因为返回的是一个list列表的形式，所以我们这里要定义一个HouseDTO类型的列表\n    ArrayList<HouseDTO> houseDTOS = new ArrayList<>();\n\n    houses.forEach(house -> {\n        HouseDTO houseDTO = modelMapper.map(house, HouseDTO.class);\n        //奇怪的是这里为什么要重复的去设置下cover呢。初步猜测原因是cover有时候会更新不出来，所以做一个异步的延时加载\n        houseDTO.setCover(this.cdnPrefix + house.getCover());\n        houseDTOS.add(houseDTO);\n    });\n\n\n    //最后返回ServiceMultiResult\n    return new  ServiceMultiResult<>(houses.getTotalElements(), houseDTOS);\n}\n```\n\nserviceimpl实现了，那么我们返回controller层中对返回的结果进行修改（之前定义的是一个null值）：\n\n```java\n        //设置一些页面的信息\n//        model.addAttribute(\"total\",serviceMultiResult.getTotal());\n        model.addAttribute(\"total\",10);\n        model.addAttribute(\"houses\",new ArrayList<>());\n```\n\n修改后：\n\n```java\nServiceMultiResult<HouseDTO> serviceMultiResult = houseService.query(rentSearch);\n\nmodel.addAttribute(\"total\", serviceMultiResult.getTotal());\nmodel.addAttribute(\"houses\", serviceMultiResult.getResult());\n```\n\n但是这里的houses并未把houseDetail的信息添加进去，所以导致了前端页面的错乱：\n\n![image-20220901173616255](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5r8nws4k3j21l30u0jum.jpg)\n\n```tex\n2022-09-01 17:35:48.071 ERROR 29197 --- [nio-8080-exec-5] org.thymeleaf.TemplateEngine             : [THYMELEAF][http-nio-8080-exec-5] Exception processing template \"rent-list\": Exception evaluating SpringEL expression: \"house.houseDetail.subwayLineName != null\" (template: \"rent-list\" - line 269, col 55)\n\norg.thymeleaf.exceptions.TemplateProcessingException: Exception evaluating SpringEL expression: \"house.houseDetail.subwayLineName != null\" (template: \"rent-list\" - line 269, col 55)\n```\n\n也就是说在传给前端参数的时候并没有house.houseDetail这个变量，所以导致了页面的错乱。\n\n所以暂时把house.houseDetail这个属性给注释掉，因为点击房子后详情的信息才能够展示出来，在rent-list这个页面上并无房屋详情页的信息展示。后续会做一个封装的操作类把houseDetail传递给前端页面进行渲染。\n\n我们对房源列表的排序设计一个排序类：\n\n```java\npackage com.zryy.soufangtest.base;\n\nimport com.google.common.collect.Sets;\nimport org.springframework.data.domain.Sort;\n\nimport java.util.Set;\n\n/**\n * 排序生成器\n */\npublic class HouseSort {\n\n    public static final String DEFAULT_SORT_KEY = \"lastUpdateTime\";\n\n    public static final String DISTANCE_TO_SUBWAY_KEY = \"distanceToSubway\";\n\n\n    //定义排序的字段\n    private static final Set<String> SORT_KEYS = Sets.newHashSet(\n            DEFAULT_SORT_KEY,\n            \"createTime\",\n            \"price\",\n            \"area\",\n            DISTANCE_TO_SUBWAY_KEY\n    );\n\n    //生成排序策略（加入了一些默认的设计逻辑）\n    public static Sort generateSort(String key, String directionKey){\n        key = getSortKey(key);\n\n        Sort.Direction direction = Sort.Direction.fromStringOrNull(directionKey);\n        if(direction == null){\n            direction = Sort.Direction.DESC;\n        }\n\n        return new Sort(direction, key);\n    }\n\n    public static String getSortKey(String key){\n        if(!SORT_KEYS.contains(key)){\n            key = DEFAULT_SORT_KEY;\n        }\n        return key;\n    }\n}\n```\n\n这样在houseServiceimpl中我们就可以修改排序变量：\n\n```java\n @Override\n    public ServiceMultiResult<HouseDTO> query(RentSearch rentSearch) {\n        //定义排序字段类\n//        Sort sort = new Sort(Sort.Direction.DESC, \"lastUpdateTime\");\n\n\n        //定义页面\n        int page = rentSearch.getStart() / rentSearch.getSize();\n```\n\n修改后的：\n\n```java\n@Override\n    public ServiceMultiResult<HouseDTO> query(RentSearch rentSearch) {\n        //定义排序字段类\n//        Sort sort = new Sort(Sort.Direction.DESC, \"lastUpdateTime\");\n        //自定义的排序方法-修改后\n        Sort sort = HouseSort.generateSort(rentSearch.getOrderBy(), rentSearch.getOrderDirection());\n\n        //定义页面\n        int page = rentSearch.getStart() / rentSearch.getSize();\n```\n\n另外呢，我们现在查询出来展示的只是house的信息，但是对于houseDetail的信息并未查询出来。\n\n然后就把houseId加进去了，然后通过id去查询出相关的详情信息，\n\n\n\n```java\nList<Long> houseIds = new ArrayList<>();\nMap<Long, HouseDTO> idToHouseMap = Maps.newHashMap();\n\n//这里我们使用的JPA的findAll，传递的参数第一个是特殊查询变量，第二个是分页\nPage<House> houses = houseRepository.findAll(specification, pageable);\n//因为返回的是一个list列表的形式，所以我们这里要定义一个HouseDTO类型的列表\nArrayList<HouseDTO> houseDTOS = new ArrayList<>();\n\nhouses.forEach(house -> {\n    HouseDTO houseDTO = modelMapper.map(house, HouseDTO.class);\n    //奇怪的是这里为什么要重复的去设置下cover呢。初步猜测原因是cover有时候会更新不出来，所以做一个异步的延时加载\n    houseDTO.setCover(this.cdnPrefix + house.getCover());\n    houseDTOS.add(houseDTO);\n    houseIds.add(house.getId());\n    idToHouseMap.put(house.getId(),houseDTO);\n});\n\n\n\nwrapperHouseList(houseIds,idToHouseMap);\n```\n\n```java\n/**\n * 渲染详细信息 及标签\n * @param houseIds\n * @param idToHouseMap\n */\nprivate void wrapperHouseList(List<Long> houseIds, Map<Long, HouseDTO> idToHouseMap) {\n    List<HouseDetail> details = houseDetailRepository.findAllByHouseIdIn(houseIds);\n    details.forEach(houseDetail -> {\n        HouseDTO houseDTO = idToHouseMap.get(houseDetail.getHouseId());\n        HouseDetailDTO houseDetailDTO = modelMapper.map(houseDetail, HouseDetailDTO.class);\n        houseDTO.setHouseDetail(houseDetailDTO);\n    });\n\n    List<HouseTag> houseTags = houseTagRepository.findAllByHouseIdIn(houseIds);\n    houseTags.forEach(houseTag -> {\n        HouseDTO houseDTO = idToHouseMap.get(houseTag.getHouseId());\n       houseDTO.getTags().add(houseTag.getName());\n    });\n}\n```\n\n\n\n另外：\n\n对于距离地铁最近的排序，要加一个判断条件，就是如果要按照地铁距离进行排序的话，那么就判断HouseSort.DISTANCE_to_SUBWAY_KEY字段必须大于-1，如果不添加这个判断的话，那么倒排的话，-1始终是在最前面的。\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5y3uabd5dj20zg0u0427.jpg\" alt=\"image-20220907160821320\" style=\"zoom: 50%;\" />\n\n```\nSpecification<House> specification = (root, criteriaQuery, criteriaBuilder) -> {\n    Predicate predicate = criteriaBuilder.equal(root.get(\"status\"), HouseStatus.PASSES.getValue());\n\n    predicate = criteriaBuilder.and(predicate, criteriaBuilder.equal(root.get(\"cityEnName\"), rentSearch.getCityEnName()));\n\n    if (HouseSort.DISTANCE_TO_SUBWAY_KEY.equals(rentSearch.getOrderBy())) {\n        predicate = criteriaBuilder.and(predicate, criteriaBuilder.gt(root.get(HouseSort.DISTANCE_TO_SUBWAY_KEY), -1));\n    }\n    return predicate;\n};\n```\n\n## 二十九、房源信息详情页\n\n上节中我们把房源信息列表查询出来了，那么接下来用户需要点击链接查看详情。\n\n首先我们定义一个GetMapping的url：\n\n```\n @GetMapping(\"rent/house/show/{id}\")\n```\n\n```java\n@GetMapping(\"rent/house/show/{id}\")\npublic String show(@PathVariable(value = \"id\"), Long houseId){\n\n    if(houseId <= 0 ){\n        return \"404\";\n    }\n\n    ServiceResult<HouseDTO> serviceResult = houseService.findCompleteOne(houseId);\n    if(!serviceResult.isSuccess()){\n        return \"404\";\n    }\n\n    HouseDTO houseDTO = serviceResult.getResult();\n    Map<SupportAddress.Level, SupportAddressDTO> addressMap = addressService.findCityAndRegion(houseDTO.getCityEnName(), houseDTO.getRegionEnName());\n\n    SupportAddressDTO city = addressMap.get(SupportAddress.Level.CITY);\n    SupportAddressDTO region = addressMap.get(SupportAddress.Level.REGION);\n\n\n    //另外还需要添加一个房屋管理员，或者是经纪人来管理的。\n    userService.findUserById(houseDTO.getAdminId());\n    return \"house-detail\";\n}\n```\n\n然后在userService接口类中去定义：\n\n```java\n/**\n *  用户服务\n */\npublic interface IUserService {\n    User findUserByUsername(String username);\n\n    ServiceResult<UserDTO> findUserById(Long userId);\n}\n```\n\n但是这里UserDTO并未定义，所以我们去定义UserDTO\n\n```java\npackage com.zryy.soufangtest.web.dto;\n\npublic class UserDTO {\n    private Long id;\n    private String name;\n    private String avatar;\n    private String phoneNumber;\n    private String lastLoginTime;\n\n    public Long getId() {\n        return id;\n    }\n\n    public void setId(Long id) {\n        this.id = id;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public String getAvatar() {\n        return avatar;\n    }\n\n    public void setAvatar(String avatar) {\n        this.avatar = avatar;\n    }\n\n    public String getPhoneNumber() {\n        return phoneNumber;\n    }\n\n    public void setPhoneNumber(String phoneNumber) {\n        this.phoneNumber = phoneNumber;\n    }\n\n    public String getLastLoginTime() {\n        return lastLoginTime;\n    }\n\n    public void setLastLoginTime(String lastLoginTime) {\n        this.lastLoginTime = lastLoginTime;\n    }\n}\n```\n\n然后去实现UserService的Impl实现类：\n\n```java\n@Override\npublic ServiceResult<UserDTO> findUserById(Long userId) {\n    User user = userRepository.findOne(userId);\n    if(user == null){\n        return ServiceResult.notFound();\n    }\n\n    UserDTO userDTO = modelMapper.map(user, UserDTO.class);\n\n    return ServiceResult.of(userDTO);\n}\n```\n\n最终的houseController的show方法代码如下：\n\n```java\n@GetMapping(\"rent/house/show/{id}\")\npublic String show(@PathVariable(value = \"id\") Long houseId,\n                   Model model){\n\n    if(houseId <= 0 ){\n        return \"404\";\n    }\n\n    ServiceResult<HouseDTO> serviceResult = houseService.findCompleteOne(houseId);\n    if(!serviceResult.isSuccess()){\n        return \"404\";\n    }\n\n    HouseDTO houseDTO = serviceResult.getResult();\n    Map<SupportAddress.Level, SupportAddressDTO> addressMap = addressService.findCityAndRegion(houseDTO.getCityEnName(), houseDTO.getRegionEnName());\n\n    SupportAddressDTO city = addressMap.get(SupportAddress.Level.CITY);\n    SupportAddressDTO region = addressMap.get(SupportAddress.Level.REGION);\n\n    model.addAttribute(\"city\",city);\n    model.addAttribute(\"region\",region);\n\n    //另外还需要添加一个房屋管理员，或者是经纪人来管理的。\n    ServiceResult<UserDTO> userDTOServiceResult = userService.findUserById(houseDTO.getAdminId());\n\n    //经纪人相关的信息\n    model.addAttribute(\"agent\",userDTOServiceResult.getResult());\n    model.addAttribute(\"house\",houseDTO);\n    return \"house-detail\";\n}\n```\n\n最后的结果如下图所示：\n\n![image-20220907182800817](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5y7vk496dj21jm0u0dj5.jpg)\n\n不过其中关于详情中，共有*套出租中 这个接口并未实现。这个会在es中进行聚合的字段。\n\n\n\n## 三十、搜索引擎实现-业务与功能分析\n\n### 搜索引擎实现\n\n在我们网站实现了**基础信息浏览功能**以后，用户就可以看到网站上的所有房源，但是在网站房源**信息量爆炸**的时候，用户是很难找到自己想要的信息的，这时候，网站就必须要有**站内搜索引擎功能**，帮助用户根据用户自身的需求快速找到想要的房源信息。那么，我们这一章节实现网站的**搜索引擎**。\n\n> 实现目标\n\n- 构建ES房源索引\n- 基于ES构建搜索引擎\n- 解决中文分词问题\n- Search-as-you-type（搜索提示）\n- 使搜索引擎结果集最优\n\n## 三十一、ES与MySQL技术选型\n\n### ElasticSearch与MySQL实现搜索对比\n\nmysql是做数据存储，来实现数据事务的特性。那么使用mysql构建搜索引擎的困境：\n\n如果想要复杂查询就需要写很复杂的sql语句去做联表查询：\n\n```sql\nselect * from house as a left join house_detail as b on a.id = b.house_id\nwhere a.title like '%国贸%' AND b.round_service like '%故宫%'\n```\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h5yypgk75jj217b0jjgo0.jpg\" alt=\"image-20220908095617560\" style=\"zoom:50%;\" />\n\n那么就可以看到我们查询两个条件就需要用到了两个联表查询。非常麻烦。\n\n如果还有其他条件，比如暖气、路线等等，那么就非常的麻烦了。\n\n当然可以使用phoneix加mysql来实现搜索，具体的链接可以查看：https://baijiahao.baidu.com/s?id=1708618149251629430&wfr=spider&for=pc\n\n> ###### Phoenix是一个基于HBase的开源**SQL引擎**，可以使用标准的JDBC API代替HBase客户端API来创建表，插入数据，查询你的HBase数据，它是完全使用Java编写，作为HBase内嵌的JDBC驱动使用。\n>\n> **Phoenix查询引擎会将SQL查询转换为一个或多个HBase扫描，并编排执行以生成标准的JDBC结果集。**\n>\n> ###### 直接使用HBase API、协同处理器与自定义过滤器，对于简单查询来说，其性能量级是毫秒，对于百万级别的行数来说，其性能量级\n>\n> ###### 是秒。\n\n- 使用MySQL构建搜索引擎的困境\n- 使用ES的优点\n\n## 三十二、索引结构设计\n\n> 目标房源信息的索引结构如何设计？\n\n首先要实现对信息的检索，哪些信息是用户比较需要的，是他们想要去检索的。\n\n然后启动es和es-head\n\n```json\n{\n\t\"mappings\":{\n\t\t\"house\":{   //tag\n\t\t\t\"dynamic\": false,  //索引不希望它变动，所以设置为false\n      \"properties\":{    //设置索引的属性\n        \"title\":{       //字段设置\n          \"type\":\t\"text\",\n          \"index\": \"analyzed\"   //需要被分析的\n        },\n        \"price\":{\n          \"type\":\t\"integer\"\n        },\n        \"area\":{\n          \"type\":\t\"integer\"\n        },\n        \"createTime\":{ \t\t //不只搜索相关信息，还需要做排序\n          \"type\":\t\"date\",\n          \"format\": \"strict_date_optional_time||epoch_millis\"\n        },\n        \"lastUpdateTime\":{\n          \"type\":\"date\",\n          \"format\": \"strict_date_optional_time||epoch_millis\"\n        },\n        \"cityEnName\":{\n          \"type\":\"keyword\"  //城市名称不需要分词\n        },\n        \"regionEnName\":{\n          \"type\":\"keyword\"   \n        },\n        \"direction\":{\n          \"type\":\"integer\"  //方向是固定的 1 2 3 4等来表示\n        },\n        \"distanceToSubway\":{\n          \"type\":\"integer\"   //距离地铁的距离\n        },\n        \"subwayLineName\":{\n          \"type\":\"keyword\"\n        },\n         \"subwayStationName\":{\n          \"type\":\"keyword\"\n        },\n         \"tags\":{\n          \"type\":\"text\"   //虽然是一个数组，在es中存储一个数组完全可以用一个单字段来存储的\n        },\n        \"street\":{\n          \"type\":\"keyword\"\n        },\n        \"district\":{\n          \"type\":\"keyword\"\n        },\n        \"description\":{\n          \"type\":\"text\",\n          \"index\":\"analyzed\"   //需要被分析的\n        },\n        \"layoutDesc\":{     //展示的文本描述\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"traffic\":{    //交通情况\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"roundService\":{   //周边的服务\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"rentWay\":{\n          \"type\":\"integer\"\n        }\n      }\n\t\t}\n\t}\n}\n```\n\n基本的索引结构就如上面这个样子。\n\n另外呢还不能直接创建，目前只有一个节点，默认是有5个分片、1个备份的。如果备份没有另外一台机器去存放的话，就会报状态是黄色。所以呢我们需要在前面添加一个：\n\n```json\n\"settings\":{\n\t\"number_of_replicas\": 0\n}\n```\n\n\n\n所以完整的json是：\n\n``` json\n{\n  \"settings\":{\n    \"number_of_replicas\": 0\n  },\n\t\"mappings\":{\n\t\t\"house\":{   //tag\n\t\t\t\"dynamic\": false,  //索引不希望它变动，所以设置为false\n      \"properties\":{    //设置索引的属性\n        \"houseId\": {\n          \"type\": \"long\"\n        },\n        \"title\":{       //字段设置\n          \"type\":\t\"text\",\n          \"index\": \"analyzed\"   //需要被分析的\n        },\n        \"price\":{\n          \"type\":\t\"integer\"\n        },\n        \"area\":{\n          \"type\":\t\"integer\"\n        },\n        \"createTime\":{ \t\t //不只搜索相关信息，还需要做排序\n          \"type\":\t\"date\",\n          \"format\": \"strict_date_optional_time||epoch_millis\"\n        },\n        \"lastUpdateTime\":{\n          \"type\":\"date\",\n          \"format\": \"strict_date_optional_time||epoch_millis\"\n        },\n        \"cityEnName\":{\n          \"type\":\"keyword\"  //城市名称不需要分词\n        },\n        \"regionEnName\":{\n          \"type\":\"keyword\"   \n        },\n        \"direction\":{\n          \"type\":\"integer\"  //方向是固定的 1 2 3 4等来表示\n        },\n        \"distanceToSubway\":{\n          \"type\":\"integer\"   //距离地铁的距离\n        },\n        \"subwayLineName\":{\n          \"type\":\"keyword\"\n        },\n         \"subwayStationName\":{\n          \"type\":\"keyword\"\n        },\n         \"tags\":{\n          \"type\":\"text\"   //虽然是一个数组，在es中存储一个数组完全可以用一个单字段来存储的\n        },\n        \"street\":{\n          \"type\":\"keyword\"\n        },\n        \"district\":{\n          \"type\":\"keyword\"\n        },\n        \"description\":{\n          \"type\":\"text\",\n          \"index\":\"analyzed\"   //需要被分析的\n        },\n        \"layoutDesc\":{     //展示的文本描述\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"traffic\":{    //交通情况\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"roundService\":{   //周边的服务\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"rentWay\":{\n          \"type\":\"integer\"\n        }\n      }\n\t\t}\n\t}\n}\n```\n\n> 注意上面的是number_of_replicas，而不是number_of_replica。\n\n\n\n然后在IDEA编译器中发送request请求：\n\n```json\nPUT http://localhost:9200/house\nAccept: text/json\n\n{\n    \"settings\":{\n      \"number_of_replicas\": 0\n    },\n\t\"mappings\":{\n\t\t\"house\":{\n\t\t\t\"dynamic\": false,\n      \"properties\":{\n        \"houseId\": {\n          \"type\": \"Long\"\n        },\n        \"title\":{\n          \"type\":\t\"text\",\n          \"index\": \"analyzed\"\n        },\n        \"price\":{\n          \"type\":\t\"integer\"\n        },\n        \"area\":{\n          \"type\":\t\"integer\"\n        },\n        \"createTime\":{\n          \"type\":\t\"date\",\n          \"format\": \"strict_date_optional_time||epoch_millis\"\n        },\n        \"lastUpdateTime\":{\n          \"type\":\"date\",\n          \"format\": \"strict_date_optional_time||epoch_millis\"\n        },\n        \"cityEnName\":{\n          \"type\":\"keyword\"\n        },\n        \"regionEnName\":{\n          \"type\":\"keyword\"\n        },\n        \"direction\":{\n          \"type\":\"integer\"\n        },\n        \"distanceToSubway\":{\n          \"type\":\"integer\"\n        },\n        \"subwayLineName\":{\n          \"type\":\"keyword\"\n        },\n         \"subwayStationName\":{\n          \"type\":\"keyword\"\n        },\n         \"tags\":{\n          \"type\":\"text\"\n        },\n        \"street\":{\n          \"type\":\"keyword\"\n        },\n        \"district\":{\n          \"type\":\"keyword\"\n        },\n        \"description\":{\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"layoutDesc\":{\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"traffic\":{\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"roundService\":{\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"rentWay\":{\n          \"type\":\"integer\"\n        }\n      }\n\t\t}\n\t}\n}\n```\n\n得到的结果如下：\n\n```json\nhttp://localhost:9200/house\n\nHTTP/1.1 200 OK\nWarning: 299 Elasticsearch-5.6.1-667b497 \"Content type detection for rest requests is deprecated. Specify the content type using the [Content-Type] header.\" \"Tue, 13 Sep 2022 06:14:50 GMT\"\nWarning: 299 Elasticsearch-5.6.1-667b497 \"Expected a boolean [true/false] for property [index] but got [analyzed]\" \"Tue, 13 Sep 2022 06:14:50 GMT\"\ncontent-type: application/json; charset=UTF-8\n\n{\n  \"acknowledged\": true,\n  \"shards_acknowledged\": true,\n  \"index\": \"house\"\n}\nResponse file saved.\n> 2022-09-13T141450.200.json\n\nResponse code: 200 (OK); Time: 392ms; Content length: 64 bytes\n```\n\n在es-head中查看结果：\n\n![image-20220913141723776](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h64ycpzt0ej20xd0bxjtb.jpg)\n\n发现house索引已经被创建成功。\n\n然后将这个house_index_mapping.json保存在db文件夹下。\n\n另外，我们不能直接使用es把数据存入进去，因为es是弱事务性的，在有增删改的时候会导致其产生很多乱糟糟的数据，当然这里我们使用es只是为了查出其索引来，并不是查询出其每个字段来。所以es对于我们来说只是检索其id，所以我们要得到的其实是他们的house_id，然后再到mysql中去查询出来。\n\n另外呢我们需要单独在java代码中新建一个类去操纵es代码，而不是去操作json代码。\n\n我们在service中新建一个search包，并新建一个索引结构模板：HouseIndexTemplate\n\n```java\npackage com.zryy.soufangtest.service.search;\n\n\nimport java.util.Date;\nimport java.util.List;\n\n/**\n * 索引结构模板\n */\npublic class HouseIndexTemplate {\n    private Long houseId;\n\n    private String title;\n\n    private int price;\n\n    private int area;\n\n    private Date createTime;\n\n    private Date lastUpdateTime;\n\n    private String cityEnName;\n\n    private String regionEnName;\n\n    private int direction;\n\n    private int distanceToSubway;\n\n    private String subwayLineName;\n\n    private String subwayStationName;\n\n    private String street;\n\n    private String district;\n\n    private String description;\n\n    private String layoutDesc;\n\n    private String traffic;\n\n    private String roundService;\n\n    private int rentWay;\n\n    private List<String> tags;\n\n    public Long getHouseId() {\n        return houseId;\n    }\n\n    public void setHouseId(Long houseId) {\n        this.houseId = houseId;\n    }\n\n    public String getTitle() {\n        return title;\n    }\n\n    public void setTitle(String title) {\n        this.title = title;\n    }\n\n    public int getPrice() {\n        return price;\n    }\n\n    public void setPrice(int price) {\n        this.price = price;\n    }\n\n    public int getArea() {\n        return area;\n    }\n\n    public void setArea(int area) {\n        this.area = area;\n    }\n\n    public Date getCreateTime() {\n        return createTime;\n    }\n\n    public void setCreateTime(Date createTime) {\n        this.createTime = createTime;\n    }\n\n    public Date getLastUpdateTime() {\n        return lastUpdateTime;\n    }\n\n    public void setLastUpdateTime(Date lastUpdateTime) {\n        this.lastUpdateTime = lastUpdateTime;\n    }\n\n    public String getCityEnName() {\n        return cityEnName;\n    }\n\n    public void setCityEnName(String cityEnName) {\n        this.cityEnName = cityEnName;\n    }\n\n    public String getRegionEnName() {\n        return regionEnName;\n    }\n\n    public void setRegionEnName(String regionEnName) {\n        this.regionEnName = regionEnName;\n    }\n\n    public int getDirection() {\n        return direction;\n    }\n\n    public void setDirection(int direction) {\n        this.direction = direction;\n    }\n\n    public int getDistanceToSubway() {\n        return distanceToSubway;\n    }\n\n    public void setDistanceToSubway(int distanceToSubway) {\n        this.distanceToSubway = distanceToSubway;\n    }\n\n    public String getSubwayLineName() {\n        return subwayLineName;\n    }\n\n    public void setSubwayLineName(String subwayLineName) {\n        this.subwayLineName = subwayLineName;\n    }\n\n    public String getSubwayStationName() {\n        return subwayStationName;\n    }\n\n    public void setSubwayStationName(String subwayStationName) {\n        this.subwayStationName = subwayStationName;\n    }\n\n    public String getStreet() {\n        return street;\n    }\n\n    public void setStreet(String street) {\n        this.street = street;\n    }\n\n    public String getDistrict() {\n        return district;\n    }\n\n    public void setDistrict(String district) {\n        this.district = district;\n    }\n\n    public String getDescription() {\n        return description;\n    }\n\n    public void setDescription(String description) {\n        this.description = description;\n    }\n\n    public String getLayoutDesc() {\n        return layoutDesc;\n    }\n\n    public void setLayoutDesc(String layoutDesc) {\n        this.layoutDesc = layoutDesc;\n    }\n\n    public String getTraffic() {\n        return traffic;\n    }\n\n    public void setTraffic(String traffic) {\n        this.traffic = traffic;\n    }\n\n    public String getRoundService() {\n        return roundService;\n    }\n\n    public void setRoundService(String roundService) {\n        this.roundService = roundService;\n    }\n\n    public int getRentWay() {\n        return rentWay;\n    }\n\n    public void setRentWay(int rentWay) {\n        this.rentWay = rentWay;\n    }\n\n    public List<String> getTags() {\n        return tags;\n    }\n\n    public void setTags(List<String> tags) {\n        this.tags = tags;\n    }\n}\n\n```\n\n\n\n\n\n另外我们还需要创建一个常量类：\n\n```java\npackage com.zryy.soufangtest.service.search;\n\n/**\n * 索引关键字统一定义\n */\npublic class HouseIndexKey {\n    public static final String HOUSE_ID = \"houseId\";\n\n    public static final String TITLE = \"title\";\n\n    public static final String PRICE = \"price\";\n    public static final String AREA = \"area\";\n    public static final String CREATE_TIME = \"createTime\";\n    public static final String LAST_UPDATE_TIME = \"lastUpdateTime\";\n    public static final String CITY_EN_NAME = \"cityEnName\";\n    public static final String REGION_EN_NAME = \"regionEnName\";\n    public static final String DIRECTION = \"direction\";\n    public static final String DISTANCE_TO_SUBWAY = \"distanceToSubway\";\n    public static final String STREET = \"street\";\n    public static final String DISTRICT = \"district\";\n    public static final String DESCRIPTION = \"description\";\n    public static final String LAYOUT_DESC = \"layoutDesc\";\n    public static final String TRAFFIC = \"traffic\";\n    public static final String ROUND_SERVICE = \"roundService\";\n    public static final String RENT_WAY = \"rentWay\";\n    public static final String SUBWAY_LINE_NAME = \"subwayLineName\";\n    public static final String SUBWAY_STATION_NAME = \"subwayStationName\";\n    public static final String TAGS = \"tags\";\n}\n```\n\n建立这样一个类方便我们后面在java代码中使用代码的形式去编辑，而不是使用json的方式去操作。\n\n> ES更适合用来检索，Mysql更适合用来做存储\n\n## 三十三、索引构建-核心逻辑\n\n- 核心逻辑实现\n- 基于Kafka的异步构建\n\n\n\n### 1、新建esConfig类\n\n然后设置目标地址   注意这里使用tcp的9300，而不是http的9200 原因不知.\n\n```java\n@Configuration\npublic class ElasticSearchConfig {\n\n    @Bean\n    public TransportClient esClient() throws UnknownHostException{\n        Settings settings = Settings.builder()\n                .put(\"cluster.name\", \"elasticsearch\")\n                .put(\"client.transport.sniff\", true)  //自动发现节点\n                .build();\n\n        //然后设置目标地址   注意这里使用tcp的9300，而不是http的9200 原因不知\n        InetSocketTransportAddress master = new InetSocketTransportAddress(\n                InetAddress.getByName(\"127.0.0.1\"), 9300\n        );\n\n        //然后构建，把配置当进去，把地址放进去\n        TransportClient client = new PreBuiltTransportClient(settings)\n                .addTransportAddress(master);\n//                .addTransportAddress(****)    当然这里可以把其他的节点加进来\n\n\n        return client;\n    }\n}\n```\n\n然后新建ISearchService类，暂时增加检索、移除接口：\n\n```java\n/**\n * 检索接口\n */\npublic interface ISearchService {\n\n    /**\n     * 索引目标房源\n     * @param houseId\n     */\n    void index(Long houseId);\n\n\n    /**\n     * 移除目标房源\n     * @param houseId\n     */\n    void remove(Long houseId);\n}\n```\n\n\n\n然后编辑SearchServiceImpl实现类。\n\n**注意要在里面添加logger的话，使用**\n\n**private static final Logger logger = LoggerFactory.getLogger(ISearchService.class);**\n\n```java\npublic class SearchServiceImpl implements ISearchService{\n    private static final Logger logger = LoggerFactory.getLogger(ISearchService.class);\n\n    @Autowired\n    private HouseRepository houseRepository;\n\n    @Autowired\n    private ModelMapper modelMapper;\n\n    @Override\n    public void index(Long houseId) {\n        House house = houseRepository.findOne(houseId);\n        if(house == null){\n             logger.error(\"index house {} does not exist\",houseId);\n             return;\n        }\n\n\n        HouseIndexTemplate indexTemplate = new HouseIndexTemplate();\n        modelMapper.map(house,indexTemplate);\n\n        //create\n\n        //update\n\n        //delete & create\n    }\n\n\n\n    @Override\n    public void remove(Long houseId) {\n\n    }\n}\n```\n\n另外呢我们还是用了log把错误输出，使用了modelMapper来把house类转换成houseIndexTemplate类。\n\n另外我们不仅仅有index检索索引、移除索引，还得有创建索、更新索引。\n\n```java\nprivate boolean create(HouseIndexTemplate houseIndexTemplate){\n\n}\nprivate boolean update(HouseIndexTemplate houseIndexTemplate){\n\n}\nprivate boolean deleteAndCreate(HouseIndexTemplate houseIndexTemplate){\n\n}\n```\n\n这时候呢，我们需要用到刚才定义的esClient\n\n```java\n@Autowired\nprivate TransportClient esclient;\n```\n\n这里关联到了我们之前定义的ElasticSearchConfig类：\n\n```java\n@Configuration\npublic class ElasticSearchConfig {\n\n    @Bean\n    public TransportClient esClient() throws UnknownHostException{\n        Settings settings = Settings.builder()\n                .put(\"cluster.name\", \"elasticsearch\")\n                .put(\"client.transport.sniff\", true)  //自动发现节点\n                .build();\n\n        //然后设置目标地址   注意这里使用tcp的9300，而不是http的9200 原因不知\n        InetSocketTransportAddress master = new InetSocketTransportAddress(\n                InetAddress.getByName(\"127.0.0.1\"), 9300\n        );\n\n        //然后构建，把配置当进去，把地址放进去\n        TransportClient client = new PreBuiltTransportClient(settings)\n                .addTransportAddress(master);\n//                .addTransportAddress(****)    当然这里可以把其他的节点加进来\n\n\n        return client;\n    }\n}\n```\n\n\n\n定义索引名称，定义索引类型，然后在\n\n```java\nprivate boolean create(HouseIndexTemplate houseIndexTemplate){\n        esclient.prepareIndex(INDEX_NAME,INDEX_TYPE)\n                .setSource()\n    }\n```\n\n需要使用TransportClient的esclient的prepareIndex方法，传入的参数是index_name、index_type，然后使用setSource来定义json代码，这里我们使用定义的HouseIndexTemplate类来使用json方法。我们使用obejctMapper来转换json。\n\n所以需要自动注解:\n\n```java\n@Autowired\nprivate ObjectMapper objectMapper;\n```\n\n使用objectMapper.writeValueAsBytes(houseIndexTemplate)来对houseIndexTemplate转换格式，另外在es5.6中需要转换成json格式，所以我们在setSource中定义了一个XContentType.JSON的参数，然后其返回值是一个IndexResponse我们获取到这个response。\n\n在下面进行判断，其有一个response.status，然后我们使用其与RestStatus.CREATED做比较。\n\n其中对于RestStatus.CREATED，其enum值有很多。比如：\n\n```java\npublic enum RestStatus {\n    CONTINUE(100),\n    SWITCHING_PROTOCOLS(101),\n    OK(200),\n    CREATED(201),\n    ACCEPTED(202),\n    NON_AUTHORITATIVE_INFORMATION(203),\n    NO_CONTENT(204),\n    RESET_CONTENT(205),\n    PARTIAL_CONTENT(206),\n    MULTI_STATUS(207),\n    MULTIPLE_CHOICES(300),\n    MOVED_PERMANENTLY(301),\n    FOUND(302),\n    SEE_OTHER(303),\n    NOT_MODIFIED(304),\n    USE_PROXY(305),\n    TEMPORARY_REDIRECT(307),\n    BAD_REQUEST(400),\n```\n\n都是一些状态码什么的。\n\n\n\n完整的create方法：\n\n```java\nprivate boolean create(HouseIndexTemplate houseIndexTemplate)  {\n    try {\n        IndexResponse response = esclient.prepareIndex(INDEX_NAME,INDEX_TYPE)\n                .setSource(objectMapper.writeValueAsBytes(houseIndexTemplate), XContentType.JSON).get();\n\n        logger.debug(\"Create index with house: \"+ houseIndexTemplate.getHouseId());\n\n        if(response.status() == RestStatus.CREATED){\n            return true;\n        }else{\n            return false;\n        }\n    } catch (JsonProcessingException e) {\n        logger.error(\"Error to index house \" + houseIndexTemplate.getHouseId(), e);\n        return false;\n    }\n}\n```\n\n然后我们去定义update方法，其逻辑同create大同小异。\n\n```java\nprivate boolean update(String esId,HouseIndexTemplate houseIndexTemplate){\n    try {\n        UpdateResponse response = esclient.prepareUpdate(INDEX_NAME,INDEX_TYPE,esId)\n                .setDoc(objectMapper.writeValueAsBytes(houseIndexTemplate), XContentType.JSON).get();\n\n        logger.debug(\"update index with house: \"+ houseIndexTemplate.getHouseId());\n\n        if(response.status() == RestStatus.CREATED){\n            return true;\n        }else{\n            return false;\n        }\n    } catch (JsonProcessingException e) {\n        logger.error(\"Error to update house \" + houseIndexTemplate.getHouseId(), e);\n        return false;\n    }\n}sss\n```\n\n只不过esclient不使用prepareIndex了而是使用prepareUpdate，然后需要多传入一个esId的参数，另外在json串那里也不是使用setSource，而是使用setDoc，另外返回的是UpdateResponse也不是IndexResponse了。另外RestStatus.CREATED也需要换成RestStatus.OK。\n\n\n\n然后就是编写delete方法了，在5.0版本已经没有delete相关的直接接口了，所以我们使用deleteByQuery方法。\n\n我们需要使用new RequestBuilder方法，然后使用其filter方法，filter方法中query出来的东西都会被删除掉，\n\nsouce方法返回的是DeleteByQueryRequestBuilder，然后需要//通过buider.get来获得返回值，通过返回值去得到已删除的数量（long deleted = response.getDeleted() ）所以我们需要在参数中接收一个总的需要删除的数量，最后再去做一个比较，如果不相等的话就报一个logger的警告，如果相等，那么就去调用create方法，把houseId当做参数传入进去创建。\n\n最后的deleteAndCreate方法如下：\n\n```java\nprivate boolean deleteAndCreate(long totalHit,HouseIndexTemplate houseIndexTemplate){\n    DeleteByQueryRequestBuilder builder = DeleteByQueryAction.INSTANCE\n            .newRequestBuilder(esclient)\n            .filter(QueryBuilders.termQuery(HouseIndexKey.HOUSE_ID, houseIndexTemplate.getHouseId()))    //query出来的东西都会被删除掉\n            .source(INDEX_NAME);//设置source，也就是索引名\n\n\n    logger.debug(\"Delete by query for house:\" + builder);\n    BulkByScrollResponse response = builder.get();  //通过buider.get来获得返回值\n    long deleted = response.getDeleted();  //通过返回值去得到已删除的数量\n    if(deleted != totalHit){\n        logger.warn(\"Need delete {} , but {} was deleted\",totalHit, deleted);\n        return false;\n    }else{\n        return create(houseIndexTemplate);\n    }\n}\n```\n\n## 三十四、索引构建-核心逻辑2\n\n\n\n在我们的houseIndexTemplate中除了houseId之外，还有一些关于subway、subwayStation等在其他的表中。我们需要查出来，然后放进来。\n\n所以我们把：\n\n```java\n@Autowired\nprivate HouseDetailRepository houseDetailRepository;\n\n@Autowired\nprivate HouseTagRepository houseTagRepository;\n```\n\n两个repository先自动装配进来。\n\n\n\n然后把相关的信息查询出来：\n\n```java\nHouseDetail houseDetail = houseDetailRepository.findByHouseId(houseId);\nif(houseDetail == null){\n    // TODO 异常情况\n}\n//然后呢把查询出来的detail映射到我们定义好的HouseIndexTemplate里面\nmodelMapper.map(houseDetail,indexTemplate);\n\nList<HouseTag> tags = houseTagRepository.findAllById(houseId);\nif(tags !=null && !tags.isEmpty()){\n    List<String> tagStrings = new ArrayList<>();\n    tags.forEach(houseTag -> tagStrings.add(houseTag.getName()));\n    indexTemplate.setTags(tagStrings);\n}\n```\n\n从这里来说，整个业务逻辑的查询就完成了，然后需要我们去查一下异常情况，比如数据是否在es中已经存在了，再去决定进行下一步的操作。\n\n\n\n然后我们去esClient查询\n\n```java\nSearchRequestBuilder requestBuilder = this.esclient.prepareSearch(INDEX_NAME).setTypes(INDEX_TYPE)\n        .setQuery(QueryBuilders.termQuery(HouseIndexKey.HOUSE_ID, houseId));\n\nlogger.debug(requestBuilder.toString());\n\nSearchResponse searchResponse = requestBuilder.get();\n\nlong totalHit = searchResponse.getHits().totalHits;\n```\n\n然后再去判断totalHit命中的数量，如果0则创建、如果1则更新、如果其他则删除重建。\n\n\n\n```java\n@Override\npublic boolean index(Long houseId) {\n    House house = houseRepository.findOne(houseId);\n    if(house == null){\n         logger.error(\"index house {} does not exist\",houseId);\n         return false;\n    }\n\n\n    HouseIndexTemplate indexTemplate = new HouseIndexTemplate();\n    modelMapper.map(house,indexTemplate);\n\n\n    HouseDetail houseDetail = houseDetailRepository.findByHouseId(houseId);\n    if(houseDetail == null){\n        // TODO 异常情况\n    }\n    //然后呢把查询出来的detail映射到我们定义好的HouseIndexTemplate里面\n    modelMapper.map(houseDetail,indexTemplate);\n\n    List<HouseTag> tags = houseTagRepository.findAllById(houseId);\n    if(tags !=null && !tags.isEmpty()){\n        List<String> tagStrings = new ArrayList<>();\n        tags.forEach(houseTag -> tagStrings.add(houseTag.getName()));\n        indexTemplate.setTags(tagStrings);\n    }\n    //从这里来说，整个业务逻辑的查询就完成了，然后需要我们去查一下异常情况，比如数据是否在es中已经存在了，再去决定进行下一步的操作。\n\n    SearchRequestBuilder requestBuilder = this.esclient.prepareSearch(INDEX_NAME).setTypes(INDEX_TYPE)\n            .setQuery(QueryBuilders.termQuery(HouseIndexKey.HOUSE_ID, houseId));\n\n    logger.debug(requestBuilder.toString());\n\n    SearchResponse searchResponse = requestBuilder.get();\n\n    long totalHit = searchResponse.getHits().totalHits;\n    boolean success;\n    if(totalHit == 0){  //如果没有的话，就需要创建\n        success = create(indexTemplate);\n    }else if(totalHit ==1){   //如果有一个的情况，那就是需要更新\n        String esId = searchResponse.getHits().getAt(0).getId();\n        success = update(esId,indexTemplate);\n    }else{   //如果其他情况都没有的话（比如一条查出了多条重复的），那么就需要删除掉然后重新创建一条\n        success = deleteAndCreate(totalHit,indexTemplate);\n    }\n    if(success){\n        logger.debug(\"Index success with house \" + houseId);\n        \n    }\n    return success;\n}\n```\n\n测试的时候报了这个错误：\n\nNoNodeAvailableException None of the configured nodes are available: \n\n然后去追踪问题发现：\n\n![image-20220914171955528](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h66990s1sdj20jc06974n.jpg)\n\n![image-20220914172248936](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h669bvajkij20j40bwdh4.jpg)\n\n没有匹配上，所以修改elasticsearch为xunwu，再次尝试。\n\n![image-20220914174557710](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h669zygi6bj20vd09fq3i.jpg)\n\n测试成功，总结：\n\n在mapping中我们定义的是索引的名称：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h66a1nc5oij20a30dp0t4.jpg\" alt=\"image-20220914174735343\"  />\n\n然后在代码中去定义索引的类型，这里当然是可以随意输入的，经测试输入house2，然后可以创建。\n\n```java\nSearchRequestBuilder requestBuilder = this.esclient.prepareSearch(INDEX_NAME).setTypes(INDEX_TYPE)\n        .setQuery(QueryBuilders.termQuery(HouseIndexKey.HOUSE_ID, houseId));\n```\n\n\n\n哦对了，在test中因为我们使用的是application-test.properties ，所以是使用的h2内存数据库，直接在db文件夹下的sql文件修改就可以。\n\n\n\n\n\n然后我们去定义remove方法逻辑：\n\n​\t\n\n```java\n@Override\npublic void remove(Long houseId) {\n    DeleteByQueryRequestBuilder builder = DeleteByQueryAction.INSTANCE\n            .newRequestBuilder(esclient)\n            .filter(QueryBuilders.termQuery(HouseIndexKey.HOUSE_ID, houseId))    //query出来的东西都会被删除掉\n            .source(INDEX_NAME);//设置source，也就是索引名\n\n\n    logger.debug(\"Delete by query for house: \" + builder);\n\n\n    BulkByScrollResponse response = builder.get();\n    long deleted = response.getDeleted();\n    logger.debug(\"Delete total: \" + deleted);\n}\n```\n\n测试程序：\n\n```java\n@Test\npublic void testRemove(){\n    Long houseId = 15L;\n    searchService.remove(houseId);\n\n}\n```\n\n测试成功，删除。\n\n\n\n> index 和remove接口是在特定情况下去执行的\n\n首先数组刚增加的时候，是不需要建立索引的，因为管理员需要进行审核，数据经过出租、下架都是一个状态的改变，所以在接口的必要路径去改变就可以了。\n\n在HouseServiceImpl的update方法中：\n\n```java\n//在数据更新的时候，如果检测到其状态是审核通过，那么就需要更新一下索引\nif(house.getStatus() == HouseStatus.PASSES.getValue()){\n    searchService.index(house.getId());\n}\n```\n\n另外在HouseServiceImpl的updateStatus方法中，在更新状态的时候也要去对索引进行操作。\n\n\n\n```java\n//上架更新索引，其他情况下都要删除索引\nif (status == HouseStatus.PASSES.getValue()){\n    searchService.index(id);\n} else {\n   searchService.remove(id); \n}\n```\n\n\n\n**页面测试数据正常，不过对于下架上架这里，有一些0、1、2、3的操作码不太正确，需要进行调整。**\n\n**另外对于SearchServiceImpl中**\n\n![image-20220914202807798](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h66eoos3h0j20ki07vgm8.jpg)\n\n这里的异常情况需要进行调整，如果houseDetail==null，return false虽然没办法走下面的代码逻辑，但是前端返回值仍然是发布成功的状态。当然，对于索引的操作肯定不会去执行。但是在前端页面中有一些错误的显示，未作调整。\n\n不止测试了下架上架功能，同样的测试了编辑功能，都可以成功的创建索引。\n\n\n\n本节我们实现了通过java代码对es索引操作，下一节就要实现异步的创建索引，也就是增加kafka消息队列。\n\n\n\n## 三十五、索引构建-消息中间件\n\nKafka，一个分布式发布订阅消息系统，同时也是一个强大的消息队列，可以处理大量的数据，可以将一个消息从一个端点传送到另外一个端点。\n\nKafka适合离线和在线的消息消费，消息是保存在磁盘上的，和其他消息队列比较：很多队列是基于内存的模型来制作的。\n\n> Spark Streaming实时流处理项目实战，可以学习一下\n\n\n\nkafka是基于zookeeper的，zookeeper是一个分布式配置，和同步服务，zookeeper是kafka代理和消费者之间的协调接口，kafka就是通过zookeeper集群来共享信息的。\n\n\n\n我们进入zookeeper的文件夹下，然后先启动zookeeper服务， 其默认是自动开放2181端口的。\n\n```\n./bin/zookeeper-server-start.sh config/zookeeper.properties \n```\n\n启动后的结果：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h67f2c95dbj20wa0ki7ab.jpg\" alt=\"image-20220915172632829\" style=\"zoom:50%;\" />\n\n\n\n当然我们可以使用命令在后台运行。\n\n```java\nnohup ./bin/zookeeper-server-start.sh config/zookeeper.peoperties &\t\t\n```\n\n然后\n\n```java\nvim nohup.out\t\n```\n\n去查看后台运行输出的情况。\n\n\n\n\n\n然后就是启动kafka，启动一个单实例\n\n```java\n./bin/kafka-server-start.sh config/server.properties\n```\n\n结果图如下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h6juiu9z5sj20u00yx7f6.jpg\" alt=\"image-20220916094227465\" style=\"zoom:50%;\" />\n\n\n\n我们可以查看server.properties的详情：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h687dgh918j20u00yg7bw.jpg\" alt=\"image-20220916094616488\" style=\"zoom:50%;\" />\n\n其中broker.id在整个集群中是唯一值， （broker：经纪人、掮客）\n\n另外还有，如果想让外网访问的话，可以把listeners=PLAINTEXT://localhost:9092打开，然后把localhost换成自己的外网ip即可。\n\nadvertised.listeners=PLAINTEXT://localhost:9092中的localhost也替换。\n\n\n\n接下来我们创建一个topic：\n\n``` shell\n./bin/kafka-topics.sh  --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic hello\n```\n\n```shell\n(base) zhiqiang@zhiQiangdeMacBook-Pro kafka_2.11-1.0.0 % ./bin/kafka-topics.sh  --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic hello\nCreated topic \"hello\".\n```\n\n\n\n获取主题列表：\n\n```shell\n./bin/kafka-topics.sh --list --zookeeper localhost:2181\n//结果就是\nhello\n```\n\n\n\n模拟生产消息(9092是kafka的地址)\n\n```shell\n(base) zhiqiang@zhiQiangdeMacBook-Pro kafka_2.11-1.0.0 % ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic hello\n>hello zhiqiang\n>hello zhiqiang , two\n```\n\n\n\n模拟消费者（2181是zookeeper地址，添加参数--from-beginning是从头开始消费消息） \n\n```shell\n(base) zhiqiang@zhiQiangdeMacBook-Pro kafka_2.11-1.0.0 % ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic hello --from-beginning\nUsing the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].\nhello zhiqiang\nhello zhiqiang , two\n```\n\n然后我们同时启动生产消息和消费消息，界面如下：\n\n![image-20220916140729770](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h68ex9wh0vj21050as76i.jpg)\n\n我们可以实时的把消息发送到中间件中，然后再实时的消费掉。\n\n我们用的话怎么用呢？\n\n用户在创建数据，我们把数据发送到kafka的队列，通过异步的模型把消息消费掉，再去构建我们的索引。dengzhe\n\n## 三十六、索引构建-代码实践\n\n> 代码水平的高低不在于能不能实现功能，而在于对异常情况的处理水平。\n\n当管理员在处理数据的时候，比如说上架下架的时候，es有一些不稳定，或者说当下时间会很久的话，就很影响用户的一个体验。这时候就需要考虑这个情况，比如说index创建索引的时候需要时间很久，用户不希望等待这么久，这时候就可能需要消息队列来进行。\n\n\n\n\n\n#### 然后进行kafka的代码实践：\n\n##### 首先得有kafka的properties的配置：\n\n```properties\n#kafka\nspring.kafka.bootstrap-servers=localhost:9092\nspring.kafka.consumer.group-id=xunwu\n```\n\nkafka得有一个group-id，这里我们定义为xunwu\n\n##### 然后去searchServiceImpl中构建kafka相关的代码逻辑：\n\n```java\n//定义kafka的topic\nprivate static final String INDEX_TOPIC = \"house_ build\";\n```\n\n##### 另外呢，还需要去定义一个kafka的template:\n\n```java\n@Autowired\nprivate KafkaTemplate<String, String> kafkaTemplate;\n```\n\n\n\n编辑一个index_topic的方法类，传入的是一个消息结构体（content）\n\n```java\n@KafkaListener(topics = INDEX_TOPIC)\nprivate void handleMessage(String content){\n\n}\n\n```\n\n另外呢，因为我们传入的是一个content，也就是请求消息结构体，index和remove的操作都包含在其中，所以我们新建一个HouseIndexMessage类：\n\n```java\npackage com.zryy.soufangtest.service.search;\n\npublic class HouseIndexMessage {\n\n    public static final String REMOVE = \"remove\";\n    public static final String INDEX = \"index\";\n\n    public static final int MAX_RETRY = 3;\n\n\n\n    /**\n     * 默认构造器，防止jackson序列化失败\n     */\n    public HouseIndexMessage() {\n    }\n\n    private Long houseId;\n    private String operation;\n    private int retry = 0;\n\n    public HouseIndexMessage(Long houseId, String operation, int retry) {\n        this.houseId = houseId;\n        this.operation = operation;\n        this.retry = retry;\n    }\n\n    public Long getHouseId() {\n        return houseId;\n    }\n\n    public void setHouseId(Long houseId) {\n        this.houseId = houseId;\n    }\n\n    public String getOperation() {\n        return operation;\n    }\n\n    public void setOperation(String operation) {\n        this.operation = operation;\n    }\n\n    public int getRetry() {\n        return retry;\n    }\n\n    public void setRetry(int retry) {\n        this.retry = retry;\n    }\n}\n```\n\n里面主要的字段是设置了houseId、operation、retry，\n\n虽然我们传了houseId，但是实质上我们其实是对message进行的操作。\n\n\n\n\n\n然后我们去定义了handleMessage方法，使用objectMapper来读取content消息结构体，转换成HouseIndexMessage类，\n\n然后对其operation进行不同的操作：（index或者remove）\n\n```java\n@KafkaListener(topics = INDEX_TOPIC)\nprivate void handleMessage(String content){\n    try {\n        HouseIndexMessage message = objectMapper.readValue(content, HouseIndexMessage.class);\n        switch(message.getOperation()){\n            case HouseIndexMessage.INDEX:\n                this.createOrUpdateIndex(message);\n                break;\n            case HouseIndexMessage.REMOVE:\n                this.removeIndex(message);\n            default:\n                logger.warn(\"Not support message content\"+ content);\n                break;\n\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n        logger.error(\"Cannot parse json for \"+content , e);\n    }\n}\n```\n\n这样呢，我们新建了createOrUpdateIndex和removeIndex方法。\n\n需要把之前的index方法直接嵌套在里面：\n\n\n\n然后在这里：\n\n```java\nprivate void createOrUpdateIndex(HouseIndexMessage message){\n    Long houseId = message.getHouseId();\n    House house = houseRepository.findOne(houseId);\n    if(house == null){\n        logger.error(\"index house {} does not exist\",houseId);\n        //这样的话，如果出现失败，那么就需要重新入队列。\n      \tthis.index(houseId, message.getRetry() + 1);\n        return false;\n    }\n```\n\n然后在this.index中重新进行消息入列。所以这里的逻辑是我们增加了一个retry的次数，不然的话就是一个死循环了。\n\n我们定义了这样的两个index函数。\n\n```\n@Override\npublic void index(Long houseId) {\n    this.index(houseId,0);\n\n}\nprivate void index(Long houseId, int retry){\n    if(retry > HouseIndexMessage.MAX_RETRY){\n        logger.error(\"Retry index times over 3 for house: \" + houseId + \"Please check it!\"   );\n        return;\n    }\n    HouseIndexMessage message = new HouseIndexMessage(houseId, HouseIndexMessage.INDEX, retry);\n    try {\n        //kafka重新对topic发送消息体\n        kafkaTemplate.send(INDEX_TOPIC, objectMapper.writeValueAsString(message));\n    } catch (JsonProcessingException e) {\n        logger.error(\"Json encode error for \" + message);\n\n    }\n}\n```\n\n即，在ISearchService定义的接口index中，我们去访问自己新定义的index（带了retry次数）然后去判断retry次数有没有超过设置的最大次数，如果没有超过那么就去执行kafkaTemplate.send(INDEX_TOPIC, objectMapper.writeValueAsString(message));\n\n然后执行kafkaTemplate.send后，又发送给了\n\n```java\nkafkaTemplate.send(INDEX_TOPIC, objectMapper.writeValueAsString(message));\n```\n\n\n\n```java\n@KafkaListener(topics = INDEX_TOPIC)\nprivate void handleMessage(String content){\n}\n```\n\n因为send的时候是发送了一个参数，发送给了Index_topic，所以在handleMessage函数中我们又设置了@KafkaListener监听。\n\n这样的话我们的createOrUpdate就实现了，接下来去实现remove的逻辑。\n\n\n\n\n\nremove同样是一样的逻辑：\n\n我们定义了removeIndex：\n\n```java\nprivate void removeIndex(HouseIndexMessage message){\n    Long houseId = message.getHouseId();\n    DeleteByQueryRequestBuilder builder = DeleteByQueryAction.INSTANCE\n            .newRequestBuilder(esclient)\n            .filter(QueryBuilders.termQuery(HouseIndexKey.HOUSE_ID, houseId))    //query出来的东西都会被删除掉\n            .source(INDEX_NAME);//设置source，也就是索引名\n\n\n    logger.debug(\"Delete by query for house: \" + builder);\n\n\n    BulkByScrollResponse response = builder.get();\n    long deleted = response.getDeleted();\n    logger.debug(\"Delete total: \" + deleted);\n\n    if( deleted <= 0){\n        this.remove(houseId, message.getRetry() +1);\n    }\n}\n```\n\n然后去获得了responese.getDeleted，得到了删除的数量，如果删除失败的话，其返回值是负值的，所以进行了this.remove重试的操作。\n\n\n\n测试通过，另外需要注意的一点是，在switch的时候，要注意加上break；另外对于kafka的index_topic不能中间有空格，不然会导致错误：\n\n```shell\nError while fetching metadata with correlation id 0  INVALID_TOPIC_EXCEPTION\n```\n\n\n\n## 三十七、搜索引擎实现\n\n### 搜索功能\n\n- ​\t搜索业务简单集成ES\n- ​    ES与MySQL结合\n\n\n\n正常情况下是通过mysql去查出相关信息的，但是在关键词搜索框中，需要通过es去查询出相关的信息，再通过mysql查询出更多详细的信息进行展示。\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h6f3kg3ijqj218s090t98.jpg\" alt=\"image-20220918123354761\" style=\"zoom:50%;\" />\n\n\n\n首先去定义一下我们的接口ISearchService\n\n```java\n/**\n * 查询房源接口\n * @param rentSearch\n * @return\n */\n\nServiceMultiResult<Long> query(RentSearch rentSearch);\n```\n\n\n\n然后，就定义实现类：\n\n首先构建QueryBuilders.boolQuery，（QueryBuilder 是es中提供的一个查询接口, 可以对其进行参数设置来进行数据查询）\n\n然后对构建好的boolQuery进行filter\n\n对city_en_name和Region_en_name\n\n面积和价格范围区间进行筛选等等\n\n然后使用esclient进行查询，然后返回结果等等。\n\n```java\n@Override\npublic ServiceMultiResult<Long> query(RentSearch rentSearch) {\n\n    BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();\n\n    boolQuery.filter(\n            QueryBuilders.termQuery(HouseIndexKey.CITY_EN_NAME,rentSearch.getCityEnName())\n    );\n\n    if(rentSearch.getRegionEnName() !=null && !\"*\".equals(rentSearch.getRegionEnName())){\n        boolQuery.filter(\n                QueryBuilders.termQuery(HouseIndexKey.REGION_EN_NAME,rentSearch.getRegionEnName())\n        );\n    }\n\n    //面积\n    RentValueBLock area = RentValueBLock.matchArea(rentSearch.getAreaBlock());\n    if(!RentValueBLock.ALL.equals(area)){\n        RangeQueryBuilder rangeQueryBuilder = QueryBuilders.rangeQuery(HouseIndexKey.AREA);\n        if(area.getMax() > 0){\n            rangeQueryBuilder.lte(area.getMax());\n        }\n        if(area.getMin() > 0){\n            rangeQueryBuilder.gte(area.getMin());\n        }\n        boolQuery.filter(rangeQueryBuilder);\n    }\n\n    //价格\n    RentValueBLock price = RentValueBLock.matchArea(rentSearch.getPriceBlock());\n    if(!RentValueBLock.ALL.equals(price)){\n        RangeQueryBuilder rangeQueryBuilder = QueryBuilders.rangeQuery(HouseIndexKey.PRICE);\n        if(price.getMax() > 0){\n            rangeQueryBuilder.lte(price.getMax());\n        }\n        if(price.getMin() > 0){\n            rangeQueryBuilder.gte(price.getMin());\n        }\n        boolQuery.filter(rangeQueryBuilder);\n    }\n    //朝向\n    if(rentSearch.getDirection() > 0){\n        boolQuery.filter(\n                QueryBuilders.termQuery(HouseIndexKey.DIRECTION, rentSearch.getDirection())\n        );\n    }\n\n /*   boolQuery.must(\n            QueryBuilders.multiMatchQuery(\n                    rentSearch.getKeywords(),\n                    HouseIndexKey.TITLE,\n                    HouseIndexKey.TRAFFIC,\n                    HouseIndexKey.DISTRICT,\n                    HouseIndexKey.ROUND_SERVICE,\n                    HouseIndexKey.SUBWAY_LINE_NAME,\n                    HouseIndexKey.SUBWAY_STATION_NAME\n                    )\n    );*/\n\n    SearchRequestBuilder requestBuilder = this.esclient.prepareSearch(INDEX_NAME)\n            .setTypes(INDEX_TYPE)\n            .setQuery(boolQuery)\n            .addSort(\n                    HouseSort.getSortKey(rentSearch.getOrderBy()),\n                    SortOrder.fromString(rentSearch.getOrderDirection())\n            )\n            .setFrom(rentSearch.getStart())\n            .setSize(rentSearch.getSize());\n\n    logger.debug(\"es请求：\\n\"+ requestBuilder.toString());\n\n\n    List<Long> houseIds = new ArrayList<>();\n    SearchResponse response = requestBuilder.get();\n    if(response.status() != RestStatus.OK){\n        logger.warn(\"Search status is no ok for\" + requestBuilder);\n        return new ServiceMultiResult<>(0,houseIds);\n    }\n    response.getHits().forEach(hit ->{\n        System.out.println(hit.getSource());\n        houseIds.add(Longs.tryParse(String.valueOf(hit.getSource().get(HouseIndexKey.HOUSE_ID))));\n    });\n\n    return new ServiceMultiResult<>(response.getHits().totalHits,houseIds);\n}\n```\n\n\n\n另外在我们的controller中rent/house接口下，我们调用的是\n\n```java\nServiceMultiResult<HouseDTO> serviceMultiResult = houseService.query(rentSearch);\t\t\n```\n\n这里是查询的mysql数据库，刚才我们定义的并不是houseService的query，而是searchService的query。\n\n所以我们要想办法让其结合起来。\n\n我们在HouseServiceImpl中的query进行添加：\n\n``` java\n@Override\npublic ServiceMultiResult<HouseDTO> query(RentSearch rentSearch) {\n\n    if(rentSearch.getKeywords() !=null && !rentSearch.getKeywords().isEmpty()){\n        ServiceMultiResult<Long> serviceResult =  searchService.query(rentSearch);\n        if(serviceResult.getTotal() == 0){\n            return new ServiceMultiResult<>(0,new ArrayList<>());\n        }\n        return new ServiceMultiResult<>(serviceResult.getTotal(),\n                wrapperHouseResult(serviceResult.getResult()));\n        //如果获得的总数不为零，也就是结果不为空\n    }\n```\n\n然后去实现wrapperHouseResult。\n\n``` java\nprivate List<HouseDTO> wrapperHouseResult(List<Long> houseIds){\n\n    ArrayList<HouseDTO> houseDTOS = new ArrayList<>();\n    houseIds.forEach(houseId->{\n        House house = houseRepository.findOne(houseId);\n        HouseDTO houseDTO = modelMapper.map(house,HouseDTO.class);\n        houseDTOS.add(houseDTO);\n    });\n}\n```\n\n起初我是这么去编辑的这里面的逻辑，但是用ArraryList不太合适，使用HashMap更加合适一些。\n\n因为可以做一个id To House的映射，给后面渲染house更多详情做准备。\n\n```java\nprivate List<HouseDTO> wrapperHouseResult(List<Long> houseIds){\n\n    Map<Long, HouseDTO> idToHouseMap = new HashMap<>();\n    Iterable<House> houses = houseRepository.findAll(houseIds);\n    houses.forEach(house->{\n        HouseDTO houseDTO = modelMapper.map(house,HouseDTO.class);\n        houseDTO.setCover(this.cdnPrefix+house.getCover());\n        idToHouseMap.put(house.getId(),houseDTO);\n    });\n}\n```\n\n这样的话就建立了一个idToHouseMap做了一个id到houseDTO的映射\n\n\n\n然后需要把houseDetails加进来\n\n```java\nwrapperHouseList(houseIds,idToHouseMap);\n```\n\n因为在之前已经写好了这个方法，所以现在也明白了为什么需要建一个id 2 house的变量了，\n\n\n\n另外需要矫正顺序，因为es查出来的houseid和mysql查出来的id顺序并不完全一致，所以这里需要进行一个矫正顺序。\n\n\n\n所以最后HouseServiceImpl中的query方法：\n\n```java\n@Override\n    public ServiceMultiResult<HouseDTO> query(RentSearch rentSearch) {\n\n        if(rentSearch.getKeywords() !=null && !rentSearch.getKeywords().isEmpty()){\n            ServiceMultiResult<Long> serviceResult =  searchService.query(rentSearch);\n            if(serviceResult.getTotal() == 0){\n                return new ServiceMultiResult<>(0,new ArrayList<>());\n            }\n            return new ServiceMultiResult<>(serviceResult.getTotal(),\n                    wrapperHouseResult(serviceResult.getResult()));\n            //如果获得的总数不为零，也就是结果不为空\n        }\n        //定义排序字段类\n//        Sort sort = new Sort(Sort.Direction.DESC, \"lastUpdateTime\");\n        //自定义的排序方法-修改后\n        Sort sort = HouseSort.generateSort(rentSearch.getOrderBy(), rentSearch.getOrderDirection());\n\n        //定义页面\n        int page = rentSearch.getStart() / rentSearch.getSize();\n\n        //* @param page 页面开始的序号\n        //  * @param size 页面需要返回的数量\n        //  * @param sort sort类\n        Pageable pageable = new PageRequest(page, rentSearch.getSize(), sort);\n\n        //然后还需要定义特殊条件，比如逻辑删除的数据不被查询出来，根据状态、城市名称等\n        Specification<House> specification = (root, criteriaQuery, criteriaBuilder) ->{\n            Predicate predicate = criteriaBuilder.equal(root.get(\"status\"), HouseStatus.PASSES.getValue());\n            criteriaBuilder.and(predicate,criteriaBuilder.equal(root.get(\"cityEnName\"),rentSearch.getCityEnName()));\n\n            return predicate;\n        };\n\n        List<Long> houseIds = new ArrayList<>();\n        Map<Long, HouseDTO> idToHouseMap = Maps.newHashMap();\n\n        //这里我们使用的JPA的findAll，传递的参数第一个是特殊查询变量，第二个是分页\n        Page<House> houses = houseRepository.findAll(specification, pageable);\n        //因为返回的是一个list列表的形式，所以我们这里要定义一个HouseDTO类型的列表\n        ArrayList<HouseDTO> houseDTOS = new ArrayList<>();\n\n        houses.forEach(house -> {\n            HouseDTO houseDTO = modelMapper.map(house, HouseDTO.class);\n            //奇怪的是这里为什么要重复的去设置下cover呢。初步猜测原因是cover有时候会更新不出来，所以做一个异步的延时加载\n            houseDTO.setCover(this.cdnPrefix + house.getCover());\n            houseDTOS.add(houseDTO);\n            houseIds.add(house.getId());\n            idToHouseMap.put(house.getId(),houseDTO);\n        });\n\n\n\n        wrapperHouseList(houseIds,idToHouseMap);\n\n        //最后返回ServiceMultiResult\n        return new  ServiceMultiResult<>(houses.getTotalElements(), houseDTOS);\n    }\n```\n\n\n\n但是上面显得比较复杂，es查询和mysql查询的应该区分开，所以我们把mysql的查询给封装起来。\n\n\n\n> 另外呢，我们对单独的类打了debug级别的日志，用于输出。\n\n```java\nlogging.level.com.zryy.soufangtest.service.search=debug\n```\n\n\n\n另外呢，这里我们对于es查询的设置了multiMatchQuery查询：\n\n```java\nboolQuery.must(\n        QueryBuilders.multiMatchQuery(\n                rentSearch.getKeywords(),\n                HouseIndexKey.TITLE,\n                HouseIndexKey.TRAFFIC,\n                HouseIndexKey.DISTRICT,\n                HouseIndexKey.ROUND_SERVICE,\n                HouseIndexKey.SUBWAY_LINE_NAME,\n                HouseIndexKey.SUBWAY_STATION_NAME\n                )\n);\n```\n\nrentSearch.getKeywords()是关键词，\n\nHouseIndexKey.TITLE \\ TRAFFIC \\ DISTRICT 等等都是需要查询的字段。\n\n\n\n另外呢，当我们搜索富力城的时候，像华侨城这样的也会被搜索出来，因为这是中文分词的问题，所以下一节我们需要处理中文分词的问题。\n\n\n\n## 三十八、中文分词-问题描述\n\n我们先测一下分词的效果：\n\n```shell\nGET http://localhost:9200/_analyze?analyzer=standard&pretty=true&text=Well,zhiqiang is a handsome teacher\nAccept: application/json\n\n<> 2022-09-20T110032.200.json\n```\n\n 结果如下：\n\n```json\nhttp://localhost:9200/_analyze\n\nHTTP/1.1 200 OK\nWarning: 299 Elasticsearch-5.6.1-667b497 \"text request parameter is deprecated and will be removed in the next major release. Please use the JSON in the request body instead request param\" \"Tue, 20 Sep 2022 03:00:31 GMT\"\nWarning: 299 Elasticsearch-5.6.1-667b497 \"analyzer request parameter is deprecated and will be removed in the next major release. Please use the JSON in the request body instead request param\" \"Tue, 20 Sep 2022 03:00:31 GMT\"\ncontent-type: application/json; charset=UTF-8\n\n{\n  \"tokens\": [\n    {\n      \"token\": \"well\",\n      \"start_offset\": 0,\n      \"end_offset\": 4,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"zhiqiang\",\n      \"start_offset\": 5,\n      \"end_offset\": 13,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"is\",\n      \"start_offset\": 14,\n      \"end_offset\": 16,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"a\",\n      \"start_offset\": 17,\n      \"end_offset\": 18,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"handsome\",\n      \"start_offset\": 19,\n      \"end_offset\": 27,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"teacher\",\n      \"start_offset\": 28,\n      \"end_offset\": 35,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 5\n    }\n  ]\n}\n```\n\n当然呢这里是英文，分词效果好很多。我们这里测试一下中文的情况：\n\n```json\n{\n  \"tokens\": [\n    {\n      \"token\": \"志\",\n      \"start_offset\": 0,\n      \"end_offset\": 1,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"强\",\n      \"start_offset\": 1,\n      \"end_offset\": 2,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"是\",\n      \"start_offset\": 2,\n      \"end_offset\": 3,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"青\",\n      \"start_offset\": 3,\n      \"end_offset\": 4,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"岛\",\n      \"start_offset\": 4,\n      \"end_offset\": 5,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"国\",\n      \"start_offset\": 5,\n      \"end_offset\": 6,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 5\n    },\n    {\n      \"token\": \"际\",\n      \"start_offset\": 6,\n      \"end_offset\": 7,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"创\",\n      \"start_offset\": 7,\n      \"end_offset\": 8,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 7\n    },\n    {\n      \"token\": \"新\",\n      \"start_offset\": 8,\n      \"end_offset\": 9,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 8\n    },\n    {\n      \"token\": \"园\",\n      \"start_offset\": 9,\n      \"end_offset\": 10,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 9\n    },\n    {\n      \"token\": \"最\",\n      \"start_offset\": 10,\n      \"end_offset\": 11,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 10\n    },\n    {\n      \"token\": \"帅\",\n      \"start_offset\": 11,\n      \"end_offset\": 12,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 11\n    },\n    {\n      \"token\": \"的\",\n      \"start_offset\": 12,\n      \"end_offset\": 13,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 12\n    },\n    {\n      \"token\": \"男\",\n      \"start_offset\": 13,\n      \"end_offset\": 14,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 13\n    },\n    {\n      \"token\": \"人\",\n      \"start_offset\": 14,\n      \"end_offset\": 15,\n      \"type\": \"<IDEOGRAPHIC>\",\n      \"position\": 14\n    }\n  ]\n}\n```\n\n所以，我们新建了一个测试的索引用来测试：\n\n```json\nPUT http://localhost:9200/test\nContent-Type: application/json\n\n{\n  \"settings\": {\n    \"number_of_replicas\": 0\n  },\n  \"mappings\": {\n    \"house\": {\n      \"dynamic\": false,\n      \"properties\": {\n        \"value\": {\n          \"type\": \"text\"\n        }\n      }\n    }\n  }\n}\n```\n\n然后增加一些内容进去：\n\n```json\nPOST http://localhost:9200/test/house\nContent-Type: application/json\n\n{\n  \"value\": \"你从中学到了什么\"\n}\n```\n\n等等\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h6d5fc9nqsj20oq05a74w.jpg\" alt=\"image-20220920162642558\" style=\"zoom:67%;\" />\n\n\n\n## 三十九、中文分词-IK配置\n\n\n\n下载elasticSearch-IK放置在es文件夹的plugins目录下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h6f3k6dey0j21f20niabz.jpg\" alt=\"image-20220920163116793\" style=\"zoom: 50%;\" />\n\n或者当版本大于5.5.1的时候，可以直接执行命令进行安装。\n\n\n\n```shell\n./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analusis-ik/releases/download/v5.6.1/elasticsearch-analysisi-ik-5.6.1.zip\n```\n\n  注意，一定要选择对应的版本。\n\n然后需要重新启动es。\n\n这个时候虽然ik分词器被加载进来了，但是还需要我们去重新定义索引，之前我们定义的索引是标准分词器。\n\n这时候我们就需要把之前创建的索引删除掉了：\n\n```shell\nDELETE http://localhost:9200/test\nAccept: application/json\n```\n\n\n\n然后重新创建索引:\n\n```java\nPUT http://localhost:9200/test\nContent-Type: application/json\n\n{\n  \"settings\": {\n    \"number_of_replicas\": 0\n  },\n  \"mappings\": {\n    \"house\": {\n      \"dynamic\": false,\n      \"properties\": {\n        \"value\": {\n          \"type\": \"text\",\n          \"analyzer\": \"ik_max_word\",\n          \"search_analyzer\": \"ik_max_word\"\n        }\n      }\n    }\n  }\n}\n```\n\n然后再发送get请求进行测试：\n\n```json\n###\nGET http://localhost:9200/_analyze?analyzer=ik_max_word&pretty=true&text=志强是青岛国际创新园最帅的男人\nAccept: application/json\n```\n\n结果如下：\n\n```json\n{\n  \"tokens\": [\n    {\n      \"token\": \"志\",\n      \"start_offset\": 0,\n      \"end_offset\": 1,\n      \"type\": \"CN_CHAR\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"强\",\n      \"start_offset\": 1,\n      \"end_offset\": 2,\n      \"type\": \"CN_CHAR\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"是\",\n      \"start_offset\": 2,\n      \"end_offset\": 3,\n      \"type\": \"CN_CHAR\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"青岛\",\n      \"start_offset\": 3,\n      \"end_offset\": 5,\n      \"type\": \"CN_WORD\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"岛国\",\n      \"start_offset\": 4,\n      \"end_offset\": 6,\n      \"type\": \"CN_WORD\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"国际\",\n      \"start_offset\": 5,\n      \"end_offset\": 7,\n      \"type\": \"CN_WORD\",\n      \"position\": 5\n    },\n    {\n      \"token\": \"创新\",\n      \"start_offset\": 7,\n      \"end_offset\": 9,\n      \"type\": \"CN_WORD\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"园\",\n      \"start_offset\": 9,\n      \"end_offset\": 10,\n      \"type\": \"CN_CHAR\",\n      \"position\": 7\n    },\n    {\n      \"token\": \"最\",\n      \"start_offset\": 10,\n      \"end_offset\": 11,\n      \"type\": \"CN_CHAR\",\n      \"position\": 8\n    },\n    {\n      \"token\": \"帅\",\n      \"start_offset\": 11,\n      \"end_offset\": 12,\n      \"type\": \"CN_CHAR\",\n      \"position\": 9\n    },\n    {\n      \"token\": \"的\",\n      \"start_offset\": 12,\n      \"end_offset\": 13,\n      \"type\": \"CN_CHAR\",\n      \"position\": 10\n    },\n    {\n      \"token\": \"男人\",\n      \"start_offset\": 13,\n      \"end_offset\": 15,\n      \"type\": \"CN_WORD\",\n      \"position\": 11\n    }\n  ]\n}\n```\n\n\n\n> 另外呢，可以在/Users/zhiqiang/Downloads/elasticsearch-5.6.1/config/analysis-ik目录下，找到IKAnalyzer.cfg.xml文件，扩展自己的分词词典。\n\n![image-20220920170025078](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h6d6eg0bd5j20jd0c1wf9.jpg)\n\n\n\n所以我们重新创建了一份建立索引的mapping.json文件：\n\n在需要分词的字段中都加入（**我们的建立索引和搜索索引都要使用分词器**）：\n\n```json\n \"analyzer\": \"ik_smart\",\n \"search_analyzer\": \"ik_smart\"\n```\n\n还有就是新建的索引，需要把之前建立的索引删除掉.\n\n\n\n我们在test索引中进行了测试：\n\n```java\nPOST http://localhost:9200/test/house/_search\nContent-Type: application/json\n\n{\n  \"query\": {\n      \"match\": {\n        \"value\": \"中国人\"\n      }\n  }\n}\n```\n\n搜索结果如下：\n\n```json\n{\n  \"took\": 159,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 1,\n    \"max_score\": 0.6548752,\n    \"hits\": [\n      {\n        \"_index\": \"test\",\n        \"_type\": \"house\",\n        \"_id\": \"AYNaU2YlB6J9J6YWOWJ0\",\n        \"_score\": 0.6548752,\n        \"_source\": {\n          \"value\": \"我是中国人\"\n        }\n      }\n    ]\n  }\n}\n```\n\n\n\n我们在重新删除之前没有分词设置的mapping—index后，重新创建带有分词设置的mapping-index，然后启动程序重新下架上架，重新倒入到es中，经过测试：\n\n```json\nPOST http://localhost:9200/house/house2/_search\nContent-Type: application/json\n\n{\n  \"query\": {\n      \"match\": {\n        \"title\": \"精装修\"\n      }\n  }\n}\n```\n\n结果成功：\n\n```json\nhttp://localhost:9200/house/house2/_search\n\nHTTP/1.1 200 OK\ncontent-type: application/json; charset=UTF-8\n\n{\n  \"took\": 24,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 1,\n    \"max_score\": 0.68640786,\n    \"hits\": [\n      {\n        \"_index\": \"house\",\n        \"_type\": \"house2\",\n        \"_id\": \"AYNad2YTN7uKa3tweIKJ\",\n        \"_score\": 0.68640786,\n        \"_source\": {\n          \"houseId\": 21,\n          \"title\": \"新康园 正规三居室 精装修 家电家具齐全1\",\n          \"price\": 1900,\n          \"area\": 18,\n          \"createTime\": 1504716767000,\n          \"lastUpdateTime\": 1663670117000,\n          \"cityEnName\": \"bj\",\n          \"regionEnName\": \"hdq\",\n          \"direction\": 3,\n          \"distanceToSubway\": 1302,\n          \"subwayLineName\": \"13号线\",\n          \"subwayStationName\": \"霍营\",\n          \"street\": \"龙域西二路\",\n          \"district\": \"融泽嘉园\",\n          \"description\": \"房子是正规三室一厅一厨一卫，装修保持的不错，家电家具都齐全。\\n\",\n          \"layoutDesc\": \"房子客厅朝北面积比较大，主卧西南朝向，次卧朝北，另一个次卧朝西，两个次卧面积差不多大。\",\n          \"traffic\": \"小区出南门到8号线育新地铁站614米，交通便利，小区500米范围内有物美，三旗百汇，龙旗广场等几个比较大的商场，生活购物便利，出小区北门朝东952米是地铁霍营站，是8号线和 13号线的换乘站，同时还有个S2线，通往怀来。（数据来源百度地图）\",\n          \"roundService\": \"小区西边300米就是物美超市和三旗百汇市场（日常百货、粮油米面、瓜果蔬菜、生鲜海货等等，日常生活很便利，消费成本低），北边200米是龙旗购物广场和永辉超市（保利影院，KFC，麦当劳等，轻松满足娱乐消费）。小区里还有商店，饭店，家政等。\",\n          \"rentWay\": 0,\n          \"tags\": [\n            \"独立阳台\"\n          ]\n        }\n      }\n    ]\n  }\n}\n```\n\n我在搜索的时候，输入“吃饭”也能检索出来，在roundService中”吃“这个字没有，但是”饭“这个字有。所以被检索出来了。\n\n\n\n另外呢，在查看kakfa的时候，如果出现了：\n\n```shell\nException thrown when sending a message with key='null' and \t\t\n```\n\n\n\n这样就是kafka挂掉了，然后在检查的时候收获了：\n\n输入了\n\n```shell\n./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic house_build --from-beginning\t\n```\n\n来监控“house_build\" 这个topic内容，发现：\n\n![image-20220920183750153](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h6d97rysorj20g90ab0uj.jpg)\n\nkafka — house_build里面存放的是这样的数值。 也就是传递的message。\n\n不过不理解的是为什么这样的内容传递到了kafka中，虽然在\n\n```java\n@KafkaListener(topics = INDEX_TOPIC)\nprivate void handleMessage(String content){\ntry {\n            HouseIndexMessage message = objectMapper.readValue(content, HouseIndexMessage.class);\n            switch(message.getOperation()){\n                case HouseIndexMessage.INDEX:\n                    this.createOrUpdateIndex(message);\n                    break;\n                case HouseIndexMessage.REMOVE:\n                    this.removeIndex(message);\n                    break;\n                default:\n                    logger.warn(\"Not support message content\"+ content);\n                    break;\n\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n            logger.error(\"Cannot parse json for \"+content , e);\n        }\n    }\n```\n\n> 中进行了es的处理，但是kafka这里面的值什么时候删除掉呢？\n\n\n\n## 四十、Search-as-you-type\n\n> 主要是基于es的suggest来实现的\n\n我们简单定义了一个controller的接口：\n\n```java\n    /**\n     * 自动补全接口\n     */\n    @GetMapping(\"rent/house/autocomplete\")\n    @ResponseBody\n    public ApiResponse autocomplete(@RequestParam(value = \"prefix\") String prefix){\n\n        if(prefix.isEmpty()){\n            return ApiResponse.ofStatus(ApiResponse.Status.NOT_FOUND);\n        }\n        List<String> result = new ArrayList<>();\n        result.add(\"超棒\");\n        result.add(\"志强\");\n        return ApiResponse.ofSuccess(result);\n    }\n```\n\n显示的效果如下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h6f3ju1q8cj21uw0mojsa.jpg\" alt=\"image-20220921091903028\" style=\"zoom: 33%;\" />\n\n\n\n\n\n在ISearchService中新建suggest，然后在impl中去新建：\n\n``` java\n@Override\npublic ServiceResult<List<String>> suggest(String prefix) {\n    return null;\n}\n```\n\n当然在执行任何操作之前，我们需要对suggest新建一个专门的类：\n\n```java\npackage com.zryy.soufangtest.service.search;\n\npublic class HouseSuggest {\n\n    private String input;\n    private int weight = 10; //默认权重\n\n    public String getInput() {\n        return input;\n    }\n\n    public void setInput(String input) {\n        this.input = input;\n    }\n\n    public int getWeight() {\n        return weight;\n    }\n\n    public void setWeight(int weight) {\n        this.weight = weight;\n    }\n}\n```\n\n然后也要在HouseIndexTemplate模版中添加这个字段，并且设置set get方法：\n\n```java\nprivate HouseSuggest houseSuggest;\n\npublic HouseSuggest getHouseSuggest() {\n    return houseSuggest;\n}\n\npublic void setHouseSuggest(HouseSuggest houseSuggest) {\n    this.houseSuggest = houseSuggest;\n}\n```\n\n然后，既然我们在houseIndexTempalte中增加了一个新的字段，所以在索引的时候我们的es中也需要去增加一个新的字段，所以我们需要在创建的index索引mapping中重新创建一个新的。\n\n```json\n\"suggest\": {\n  \"type\": \"completion\"\n}\n```\n\n 也就是在最后增加了这个字段。\n\n```json\n{\n  \"settings\":{\n    \"number_of_replicas\": 0\n  },\n  \"mappings\":{\n    \"house\":{\n      \"dynamic\": false,\n      \"properties\":{\n        \"houseId\": {\n          \"type\": \"long\"\n        },\n        \"title\":{\n          \"type\":  \"text\",\n          \"index\": \"analyzed\",\n          \"analyzer\": \"ik_smart\",\n          \"search_analyzer\": \"ik_smart\"\n        },\n        \"price\":{\n          \"type\":  \"integer\"\n        },\n        \"area\":{\n          \"type\":  \"integer\"\n        },\n        \"createTime\":{\n          \"type\":  \"date\",\n          \"format\": \"strict_date_optional_time||epoch_millis\"\n        },\n        \"lastUpdateTime\":{\n          \"type\":\"date\",\n          \"format\": \"strict_date_optional_time||epoch_millis\"\n        },\n        \"cityEnName\":{\n          \"type\":\"keyword\"\n        },\n        \"regionEnName\":{\n          \"type\":\"keyword\"\n        },\n        \"direction\":{\n          \"type\":\"integer\"\n        },\n        \"distanceToSubway\":{\n          \"type\":\"integer\"\n        },\n        \"subwayLineName\":{\n          \"type\":\"keyword\"\n        },\n        \"subwayStationName\":{\n          \"type\":\"keyword\"\n        },\n        \"tags\":{\n          \"type\":\"text\"\n        },\n        \"street\":{\n          \"type\":\"keyword\"\n        },\n        \"district\":{\n          \"type\":\"keyword\"\n        },\n        \"description\":{\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"layoutDesc\":{\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"traffic\":{\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"roundService\":{\n          \"type\":\"text\",\n          \"index\":\"analyzed\"\n        },\n        \"rentWay\":{\n          \"type\":\"integer\"\n        },\n        \"suggest\": {\n          \"type\": \"completion\",\n          \"analyzer\": \"ik_smart\",\n          \"search_analyzer\": \"ik_smart\"\n        }\n      }\n    }\n  }\n}\n```\n\n\n\n在获取到关键词之前呢，还需要对suggest进行一些填充操作。\n\n```java\n//主要在对index进行操作的时候处理，比如create，update的时候\nprivate boolean updateSuggest(HouseIndexTemplate indexTemplate){\n    AnalyzeRequestBuilder requestBuilder = new AnalyzeRequestBuilder(this.esclient,\n            AnalyzeAction.INSTANCE,\n            indexTemplate.getTitle(),\n            indexTemplate.getLayoutDesc(),\n            indexTemplate.getRoundService(),\n            indexTemplate.getDescription(),\n            indexTemplate.getSubwayLineName(),\n            indexTemplate.getSubwayStationName());\n\n    requestBuilder.setAnalyzer(\"ik_smart\");\n\n    AnalyzeResponse analyzeResponse = requestBuilder.get();\n    //结果中是分词后的\n    List<AnalyzeResponse.AnalyzeToken> tokens = analyzeResponse.getTokens();\n    if(tokens == null){\n        logger.warn(\"Can not analyze token for house: \" + indexTemplate.getHouseId());\n\n        return false;\n    }\n\n    List<HouseSuggest> suggests = new ArrayList<>();\n    for(AnalyzeResponse.AnalyzeToken token: tokens){\n        //排序数字类型 & 小于2个字符的分词结果\n        if( \"<NUM>\".equals(token.getType()) || token.getTerm().length() < 2){\n            continue;\n        }\n\n        HouseSuggest suggest = new HouseSuggest();\n        //如果对其权重有一些更新的话，在这里修改一下权重就可以了\n        suggest.setInput(token.getTerm());\n        suggests.add(suggest);\n    }\n\n    //这里只是对一些需要分词的字段进行，但是对一些不需要分词的比如keyword的字段该怎么做呢？\n\n    //定制化小区自动补全\n    HouseSuggest suggest = new HouseSuggest();\n    suggest.setInput(indexTemplate.getDistrict());\n    suggests.add(suggest);\n    indexTemplate.setSuggests(suggests);\n    \n    return true;\n}\n```\n\n\n\n\n\n然后在定义的suggest方法中，首先构建一个suggestBuilders.completionSuggestion\n\n第一个参数传递的是在es索引中我们创建的字段，这里我们定义了“suggest”\n\n所以第一个参数传递的是fieldname类型，然后.prefix    .size(5) 设置补全的数量。\n\n\n\n```java\n@Override\npublic ServiceResult<List<String>> suggest(String prefix) {\n    CompletionSuggestionBuilder suggestion = SuggestBuilders.completionSuggestion(\"suggest\")\n            .prefix(prefix)\n            .size(5);\n```\n\n> 出现了错误，最终在排查问题的时候，找到了这里：\n\n```java\n//主要在对index进行操作的时候处理，比如create，update的时候\nprivate boolean updateSuggest(HouseIndexTemplate indexTemplate){\n    AnalyzeRequestBuilder requestBuilder = new AnalyzeRequestBuilder(this.esclient,\n            AnalyzeAction.INSTANCE,\n            indexTemplate.getTitle(),\n            indexTemplate.getLayoutDesc(),\n            indexTemplate.getRoundService(),\n            indexTemplate.getDescription(),\n            indexTemplate.getSubwayLineName(),\n            indexTemplate.getSubwayStationName());\n```\n\n第一个参数是client，第二个参数是AnalyzeAction.INSTANCE，**第三个参数是INDEX_NAME（索引的名称）** 当我没有写INDEX_NAME的时候，会默认的把延后的参数即：indexTemplate.getTitle()当作了index_name\n\n于是出现的结果是虽然在kafka中加入了message，但是es中没有添加任何数据，错误如下：\n\n\n\n```shell\nrg.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'private void com.zryy.soufangtest.service.search.SearchServiceImpl.handleMessage(java.lang.String)' threw exception; nested exception is [大标题-编辑] IndexNotFoundException[no such index]\n\tat org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:188) ~[spring-kafka-1.1.6.RELEASE.jar:na]\n\tat org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:72) ~[spring-kafka-1.1.6.RELEASE.jar:na]\n\tat org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:47) ~[spring-kafka-1.1.6.RELEASE.jar:na]\n\tat org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:794) [spring-kafka-1.1.6.RELEASE.jar:na]\n\tat org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:738) [spring-kafka-1.1.6.RELEASE.jar:na]\n\tat org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:570) [spring-kafka-1.1.6.RELEASE.jar:na]\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_321]\n\tat java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) [na:1.8.0_321]\n\tat java.util.concurrent.FutureTask.run(FutureTask.java) [na:1.8.0_321]\n\tat java.lang.Thread.run(Thread.java:750) [na:1.8.0_321]\nCaused by: org.elasticsearch.index.IndexNotFoundException: no such index\n```\n\n当时没有找到问题的原因，可以看到第二行中描述了：handleMessage中抛出的异常。nested exception is [大标题-编辑] IndexNotFoundException[no such index]，[大标题-编辑]这个index没有找到，\n\n然后在最下面，es也报出了这样的错误：\n\nCaused by: org.elasticsearch.index.IndexNotFoundException: no such index\n\n没有这个index\n\n所以修改之后的为下：(添加了index—name)\n\n```java\n//主要在对index进行操作的时候处理，比如create，update的时候\nprivate boolean updateSuggest(HouseIndexTemplate indexTemplate){\n    AnalyzeRequestBuilder requestBuilder = new AnalyzeRequestBuilder(this.esclient,\n            AnalyzeAction.INSTANCE,\n            INDEX_NAME,\n            indexTemplate.getTitle(),\n            indexTemplate.getLayoutDesc(),\n            indexTemplate.getRoundService(),\n            indexTemplate.getDescription(),\n            indexTemplate.getSubwayLineName(),\n            indexTemplate.getSubwayStationName());\n```\n\n这样的话，结果就正确了，在控制台中显示的如下：\n\n```shell\n2022-09-22 14:57:29.500 DEBUG 80951 --- [ntainer#0-0-C-1] c.z.s.service.search.ISearchService      : {\n  \"query\" : {\n    \"term\" : {\n      \"houseId\" : {\n        \"value\" : 29,\n        \"boost\" : 1.0\n      }\n    }\n  }\n}\n2022-09-22 14:57:30.175 DEBUG 80951 --- [ntainer#0-0-C-1] c.z.s.service.search.ISearchService      : Create index with house: 29\n2022-09-22 14:57:30.175 DEBUG 80951 --- [ntainer#0-0-C-1] c.z.s.service.search.ISearchService      : Index success with house 29\n```\n\n\n\n\n\n\n\n然后在输入提示中，设置的suggest，默认是simple这个analyzer的：\n\n```java\n\"suggest\": {\n\"max_input_length\": 50,\n\"analyzer\": \"simple\",\n\"preserve_position_increments\": true,\n\"type\": \"completion\",\n\"preserve_separators\": true\n```\n\n然后在代码中设置的分析器是：\n\n```java\nprivate boolean updateSuggest(HouseIndexTemplate indexTemplate) {\n    AnalyzeRequestBuilder requestBuilder = new AnalyzeRequestBuilder(\n            this.esClient, AnalyzeAction.INSTANCE, INDEX_NAME, indexTemplate.getTitle(),\n            indexTemplate.getLayoutDesc(), indexTemplate.getRoundService(),\n            indexTemplate.getDescription(), indexTemplate.getSubwayLineName(),\n            indexTemplate.getSubwayStationName());\n\n    requestBuilder.setAnalyzer(\"ik_smart\");\n```\n\n所以就导致了报错：找不到这个index \n\n> 考虑如何在mapping中设置suggest这个字段的analyzer_type?\n\n\n\n大体的逻辑就是在handleMessage的时候判断是否是create还是update操作，然后在create和update的一开始更新suggest（updateSuggest(houseIndexTemplate)）。也就是给传递的参数indexTemplate更新suggest字段操作\n\n在updateSuggest函数中：\n\n首先定义一个AnalyzeRequestBuilder，实质上就是分词分析器，结果是分好词的。传递的参数是indexTemplate.getTitle、LayoutDesc等等。\n\nrequestBuilder.get().getTokens()获取的是List <AnalyzeResponse.AnalyzeToken>\n\n然后循环遍历把suggests加入到indexTemplate中，然后返回出来，\n\n返回出来后，其实就是在create和updateindex方法中了，然后就是把indexTemplate转成json格式，然后创建或者更新索引。\n\n然后这是创建索引中的suggest字段。\n\n\n\n接下来就是suggest接口，请求的suggest返回的接口：\n\n```java\n@Override\npublic ServiceResult<List<String>> suggest(String prefix) {\n    CompletionSuggestionBuilder suggestion = SuggestBuilders.completionSuggestion(\"suggest\")\n            .prefix(prefix)\n            .size(5);\n\n    SuggestBuilder suggestBuilder = new SuggestBuilder();\n    suggestBuilder.addSuggestion(\"autocomplete\", suggestion);\n\n    SearchRequestBuilder requestBuilder = this.esclient.prepareSearch(INDEX_NAME)\n            .setTypes(INDEX_TYPE)\n            .suggest(suggestBuilder);\n\n    logger.debug(requestBuilder.toString());\n\n    SearchResponse response = requestBuilder.get();\n    Suggest suggest = response.getSuggest();\n    Suggest.Suggestion result  = suggest.getSuggestion(\"autocomplete\");\n    //获取的结果可能有重复 需要进行过滤\n\n    int maxSuggest = 0;\n    Set<String> suggestSet = new HashSet<>();\n\n    for(Object term : result.getEntries()){\n        if(term instanceof CompletionSuggestion.Entry){\n            CompletionSuggestion.Entry item = (CompletionSuggestion.Entry) term;\n\n            if(item.getOptions().isEmpty()){\n                continue;\n            }\n\n            for (CompletionSuggestion.Entry.Option option : item.getOptions()){\n                String tip = option.getText().string();\n                if(suggestSet.contains(tip)){\n                    continue;\n                }\n                suggestSet.add(tip);\n                maxSuggest++;\n            }\n        }\n\n        if(maxSuggest > 5 ){\n            break;\n        }\n    }\n    List<String> suggests = Lists.newArrayList(suggestSet.toArray(new String[]{}));\n    return ServiceResult.of(suggests);\n\n}\n```\n\n首先新建一个SuggestBuilders，然后实现completionSuggestion()，并且把suggest这个index字段当作参数扔进去。\n\n然后设置prefix 和设置size参数\n\n\n\n然后执行esclient去prepareSearch对suggestBuilder进行请求，返回的是SearchRequestBuilder的变量，\n\n然后通过get方法获得SearchResponse，然后\n\n对SearchResponse执行getSuggest（）方法，得到基于prefix的suggest字段，然后suggest.getSuggestion(\"autocomplete\");就得到补全的单词结果了。\n\n\n\n然后呢可能获取到的结果有重复，所以我们做一个过滤操作：\n\n\n\n```java\nint maxSuggest = 0;\nSet<String> suggestSet = new HashSet<>();\n\nfor(Object term : result.getEntries()){\n    if(term instanceof CompletionSuggestion.Entry){\n        CompletionSuggestion.Entry item = (CompletionSuggestion.Entry) term;\n\n        if(item.getOptions().isEmpty()){\n            continue;\n        }\n\n        for (CompletionSuggestion.Entry.Option option : item.getOptions()){\n            String tip = option.getText().string();\n            if(suggestSet.contains(tip)){\n                continue;\n            }\n            suggestSet.add(tip);\n            maxSuggest++;\n        }\n    }\n\n    if(maxSuggest > 5 ){\n        break;\n    }\n}\n```\n\n```java\nList<String> suggests = Lists.newArrayList(suggestSet.toArray(new String[]{}));\nreturn ServiceResult.of(suggests);\n```\n\n然后转换格式，把这个返回出去。\n\n最终的效果图如下：\n\n![image-20220923111040881](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h6gd5hrdwnj21ma0e0q4b.jpg)\n\n\n\n> 另外呢，我们会把用户常输入的一些关键词、热词等等，最常出现的词单独去**新建一个索引**放进去，这样做的话效果会更好一些。\n\n\n\n他妈的搞了半天，在houseIndexTemplate中把suggest定义成了suggests了，所以导致在补全接口的时候一直找不到相关内容。\n\n## 四十一、小区房源统计功能\n\n\n\n![image-20220923113046846](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h6gdqduo9ej21jy0pcwgq.jpg)\n\n\n\n我们之前在HouseController中的show方法中把houseCountInDistrict固定成了0了：\n\n```java\nmodel.addAttribute(\"houseCountInDistrict\", 0);\n```\n\n需要增加这样一个静态变量\n\n```java\npublic static final String AGG_DISTRICT = \"agg_district\";\n```\n\n```java\n@Override\npublic ServiceResult<Long> aggregateDistrictHouse(String cityEnName, String regionEnName, String district) {\n    //新建boolQueryBuilder\n    BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery()\n            .filter(QueryBuilders.termQuery(HouseIndexKey.CITY_EN_NAME,cityEnName))\n            .filter(QueryBuilders.termQuery(HouseIndexKey.REGION_EN_NAME,regionEnName))\n            .filter(QueryBuilders.termQuery(HouseIndexKey.DISTRICT,district));\n\n    SearchRequestBuilder requestBuilder = this.esclient.prepareSearch(INDEX_NAME)\n            .setTypes(INDEX_TYPE)\n            .setQuery(boolQueryBuilder)\n            .addAggregation(\n                    AggregationBuilders.terms(HouseIndexKey.AGG_DISTRICT)\n                            .field(HouseIndexKey.DISTRICT)\n            ).setSize(0);  //我们不需要原始的数据，只需要聚合的数据\n\n\n    logger.debug(requestBuilder.toString());\n\n    SearchResponse response = requestBuilder.get();\n    if(response.status() == RestStatus.OK){\n        Terms terms = response.getAggregations().get(HouseIndexKey.AGG_DISTRICT);\n        if(terms.getBuckets() !=null && !terms.getBuckets().isEmpty()){\n            return ServiceResult.of(terms.getBucketByKey(district).getDocCount());\n        };\n    }else{\n        logger.warn(\"Failed to Aggregate for \"+ HouseIndexKey.AGG_DISTRICT);\n\n    }\n    return ServiceResult.of(0L);\n}\n```\n\n其结果的json形式是这样的：\n\n```json\n  \"aggregations\" : {\n    \"agg_district\" : {\n      \"terms\" : {\n        \"field\" : \"district\",\n        \"size\" : 10,\n        \"min_doc_count\" : 1,\n        \"shard_min_doc_count\" : 0,\n        \"show_term_doc_count_error\" : false,\n        \"order\" : [\n          {\n            \"_count\" : \"desc\"\n          },\n          {\n            \"_term\" : \"asc\"\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\n\n\n思路就是先进行filter的筛选过滤，然后在进行聚合，然后把response中的 结果拿出来，就是我们定义了agg_district这样一个聚合字段，然后get(HouseIndexKey.AGG_DISTRICT) 拿出来，然后terms.getBucketByKey(district).getDocCount()把这个数量取出来。\n\n最终效果如下：\n\n\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h6gl0i1gqtj21iu0u0778.jpg\" alt=\"image-20220923154242836\" style=\"zoom:80%;\" />\n\n\n\n## 四十二、搜索引擎优化\n\n\n\n当我们搜索的时候\n\n```json\n{\n  \"from\" : 0,\n  \"size\" : 5,\n  \"query\" : {\n    \"bool\" : {\n      \"must\" : [\n        {\n          \"multi_match\" : {\n            \"query\" : \"融泽嘉园\",\n            \"fields\" : [\n              \"district^1.0\",\n              \"roundService^1.0\",\n              \"subwayLineName^1.0\",\n              \"subwayStationName^1.0\",\n              \"title^1.0\",\n              \"traffic^1.0\"\n            ],\n            \"type\" : \"best_fields\",\n            \"operator\" : \"OR\",\n            \"slop\" : 0,\n            \"prefix_length\" : 0,\n            \"max_expansions\" : 50,\n            \"lenient\" : false,\n            \"zero_terms_query\" : \"NONE\",\n            \"boost\" : 1.0\n          }\n        }\n      ],\n      \"filter\" : [\n        {\n          \"term\" : {\n            \"cityEnName\" : {\n              \"value\" : \"bj\",\n              \"boost\" : 1.0\n            }\n          }\n        }\n      ],\n      \"disable_coord\" : false,\n      \"adjust_pure_negative\" : true,\n      \"boost\" : 1.0\n    }\n  },\n  \"sort\" : [\n    {\n      \"lastUpdateTime\" : {\n        \"order\" : \"desc\"\n      }\n    }\n  ]\n}\n```\n\n在query方法中我们使用了boolQuery.must 方法，对Title、traffic、district等字段进行了查询，但是这几个字段的权重都是相同的也就是默认1.0的方式。\n\n```java\n//修改Title查询字段的权重\nboolQuery.must(\n        QueryBuilders.matchQuery(HouseIndexKey.TITLE,rentSearch.getKeywords())\n                .boost(2.0f)\n);\n```\n\n然后把QueryBuilder.multimatchQuery中的title字段删除掉：\n\n```java\nboolQuery.must(\n        QueryBuilders.multiMatchQuery(\n                rentSearch.getKeywords(),\n                HouseIndexKey.TRAFFIC,\n                HouseIndexKey.DISTRICT,\n                HouseIndexKey.ROUND_SERVICE,\n                HouseIndexKey.SUBWAY_LINE_NAME,\n                HouseIndexKey.SUBWAY_STATION_NAME\n                )\n);\n```\n\n这样的搜索json就变成了title权重为2.0了：\n\n```json\n{\n  \"from\" : 0,\n  \"size\" : 5,\n  \"query\" : {\n    \"bool\" : {\n      \"must\" : [\n        {\n          \"match\" : {\n            \"title\" : {\n              \"query\" : \"融泽嘉园\",\n              \"operator\" : \"OR\",\n              \"prefix_length\" : 0,\n              \"max_expansions\" : 50,\n              \"fuzzy_transpositions\" : true,\n              \"lenient\" : false,\n              \"zero_terms_query\" : \"NONE\",\n              \"boost\" : 2.0\n            }\n          }\n        },\n        {\n          \"multi_match\" : {\n            \"query\" : \"融泽嘉园\",\n            \"fields\" : [\n              \"district^1.0\",\n              \"roundService^1.0\",\n              \"subwayLineName^1.0\",\n              \"subwayStationName^1.0\",\n              \"title^1.0\",\n              \"traffic^1.0\"\n            ],\n            \"type\" : \"best_fields\",\n            \"operator\" : \"OR\",\n            \"slop\" : 0,\n            \"prefix_length\" : 0,\n            \"max_expansions\" : 50,\n            \"lenient\" : false,\n            \"zero_terms_query\" : \"NONE\",\n            \"boost\" : 1.0\n          }\n        }\n      ],\n      \"filter\" : [\n        {\n          \"term\" : {\n            \"cityEnName\" : {\n              \"value\" : \"bj\",\n              \"boost\" : 1.0\n            }\n          }\n        }\n      ],\n      \"disable_coord\" : false,\n      \"adjust_pure_negative\" : true,\n      \"boost\" : 1.0\n    }\n  },\n  \"sort\" : [\n    {\n      \"lastUpdateTime\" : {\n        \"order\" : \"desc\"\n      }\n    }\n  ]\n}\n\t\n```\n\n另外呢，很多我们筛选的条件没必要都是must，可以修改成should。只要满足一些搜索的条件就可以：\n\n所以我们把两个matchQuery修改成should的形式:\n\n```java\n        //修改Title查询字段的权重\n        boolQuery.should(\n                QueryBuilders.matchQuery(HouseIndexKey.TITLE,rentSearch.getKeywords())\n                        .boost(2.0f)\n        );\n        boolQuery.should(\n                QueryBuilders.multiMatchQuery(\n                        rentSearch.getKeywords(),\n                        HouseIndexKey.TRAFFIC,\n                        HouseIndexKey.DISTRICT,\n                        HouseIndexKey.ROUND_SERVICE,\n                        HouseIndexKey.SUBWAY_LINE_NAME,\n                        HouseIndexKey.SUBWAY_STATION_NAME\n                        )\n        );\n```\n\n都修改为should后反而什么乱七八糟的都查出来了。建议修改为must\n\n\n\n另外一个优化的点是：\n\n我们的es是需要检索出重要的数据字段即可，然后再去mysql中查询，而现在是在es中把所有的数据都查询出来了，如果有海量的大数据，我们该怎么处理？\n\n```java\nSearchRequestBuilder requestBuilder = this.esclient.prepareSearch(INDEX_NAME)\n        .setTypes(INDEX_TYPE)\n        .setQuery(boolQuery)\n        .addSort(\n                HouseSort.getSortKey(rentSearch.getOrderBy()),\n                SortOrder.fromString(rentSearch.getOrderDirection())\n        )\n        .setFrom(rentSearch.getStart())\n        .setSize(rentSearch.getSize())\n        .setFetchSource(HouseIndexKey.HOUSE_ID, null);\n```\n\n我们在prepareSearch这里设置FetchSource，把House_ID扔进去，只返回house_Id的字段：\n\n```json\n{houseId=21}\n{houseId=29}\n{houseId=19}\n{houseId=24}\n```\n\n这样的话查询的量级也比较小一些。\n\n**setFetchSource**\n\n\n\n## 四十三、基于百度地图的找房功能\n\n### 业务与需求分析：\n\n> 在网站拥有了搜索引擎后，用户就可以非常方便的搜到自己想要的房源信息了，但是有很多时候，用户并不能想清楚自己想要什么，那么这就就迫切的需要一个具体**提示性质**的功能，来方便用户查询信息，所以我们的地图找房功能就很有必要了。我们的目标就是能够在指定区域显示用户想要的房源，方便用户根据地理位置寻找房源信息，提升网站的竞争力。\n\n### 实现目标：\n\n- ​\t构建房源地理坐标\n- ​    聚合地图范围内房源\n- ​    根据地图视野查询房源\n- ​    地图事件绑定数据源\n- ​    地图展示房源数据麻点图\n\n\n\n### 基于ES的功能开发：\n\n- 地图房源信息聚合功能\n- 根据视野变化动态更新数据源功能\n- 地图事件更新数据源功能\n\n### 基于地址获取经纬度的开发：\n\n- 获取百度地图经纬度\n- 使用ES存储地理数据信息\n\n百度地图浏览器端：NwrFdXX7d9ulyeUvuvrRL1pRwHEHl7qU\n\n百度地图服务端：vVo0GpUwrXGIyq7Xpv6U7Bui5dNGDjaL\n\n\n\n对AK值做一个替换：\n\n![image-20220925105711798](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h6io025ecpj20wm0qqn0k.jpg)\n\n然后在HouseController中去定义一个map的新接口：\n\n```java\n@GetMapping(\"rent/house/map\")\npublic String rentMapPage(@RequestParam(value = \"cityEnName\") String cityEnName,\n                          Model model,\n                          HttpSession httpSession,\n                          RedirectAttributes redirectAttributes){\n    ServiceResult<SupportAddressDTO> city = addressService.queryCity(cityEnName);\n    if(!city.isSuccess()){\n        redirectAttributes.addAttribute(\"msg\",\"must_chose_city\");\n        return \"redirect:/index\";\n    }else {\n        httpSession.setAttribute(\"cityName\", cityEnName);\n        model.addAttribute(\"city\",city.getResult());\n    }\n\n    ServiceMultiResult<SupportAddressDTO> regions = addressService.findAllRegionsByCityName(cityEnName);\n    model.addAttribute(\"total\", 0);\n    model.addAttribute(\"regions\", regions.getResult());\n    return \"rent-map\";\n}\n```\n\n\n\n\n\n另外有时候springmvc的动态cache不跟着配置走，怎么办呢？\n\n在WebMvcConfig中\n\n```java\n@Configuration\npublic class WebMvcConfig extends WebMvcConfigurerAdapter implements ApplicationContextAware {\n\n    @Value(\"${spring.thymeleaf.cache}\")\n    private boolean thymeleafCacheEnable = true;\n```\n\n然后在springResourceTemplateResolver中设置\n\n```java\n/**\n * 模板资源解析器\n * @return\n */\n@Bean\n@ConfigurationProperties(prefix = \"spring.thymeleaf\")\npublic SpringResourceTemplateResolver templateResolver() {\n    SpringResourceTemplateResolver templateResolver = new SpringResourceTemplateResolver();\n    templateResolver.setApplicationContext(this.applicationContext);\n    /*解析的页面乱码 这里调整*/\n    templateResolver.setCharacterEncoding(\"UTF-8\");\n\n    templateResolver.setCacheable(thymeleafCacheEnable);\n    return templateResolver;\n}\n```\n\n这样在前端开发的时候，就不需要重启，也不需要重新加载了。\n\n保持cache是true。\n\n\n\n\n\n\n\n然后再去实现城市的聚合操作：\n\n```java\n/**\n * 聚合城市的出租数量\n */\n\nServiceMultiResult<HouseBucketDTO> mapAggregate(String cityEnName);\n```\n\n```java\n@Override\npublic ServiceMultiResult<HouseBucketDTO> mapAggregate(String cityEnName) {\n    BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();\n    boolQuery.filter(QueryBuilders.termQuery(HouseIndexKey.CITY_EN_NAME,cityEnName));\n\n    AggregationBuilder aggBuilder = AggregationBuilders.terms(HouseIndexKey.AGG_REGION)\n            .field(HouseIndexKey.REGION_EN_NAME);\n\n    SearchRequestBuilder requestBuilder  = this.esclient.prepareSearch(INDEX_NAME)\n            .setTypes(INDEX_TYPE)\n            .setQuery(boolQuery)\n            .addAggregation(aggBuilder);\n\n    logger.debug(requestBuilder.toString());\n\n    SearchResponse searchResponse  = requestBuilder.get();\n    List<HouseBucketDTO> buckets  = new ArrayList<>();\n    if(searchResponse.status() != RestStatus.OK){\n        logger.warn(\"Aggregate status is not ok for \"+ requestBuilder);\n        return new ServiceMultiResult<>(0,buckets);\n    }\n\n    Terms terms = searchResponse.getAggregations().get(HouseIndexKey.AGG_REGION);\n    for(Terms.Bucket bucket :  terms.getBuckets()){\n        buckets.add(new HouseBucketDTO(bucket.getKeyAsString(),bucket.getDocCount()));\n    }\n    return new ServiceMultiResult<>(searchResponse.getHits().getTotalHits(),buckets);\n}\n```\n\n这样呢service就完善了，然后再去controller中去配置。\n\n```java\n@GetMapping(\"rent/house/map\")\npublic String rentMapPage(@RequestParam(value = \"cityEnName\") String cityEnName,\n                          Model model,\n                          HttpSession httpSession,\n                          RedirectAttributes redirectAttributes){\n    ServiceResult<SupportAddressDTO> city = addressService.queryCity(cityEnName);\n    if(!city.isSuccess()){\n        redirectAttributes.addAttribute(\"msg\",\"must_chose_city\");\n        return \"redirect:/index\";\n    }else {\n        httpSession.setAttribute(\"cityName\", cityEnName);\n        model.addAttribute(\"city\",city.getResult());\n    }\n\n    ServiceMultiResult<SupportAddressDTO> regions = addressService.findAllRegionsByCityName(cityEnName);\n    ServiceMultiResult<HouseBucketDTO> mapAggResult = searchService.mapAggregate(cityEnName);\n\n    model.addAttribute(\"aggData\",mapAggResult.getResult());\n    model.addAttribute(\"total\", mapAggResult.getTotal());\n    model.addAttribute(\"regions\", regions.getResult());\n    return \"rent-map\";\n}\n```\n\n### 基于ES的地图点聚合\n\n> 剩下的基于时间紧张，并没有详细去学习研究，记录在此。\n\n\n\n## 七、搜索引擎实现\n\n### 1、业务与功能分析\n\n- ​\t在我们网站实现了基础信息浏览功能以后，用户就可以看到网站上的所有房源，但是在网站房源信息量爆炸的时候，用户是很难找到自己想要的信息的，这时候，网站就必须要有**站内搜索引擎**功能，帮助用户根据用户自身的需求快速的找到想要的房源信息。\n- 实现目标\n  - 后台登录\n  - 权限控制\n  - 注销功能\n\n## \n\n\n### 2、实现目标\n\n- 构建ES房源索引\n- 基于ES构建搜索引擎\n- 解决中文分词问题\n- search-as-you-type\n- 使搜索引擎结果集最优\n\n### 3、搜索引擎实现\n\n- 索引结构设计\n\n  > 为什么不直接使用es来存数据呢？因为es是弱事务的，并不能使用事务来处理，很容易产生很多脏数据。\n\n\n\n\n\n","tags":["大数据","elasticSearch","前后端","JAVA"],"categories":["AI"]},{"title":"【向量模型】几种多模态向量检索引擎的了解与思考","url":"/2023/07/23/几种多模态向量检索引擎Faiss 、milvus、Proxima、vearch、Jina等/","content":"\n# 几款多模态向量检索引擎：Faiss 、milvus、Proxima、vearch、Jina等\n\n> 本文学习了 [sunny5156](http://blog.sunqiang.me/author/admin/)发表的博文，并对此做了扩展批注，考虑罗列几个向量搜索引擎在工程中的实践与落地。\n\n## 引用文章[7]的开篇，来表示，什么是： 向量化搜索？\n\n​\t人工智能算法可以对物理世界的人/物/场景所产生各种非结构化数据（如语音、图片、视频，语言文字、行为等）进行抽象，变成多维的向量。\n\n​\t这些向量如同数学空间中的坐标，标识着各个实体和实体关系。我们一般将非结构化数据变成向量的过程称为 Embedding，而非结构化检索则是对这些生成的向量进行检索，从而找到相应实体的过程。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86yjzwaglj30o607rjs8.jpg)\n\n非结构化检索本质是向量检索技术，其主要的应用领域如人脸识别、推荐系统、图片搜索、视频指纹、语音处理、自然语言处理、文件搜索等。\n\n随着 AI 技术的广泛应用，以及数据规模的不断增长，**向量检索也逐渐成了 AI 技术链路中不可或缺的一环**，更是对传统搜索技术的补充，并且具备多模态搜索的能力。\n\n向量检索的应用场景远不止上面提到的这些类型。\n\n如下图所示，它几乎覆盖了大部分的可以应用AI的业务场景。\n\n![几款多模态向量检索引擎：Faiss 、milvus、Proxima、vearch、Jina等 - 第2张  | 技术人生](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h868p1pu38j30o60cd0u7.jpg)\n\n### 1、**facebook – Faiss**\n\n> github: https://github.com/facebookresearch/faiss\n>\n> tutorial: https://github.com/facebookresearch/faiss/wiki/Getting-started\n\n向量化检索开山鼻祖的应用，Faiss库是由 Facebook 开发的适用于稠密向量匹配的开源库，支持 c++ 与 python 调用。\n\nFaiss 支持多种向量检索方式，包括内积、欧氏距离等，同时支持精确检索与模糊搜索，篇幅有限嘛，我就先简单介绍精确检索相关内容。\n\n**Faiss 主要特性：**\n\n· 支持相似度检索和聚类；\n\n· 支持多种索引方式；\n\n· 支持CPU和GPU计算；\n\n· 支持Python和C++调用；\n\n**Faiss 使用场景：**\n\n最常见的人脸比对，指纹比对，基因比对等。\n\n> 在模糊搜索项目中我们使用了faiss+texSmart词向量，融合了es进行短词、短语的检索。关于faiss更多的索引方式并未深入研究，这里记录待后续整理补充。\n\n\n\n### 2、Faiss原理\n\n#### 2.1整体流程图\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86y4jqizwj30y60f3ab0.jpg)\n\n> Faiss核心原理就两个，**倒排索引 IVF**和**乘积量化 PQ**，这两个方法是Faiss实现高速，少内存以及精确检索的主要手段。\n\n#### 2.2 倒排索引 IVF（Inverted File System）\n\n​\tIVF其实很好理解，假如我们想要在全国范围内找到一个给定年龄，性别，身高体重等信息的人，那么其中的一个办法是，拿这些信息和全国的人都一一对比，然后找到和这个条件最相近的一类人。但是如果我们先把全国的人按照省份进行划分，先看这个人是哪个省份的，接着从这个省份里去全量搜，那么计算了一下子降了一个数量级，如果是按城市划分，则计算量可以降低好几个量级，这就是IVF的基本原理。\n​\t暴力搜索的方式是在全空间进行搜索，为了加快查找速度，几乎所有的ANN方法都是通过对全空间进行分割（聚类），将其分割成很多小的子空间，在搜索时，通过某种方式快速锁定在某一(几)个子空间，然后在这几个子空间中进行遍历。\n\n\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86y5s5ij1j30vj0u0jt9.jpg)\n\n​\t**确定子空间： 通过计算query向量和所有子空间中心(如子空间内所有向量的均值)的距离，选出距离最近的K个子空间，表示和该query最相近的向量，最有可能在这几个子空间里。**\n\n**缺点：**\n\n- 可能会损失精度，找到的是局部解，不是全局最优\n\n  ​\t很可能更相近的向量不在遍历的这几个空间内，因此找到的相似向量不是全局最优的，在faiss中有两个参数，分别是nlist 和 nprobe ，其中前者决定了对全量用户聚类的个数，一般来说聚类个数越大，训练过程越慢，检索速度越快（每个子空间需要遍历的向量变少），后者nprobe决定了每次检索几个子空间，一般来说，检索的子空间越多，检索越精确，但是检索速度越慢。两者需要做一定的权衡。特别地，当两者相等时，等价于暴力检索。\n\n- 检索速度可能不太稳定\n\n  ​\t一般来说，在进行聚类后，检索的速度应该是暴力检索的 1/nlist ，但是由于聚类算法不可能保证每个类包含的向量数量都是一样的，实际直觉上各个类的大小也不应该一样（如每个省份的人有多有少），因此在需要对较大的子空间进行遍历时，需要消耗较多的时间，反之则速度更快。一般有个常量C来表示，整体的查询效率为 C/nlist 。\n\n- 内存消耗较大\n\n  ​\t无论是训练还是最后的检索，为了提升速度，都需要把全量数据加载到内存中，这种方法没有对向量进行压缩，内存消耗较大。\n\n#### 2.3 乘积量化\n\n> 乘积量化的核心思想还是聚类。其主要分为两个步骤，Cluster 和 Assign，也即聚类和量化。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86y8g88a1j31010myach.jpg)\n\n​\t实际上就是先切块然后聚类，再每个切的小块上通过量化编码的形式进行拼接组合，节省了大量内存，压缩比率2000+\t\n\n乘积量化有个重要的参数m_split ，这个参数控制着向量被切分的段数，如图所示，假设每个向量的维度为128，每个向量被切分为4段，这样就得到了4个小的向量，对每段小向量分别进行聚类，聚类个数为256个，这就完成了Cluster。然后做Assign操作，先每个聚类进行编码，然后分别对每个向量来说，先切分成四段小的向量，对每一段小向量，分别计算其对应的最近的簇心，然后使用这个簇心的ID当做该向量的第一个量化编码，依次类推，每个向量都可以由4个ID进行编码。\n每个ID可以由一个字节保存，每个向量只需要用4个字节就可以编码，这样就完成的向量的压缩，节省了大量的内存，压缩比率2000+。\n这一步其实就是Faiss训练的部分，目的是为了获取索引Index。在完成向量压缩，获取索引之后，就要考虑如何进行向量查询，下图表示了某个查询向量Query进行查询时的操作流程：\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86yay17s1j30z70lnmz6.jpg)\n\n查询也是和训练时一样，先对查询的向量进行切分，得到四小段向量，计算每个小向量和对应的256个类中心距离，存在一个距离矩阵或者数组中，接着就可以通过查表，计算query向量和每个向量之间的距离，类加每个小向量之间的距离之和。\n\n### 3、优化效率计算\n\n由上可以看出，在原来的暴力算法下，进行一次查询需要的计算量是 N128次计算，但是使用PQ后，只需要 425632次计算，加上4N次查表计算，因此计算量之比为：\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86ydue4egj307302eq2q.jpg)\n\n当N特别大时，425632 可以忽略，**计算量减少32倍。**（注：分母的查表操作耗时远小于分子的浮点乘法计算，因此计算效率会远大于32）\n但是，考虑到3.2小节提到的IVF方法，先进行聚类，再进行PQ，那么计算量再次减少，变成 :\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86ye07f89j308601wq2r.jpg)\n\n假设nlist设置为2000，常数c为5，则优化效率至少为1200，提升了3个数量级,计算量再次减少，最终可以实现快速，高效的相似向量查询。\n\n### 4、总结\n\n本文只着重讲解Faiss比较核心的原理，还有基于局部哈希敏感，基于图检索的分层小世界导航等方法，有兴趣的同学可以看它的github以及相关的博客等。另外具体使用方法也可自行查找相关资料。\n\n## Reference\n\n[1] https://github.com/facebookresearch/faiss\n[2] https://zhuanlan.zhihu.com/p/164888905\n\n\n\n\n\n构建索引的方式有暴力检索的方法FlatL2，L2代表构建的index采用相似度度量方法是L2范数，欧氏距离。\n\n检索TopK相似query\n\n**Flat ：暴力检索**\n\n- 优点：该方法是Faiss所有index中最准确的，召回率最高的方法，没有之一；\n- 缺点：速度慢，占内存大。\n- 使用情况：向量候选集很少，在50万以内，并且内存不紧张。\n\n **IVFx Flat ：倒排暴力检索**\n\n- 优点：IVF主要利用倒排的思想，在文档检索场景下的倒排技术是指，一个kw后面挂上很多个包含该词的doc，由于kw数量远远小于doc，因此会大大减少了检索的时间。在向量中如何使用倒排呢？可以拿出每个聚类中心下的向量ID，每个中心ID后面挂上一堆非中心向量，每次查询向量的时候找到最近的几个中心ID，分别搜索这几个中心下的非中心向量。通过减小搜索范围，提升搜索效率。\n- 缺点：速度也还不是很快。\n- 使用情况：同Flat，\n- 参数：IVFx中的x是k-means聚类中心的个数\n\n**PQx ：乘积量化**\n\n- 优点：利用乘积量化的方法，改进了普通检索，将一个向量的维度切成x段，每段分别进行检索，每段向量的检索结果取交集后得出最后的TopK。因此速度很快，而且占用内存较小，召回率也相对较高。\n- 缺点：召回率相较于暴力检索，下降较多。\n- 使用情况：内存及其稀缺，并且需要较快的检索速度，不那么在意召回率\n- 参数：PQx中的x为将向量切分的段数，因此，**x需要能被向量维度整除**，且x越大，切分越细致，时间复杂度越高\n\n**IVFxPQy 倒排乘积量化**\n\n- 优点：工业界大量使用此方法，各项指标都均可以接受，利用乘积量化的方法，改进了IVF的k-means，将一个向量的维度切成x段，每段分别进行k-means再检索。\n- 缺点：集百家之长，自然也集百家之短\n- 使用情况：一般来说，各方面没啥特殊的极端要求的话，**最推荐使用该方法！**\n- 参数：IVFx，PQy，其中的x和y同上\n\n**LSH 局部敏感哈希**\n\n- 原理：哈希对大家再熟悉不过，向量也可以采用哈希来加速查找，我们这里说的哈希指的是局部敏感哈希（Locality Sensitive Hashing，LSH），不同于传统哈希尽量不产生碰撞，局部敏感哈希依赖碰撞来查找近邻。高维空间的两点若距离很近，那么设计一种哈希函数对这两点进行哈希值计算，使得他们哈希值有很大的概率是一样的，若两点之间的距离较远，他们哈希值相同的概率会很小。不同距离度量的哈希函数不同，不是所有距离度量（如内积）都能找到对应局部敏感哈希。摘自[这篇文章](https://zhuanlan.zhihu.com/p/264367144)。\n- 优点：训练非常快，支持分批导入，index占内存很小，检索也比较快\n- 缺点：召回率非常拉垮。在候选语料比较多的时候（亿级别），检索也不是特别快，大概是秒级别的。\n- 使用情况：候选向量库非常大，离线检索，内存资源比较稀缺的情况\n\n**HNSWx** \n\n- 优点：该方法为基于图检索的改进方法，检索速度极快，10亿级别秒出检索结果，而且召回率几乎可以媲美Flat，能达到惊人的97%。检索的时间复杂度为loglogn，几乎可以无视候选向量的量级了。并且支持分批导入，极其适合线上任务，毫秒级别体验。\n- 缺点：构建索引极慢，占用内存极大（是Faiss中最大的，大于原向量占用的内存大小）\n- 参数：HNSWx中的x为构建图时每个点最多连接多少个节点，x越大，构图越复杂，查询越精确，当然构建index时间也就越慢，x取4~64中的任何一个整数。\n- 使用情况：不在乎内存，并且有充裕的时间来构建index\n\n\n\n对不同index做过一些对比实验结果：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86ywvxjntj30k005nt9d.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t召回率、搜索时间对比\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86yxn49q8j30k003rweu.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t召回率、检索时间、内存消耗、构建时间对比\t\n\n\n\n当然可以先使用pca降维，在用IVF等方法构建索引。\n\n----------------------\n\n\n\n### 2、国产 – Milvus\n\n文章1、8提及，\nMilvus 是一款开源的特征向量相似度搜索引擎Milvus 使用方便、实用可靠、易于扩展、稳定高效和搜索迅速。\n\nMilvus能够很好地应对海量向量数据，它集成了目前在向量相似性计算领域比较知名的几个开源库（Faiss， SPTAG等），通过对数据和硬件算力的合理调度，以获得最优的搜索性能。\n\n> https://milvus.io/\n\nMilvus 提供完整的向量数据更新，索引与查询框架。Milvus 利用 GPU（Nvidia）进行索引加速与查询加速，能大幅提高单机性能。除了提供针对向量的近实时搜索能力外，Milvus 可以对标量数据进行过滤。\n\n随着数据和查询规模的增加，Milvus 还提供了集群分片的解决方案，支持读写分离、水平扩展、动态扩容等功能，实现了对于超大数据规模的支持。\n\n目前，Milvus 是一个单节点主从式架构（Client-server model）的服务器，最高可以支持 TB 级特征数据的存储和搜索服务。对于有更大数据规模或者高并发需求的用户，可以使用目前尚在实验阶段的集群分片中间件 Mishards 进行部署。\n\n![几款多模态向量检索引擎：Faiss 、milvus、Proxima、vearch、Jina等 - 第4张  | 技术人生](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h868xtdqomj30k40dbt9k.jpg)\n\nMilvus索引类型：\n\n· IVF：集成 Faiss IVF、Milvus 团队自研 IVF\n\n· Graph：Milvus 团队实现的 NSG 索引、集成 Faiss hnsw\n\n· Tree & Graph：集成微软 SPTAG\n\n· IVF & Graph：Milvus 团队实现\n\n可支撑的应用场景：\n\n#### **2.1 图像视频检索**\n\n\n\n深度学习模型最开始就是用来对图像、视频等进行处理，通过训练可以精准的提取图片、视频中的特征从而对图片、视频进行分类，打标签，以图搜图，以图搜视频等等。Milvus凭借其出色的性能和数据管理能力，可以支持各种深度学习模型，实现对海量图片和视频的高性能分析检索能力。\n\n· 图片搜索\n\n· 图片去重\n\n· 视频去重\n\n· 以商品搜商品\n\n#### **2.2 智能问答机器人**\n\n\n\n传统的问答机器人大都是基于规则的知识图谱方式实现，这种方式需要对大量的语料进行分类整理。而基于深度学习模型的实现方式可以彻底摆脱对语料的预处理，只需提供问题和答案的对应关系，通过自然语言处理的语义分析模型对问题库提取语义特征向量存入Milvus中，然后对提问的问题也进行语义特征向量提取，通过对向量特征的匹配就可以实现自动回复，轻松实现智能客服等应用。\n\n· 语义提取\n\n· 个性化推荐\n\n· 语料分析和推荐\n\n#### **2.3 赋能传统向量计算**\n\n\n\n在传统的数据处理领域也存在大量向量计算的场景，使用传统的计算方式需要消耗大量的算力而Milvus凭借先进的算法可以在同等算力资源下将向量数据处理能力提高至少两个数量级。\n\n· 分子结构相似性分析\n\n· 分子药理分析\n\n· 药物分子虚拟筛选\n\n#### **2.4 音频数据处理**\n\n\n\n利用深度学习模型对音频数据进行分析和处理能够大大提高语音识别的准确率，而其核心也是对相关音频切片进行向量化处理并且通过向量距离的计算来判断其表达的含义，因此，Milvus在语音、音乐等音频数据处理领域的也有丰富的应用。\n\n· 个性化音乐推荐\n\n· 音乐去重\n\n· 声纹验证\n\n· 语音识别\n\n· 智能语音小助手\n\n· 智能翻译机器人\n\n### 3、**国产 – Jina- 神经网络搜索**\n\n文章[15]提及，\n\n​\tJina AI是一家专注基于深度学习模型搭建搜索引擎技术的开源商业公司，打造下一代的开源神经搜索引擎开发平台，通过深度学习和人工智能搜索能力的结合做到全内容搜索，无论是文本、图片、语音、视频、源代码、元数据亦或是文件都可以称为搜索引擎输入源进行全域全方位搜索。\n\n​\t由AI界大名鼎鼎的肖涵老师带领的开源团队创始和开发，秉承了肖涵老师团队优秀的开源文化，其团队名下的Bert as server 和Fashion-Mnist 在GitHub的star数都高达8000，故本次的Jina质量可见一斑。\nJina以通用性为目标，几乎可以搜索任何内容形式（例如文本，图像，视频，音频）；它的目标是在AI生产中，利用现代软件基础架构并以最佳工程实践进行构建。旨在易于使用，针对多个平台，架构和用例进行优化。\n\n![几款多模态向量检索引擎：Faiss 、milvus、Proxima、vearch、Jina等 - 第5张  | 技术人生](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8693172n9j30rv0ant9o.jpg)\n\n​\tJina也有着自身的Yaml语法，可通过API和仪表盘迅速的搭建出一个属于自己的云端系统。同时他最大的亮点就是其可以在多个平台和架构上实现任意类型的大规模索引和查询。\n\n**Jina Hub**\n\n· 90+ 的 Pod 镜像可供使用\n\n· 支持最先进的AI模型\n\n· 支持多种向量数据库\n\n· 支持多种Evaluation 的方式\n\n### 4、 **阿里达摩院 – Proxima & 蚂蚁金服- ZSearch**\n\n#### 4.1、** 阿里达摩院 -Proxima**\n\n文章[7]提及，\nProxima 是阿里巴巴达摩院自研的向量检索内核。目前，其核心能力广泛应用于阿里巴巴和蚂蚁集团内众多业务，如淘宝搜索和推荐、蚂蚁人脸支付、优酷视频搜索、阿里妈妈广告检索等。\n同时，Proxima 还深度集成在各式各类的大数据和数据库产品中，如阿里云 Hologres、搜索引擎 Elastic Search 和 ZSearch、离线引擎 MaxCompute (ODPS) 等，为其提供向量检索的能力。\nProxima 是通用化的向量检索工程引擎，实现了对大数据的高性能相似性搜索，支持 ARM64、x86、GPU 等多种硬件平台，支持嵌入式设备和高性能服务器，从边缘计算到云计算全面覆盖，支持单片索引十亿级别下高准确率、高性能的索引构建和检索。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869hqqg1ej30o60b9ab6.jpg)\n\n向量检索的算法繁多且缺乏通用性，应对不同数据维度和分布有不同算法，但总体可归为三类思想：\n\n空间划分法：空间划分法以 KD-Tree、聚类检索为代表，检索时快速定位到这些小集合，从而减少需要扫描的数据点的量，提高检索效率。\n\n空间编码和转换法：空间编码和转换法，如 p-Stable LSH、PQ 等方法，将数据集重新编码或变换，映射到更小的数据空间，从而减少扫描的数据点的计算量。\n\n· 邻居图法：邻居图法，如 HNSW、SPTAG、ONNG 等，通过预先建立关系图的方法，去加快检索时的收敛速度，减少需要扫描的数据点的量，以提高检索效率。\n\n向量检索发展多年，并逐渐成为非结构化检索的主流方法，但仍存在了不少的技术挑战和问题。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869ibuvrbj30o6087js2.jpg)\n\n- **业务场景：标签+向量的联合检索**\n\n在大多数业务场景下，需要同时满足标签检索条件和相似性检索的要求，如查询某些属性条件组合下相似性的图片等，我们称这种检索为“带条件的向量检索”。\n\n目前，业内采用多路归并的方式，即分别检索标签和向量再进行结果合并，虽可以解决部分问题，但多数情况下结果不甚理想。主要原因在于，向量检索无范围性，其目标是尽可能保证 TOPK 的准确性，TOPK 很大时，准确性容易下降，造成归并结果的不准确甚至为空的情况。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869ikryaaj30o604adg0.jpg)\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869itsys9j30o608wgma.jpg)\n\n- **业务场景： 语音/图像/视频检索**\n\n以图片搜索为例，我们先以离线的方式对所有历史图片进行机器学习分析，将每一幅图片（或者图片里分割出来的人物）抽象成高维向量特征，然后将所有特征构建成高效的向量索引，当一个新查询（图片）来的时候，我们用同样的机器学习方法对其进行分析并产出一个表征向量，然后用这个向量在之前构建的向量索引中查找出最相似的结果，这样就完成了一次以图片内容为基础的图像检索。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869kj3i92j30o60da75s.jpg)\n\n- **业务场景： 搜索/推荐/广告**\n\n在电商领域的搜索/推荐/广告业务场景中，常见的需求是找到相似的同款商品和推荐给用户感兴趣的商品，这种需求绝大多数都是采用商品协同和用户协同的策略来完成的。\n\n新一代的搜索推荐系统吸纳了深度学习的 Embedding 的能力， 通过诸如 Item-Item (i2i)、User-Item (u2i)、User-User-Item (u2u2i)、User2Item2Item (u2i2i) 等向量召回的方式实现快速检索。\n\n算法工程师通过对商品的相似和相关关系，以及被浏览和被购买的用户行为的抽象，将它们表征成高维向量特征并存储在向量引擎中。这样，当我们需要找一个商品的相似商品（i2i）时，就可以高效快捷地从向量引擎中检索出来。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869kuihi5j30o60chjsn.jpg)\n\n#### **4.2 蚂蚁金服- ZSearch**\n\n文章[5]提及，\n\nElasticSearch（简称 ES）是一个非常受欢迎的分布式全文检索系统，常用于数据分析、搜索、多维过滤等场景。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869qyozmpj30u40gcacb.jpg)\n\nElasticSearch 广泛应用于蚂蚁金服内部的日志分析、多维分析、搜索等场景。当我们的 ElasticSearch 集群越来越多，用户场景越来越丰富，我们会面临越来越多的痛点：\n\n· 如何管理集群；\n\n· 如何方便用户接入和管理用户；\n\n· 如何支持用户不同的个性化需求；\n\n为了解决这些痛点，我们开发了 ZSearch 通用搜索平台：\n\n· 基于 K8s 底座，快速创建 ZSearch 组件，快捷运维，故障机自动替换；\n\n· 跨机房复制，重要业务方高保；\n\n· 插件平台，用户自定义插件热加载；\n\n· SmartSearch 简化用户搜索，开箱即用\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869ravq1lj30u40dswft.jpg)\n\n#### **4.3 ZSearch + Proxima => ProximaEngine**\n\n文章[5]提及，\nproxima 是阿里内部达摩院开发的一个通用向量检索引擎框架，类似与 facebook 开源的 faiss；支持多种向量检索算法。\n\n**写入流程**\n\n扩展 ElasticSearch 本身的 InternalEngine，在写完 Lucene 以后，先写 Proxima 框架，Proxima 框架的数据通过 mmap 方式会直接刷到磁盘，一次请求的最后，Translog 刷入磁盘。就是一次完整的写入请求了。至于内存中的 segment，ElasticSearch 会异步到达某个条件是刷入磁盘。\n\n**Query 流程**\n\n查询的时候，通过 VectorQueryPlugin，先从 Proxima 向量检索引擎中查找 topN 的向量，获得 seqNo 和相似度，再通过构造 newSetQuery 的 FunctionScoreQuery，去 join 其他查询语句。这里的数字型 newSetQuery 底层是通过 BKDTree 去一次遍历所得，性能还是很高效的。\n\n![几款多模态向量检索引擎：Faiss 、milvus、Proxima、vearch、Jina等 - 第15张  | 技术人生](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86a782uizj30u40g70ua.jpg)\n\n### **5 京东 – vearch**\n\ngithub：https://github.com/vearch/vearch\n文档：https://vearch.readthedocs.io/zh_CN/latest/\n\n文章[16]提及，vearch 是一个分布式向量搜索系统，可用来存储、计算海量的特征向量，为 AI 领域的向量检索提供基础系统支撑与保障。该系统能够广泛地应用于图像， 音视频和自然语言处理等各个机器学习领域。\n\nvearch 基于 Facebook AI 研究院开源的 Faiss 实现，但 Faiss 本身只是一个能够单机运行的支持各种向量检索模型的机器学习算法基础库，不支持分布式、实时索引和检索，同时也不支持标量字段的存储和索引等等。\n\n因此结合在实际应用中的需求，我们在 faiss 的基础上研发了 vearch，提供了类似 ElasticSearch 的灵活易用的 RESTFul API，可以方便地对表结构及数据进行管理查询。\n\n为了提升检索召回及排序的质量，在实际应用场景中，通过量化，Hash 及图等检索模型召回后得到的 TopN 候选集，需要进一步比对计算其原始向量距离。因此在实际生产环境及成熟产品应用中，庞大原始向量的存储是不能逃避开的一个问题。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869ok0q7tj30u40q641o.jpg)\n\n如下图清晰地展示了 vearch 的总体用法及其内部结构。vearch 的使用主要分为三个步骤。 首先，创建数据库和空间，然后导入数据，最后可以搜索自己的数据集。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869plc0tlj30sk0bwgm9.jpg)\n\nvearch 主要的应用场景有：\n\n· 图像/视频/音频检索和去重；\n\n· 安防领域视频智能监控\n\n· 文本相似度计算；\n\n· 推荐，搜索召回及排序；\n\n\n\n案例 1：人脸识别底层特征向量存储、计算引擎，如图 6 所示，目前已经应用到京东之家业务中，为业务过滤掉了 40%左右的不良数据。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869md5l7oj30u40k1760.jpg)\n\n案例 2：vearch 可提供实时在线相似性图片搜索服务，比如在电商平台可以用来搜同款。详细流程如图 7 所示：\n\n![](https://image.baidu.com/search/down?url=http://blog.cxiangnet.cn/wp-content/uploads/2022/01/wpsDA5F.tmp_-1-1.png)\n\n案例 3：海量重复图片去重，在目前已经应用到了京东重复铺货检测，如图 8 所示：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869myjj3fj30u40cs75i.jpg)\n\n案例 4：可应用于搜索，推荐系统。比如视频网站推荐系统将各种特征 embedding 成向量后，利用向量相似性检索做召回，召回模型细节如图 9 所示：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86a1iev2vj30io0e6gmo.jpg)\n\n### **6 应用案例一：贝壳找房 – 向量检索框架的选择**\n\n#### **6.1 技术选型**\n\n在技术选型阶段，我们调研了业界已经比较成熟的工具，如facebook的faiss、微软的SPTAG，以及国内发起的开源项目Vearch，Milvus。\n\n具体对比见表1,2。Vearch和Milvus属于同类型产品，对比faiss和SPTAG的优势在于，后两者为开发库，不能开箱即用，在生产环境中使用涉及更多的开发、维护成本。Milvus和Vearch是两款基于现有的开发库，开箱即用的应用，在实现基本的相似计算功能的基础上，围绕服务整体易用性、部署、稳定性等方面做了更多工作。\n\n另外，Milvus对比Vearch，在社区活跃度、支持度上具有更明显的优势。基于以上的调研，综合考虑各方面的成本，我们选择Milvus作为底层引擎。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869t51yswj30u40ma40h.jpg)\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869t81fbej30u40hn75v.jpg)\n\n#### **6.2 服务框架**\n\n\n\nMilvus基于Faiss、Annoy等比较成熟的开源库，并针对性做了定制，支持结构化查询、多模查询等业界比较急需的功能；Milvus支持cpu、gpu、arm等多种类型的处理器；同时使用mysql存储元数据，并且在共享存储的支持下，Milvus可以支持分布式部署。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869tolt8ij30u40guq5r.jpg)\n\n贝壳找房整体架构分三层，网关层、应用层和引擎层。\n\n![](http://blog.cxiangnet.cn/wp-content/uploads/2022/01/wpsDA84.tmp_-1-1.png)\n\n### **7 应用案例二：58同城向量检索平台架构实践**\n\n\n\n为了满足业务上对向量检索的需求，降低学习成本，提高开发效率，我们开发并上线了向量检索平台。我们支持了Faiss算法库，实现了几种常用索引的全量索引构建、实时增量索引、实时在线检索，旨在帮助用户更快更好的使用海量高维向量相似性检索功能。\n\n![](http://blog.cxiangnet.cn/wp-content/uploads/2022/01/wpsDA85.tmp_-1-1.png)\n\n平台基于Facebook的Faiss进行向量索引的构建，目前支持三种比较具有代表性的索引全量构建，分别是IndexFlatL2（基于欧氏距离的暴力索引）、IndexIVFFlat（加聚类的倒排索引，支持欧式距离和向量内积两种距离算法）、IndexIVFPQ（加聚类、加量化的倒排索引），并且支持分布式索引（支持大规模数据的分布式索引构建）。\n\n分布式索引构建Faiss本身只是一个能够单机运行的向量检索基础算法库，不支持分布式。在海量数据的场景下，向量数据集达到百亿级及以上时，单机构建索引会产生如下问题：\n\n1、 单机处理数据量，构建索引耗时较长\n\n· 2、 构建的索引，可能达到百GB以上甚至达到TB级别，会超过单机物理节点的内存资源上限针对以上问题，向量检索平台支持了分布式索引的构建。\n\n### **8 拓展阅读**\n\n#### **8.1 google – ScaNN**\n\n> [google-research/scanngithub.com/google-research/google-research/tree/master/scann](https://github.com/google-research/google-research/tree/master/scann)\n>\n> [google-research/google-researchgithub.com/google-research/google-research/blob/master/scann/docs/algorithms.md](https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md)\n>\n> [google-research/google-researchgithub.com/google-research/google-research/blob/master/scann/docs/example.ipynb](https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb)\n\nICML 2020 论文“Accelerating Large-Scale Inference with Anisotropic Vector Quantization”中，我们通过关注如何压缩数据集向量来实现快速近似距离计算解决这一问题，并提出了一种新的压缩技术，与以前的工作相比，这项技术可以大大提高准确率。我们在近期开源的向量相似性搜索库 (ScaNN) 中应用了这项技术，与其他向量相似性搜索库相比，我们的性能高出两倍（在 [ANN-Benchmarks](http://ann-benchmarks.com/) 上测得）。\n\n各向异性向量量化使 ScaNN 可以更好地估计前 k 个 MIPS 结果中的内积，从而实现更高的精度。在来自 [ANN-Benchmarks](http://ann-benchmarks.com/) 的 glove-100-angular 基准测试中，ScaNN 的性能优于其他 11 个经过精心调整的向量相似性搜索库，在给定的精度下，每秒处理的查询数量大约是第二快的库的两倍。*\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86a0gwxndj30hw0aldgy.jpg)\n\nRecall@k 是最近邻搜索精度的常用指标，用于衡量算法返回的 k 个近邻中存在的真正的最近 k 个近邻的比例：ScaNN（上面的紫色线）在速度-精度权衡的各个方面始终展现出优异的性能\n\n我们已经将 ScaNN 在 GitHub 上开源。ScaNN 可以通过 Pip 直接安装，并具有用于 TensorFlow 和 Numpy 输入的接口。\n\n原文链接：\n\n[揭开 ScaNN 的神秘面纱：高效的向量相似性搜索blog.csdn.net/jilrvrtrc/article/details/107969603](https://blog.csdn.net/jilrvrtrc/article/details/107969603)\n\n### **9 「繁星」：快手搜索在向量检索方向的探索和实践**\n\n\n\n2021-11-30更新；参考：\n\n[「繁星」：快手搜索在向量检索方向的探索和实践mp.weixin.qq.com/s/1ed9BDZKzjQIgDScyxpbHw](https://mp.weixin.qq.com/s/1ed9BDZKzjQIgDScyxpbHw)\n\n![几款多模态向量检索引擎：Faiss 、milvus、Proxima、vearch、Jina等 - 第28张  | 技术人生](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869vots07j30u40cw0u9.jpg)\n\n**「繁星」向量检索平台**\n\n· **性能挑战：**开源算法不一定代表最优性能！需要不断自研提升开源算法性能吞吐进而降低服务部署成本，吞吐 = 金钱；\n\n· **易用性挑战：**开源算法通常是不易用的。开源算法功能比较单一太底层，没有提供面向业务的功能，重复建设成本高；\n\n· **超大规模挑战：**如何设计合理的在线、离线架构支持超大规模数据集(百亿)、超高并发(百万级qps)等特殊业务场景需求，对工程能力挑战极大；\n\n· **综合检索场景挑战：**单纯向量检索能力已不能满足实际业务中复杂场景检索需求，更高级的结构化检索能力成为工业界新的发展方向，融合”非结构化数据向量检索”+”结构化数据布尔检索”的综合检索实现，才能满足类似于(Brand = Adidas) and (Pirce in [100-200]) and (Location = Beijing) and (Ann dis : top 100) results 等相关性限制条件下的复杂场景向量检索需求；\n\n· **算法及超参挑战：**实际生产中业务场景极度复杂，向量检索算法数十种，但没有’银弹’算法适用所有场景，每种算法又有众多的超参影响精度和性能，如何合理选择算法及其并使用合理参数来达到最优的效果是一项十分复杂的工程，需要不断的积累与探索；\n\n· **接入效率挑战：**需求到线上服务需要多长时间？在业务需求很多的情况下，效率往往会成为使用方更大的关注点；\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869w6n7c7j30pe0gm41k.jpg)\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869x35agpj30u40jj41g.jpg)\n\n向量检索平台具备丰富的“算法特性”、“功能特性”及”架构特性”，我们主要选择几个主要特性进行展开介绍：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h869xji9hvj30su0xi0vo.jpg)\n\n#### **参考文献**\n\n\n\n1 向量搜索 产品调研报告 jina AI 以及milvus\n\n2 Milvus 向量搜索引擎开源了！\n3 还在手撸faiss算法库？不如试试这个开源向量搜索系统，单机十亿好轻松\n4 京东开源的分布式特征储存检索系统vearch理解\n5 蚂蚁金服 ZSearch 在向量检索上的探索\n6 [贝壳商业化算法中台架构实践](https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247516794&idx=1&sn=a3f2243dfba429f3956514c2e00aca1b&scene=21%23wechat_redirect)\n7 [比 Faiss 更胜一筹？达摩院自主研发的向量检索引擎 Proxima 首次公开！](https://mp.weixin.qq.com/s?__biz=MzI0NTE4NjA0OQ==&mid=2658373467&idx=1&sn=256a43f943f963bac9328053a61c849d&scene=21%23wechat_redirect)\n8 Milvus 是一款开源的特征向量相似度搜索引擎\n9 [基于Milvus的向量搜索实践（一）](https://mp.weixin.qq.com/s?__biz=MzU3OTY2MjQ2NQ==&mid=2247485325&idx=1&sn=f62c471d2e7cf9051f17602c12a364c3&scene=21%23wechat_redirect)\n10 [基于Milvus的向量搜索实践（二）](https://mp.weixin.qq.com/s?__biz=MzU3OTY2MjQ2NQ==&mid=2247485384&idx=1&sn=fee83ac6c9d5d2b6ed76a5699f137c50&scene=21%23wechat_redirect)\n11 [基于Milvus的向量搜索实践（三）](https://mp.weixin.qq.com/s?__biz=MzU3OTY2MjQ2NQ==&mid=2247485412&idx=1&sn=9f0f790355e4867f26822d1a7e86fffa&scene=21%23wechat_redirect)\n12 [58同城向量检索平台架构实践](https://mp.weixin.qq.com/s?__biz=MzI1NDc5MzIxMw==&mid=2247491376&idx=1&sn=e2e0cb28dd630fbfc1eb6376e4d8c9de&scene=21%23wechat_redirect)\n13 [万变不离其宗：用统一框架理解向量化召回](https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&mid=2650420026&idx=2&sn=da08cdb815fe561661b32aa182321116&scene=21%23wechat_redirect)\n14 [完全解析 | 使用Faiss进行海量特征的相似度匹配](https://mp.weixin.qq.com/s?__biz=Mzg2ODExMjc4NQ==&mid=2247487631&idx=1&sn=22c8351718a4e483439b44cb190560e4&scene=21%23wechat_redirect)\n15 创立半年融资近千万美元，「Jina AI」打造下一代开源神经搜索引擎开发平台\n16 京东分布式向量检索系统 vearch 如何一招搞定海量特征存储与计算？\n\n\n\n\n","tags":["向量数据库"],"categories":["AI"]},{"title":"UIE-通用信息抽取","url":"/2023/05/17/UIE-通用信息抽取/","content":"\n\n\n# UIE-通用信息抽取（2022）\n\n> **统一的文本到结构生成框架**，即**UIE**\n>\n> 目前，大多数IE方法都是面向**特定任务**的，这导致了针对不同IE任务的专用体系结构、独立模型和专用知识源。阻碍了IE系统的快速体系结构开发、快速跨领域适应等。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h86z8l5j2rj30k00gm75w.jpg)\n\n实际上，所有IE任务都可以建模为文本到结构的转换，不同的文本对应不同的结构。\n\n- 实体是命名的span结构\n- 事件是模式定义的记录结构\n\n可以进一步分解为几个原子转换操作：\n\n1. Spotting，定位给定特定语义类型相关的spans\n2. Associating，通过在预定义的模式中为spans分配语义角色来关联spans。\n\n1. **定位：**指示从句子中定位目标信息块，例如事件中的实体和触发词。\n2. **关联：**表示根据所需的关联（例如，实体对之间的关系或事件及其参数之间的角色）连接不同的信息块。\n\n**实体提取可以被视为发现相应实体类型的提及范围**，而**事件检测可以被重新表述为发现具有事件类型的触发器范围，这两项任务可以共享发现能力。**\n\n\n\n> 论文通过统一的预训练算法对大规模文本到结构生成模型进行预训练。\n\n\n\n我们首先从Web收集多个大规模数据集，包括结构化（例如知识库）、非结构化（例如原始文本）和并行（例如Wikipedia Wikidata links）数据，然后在这些异构数据集上统一预训练UIE模型。最后，我们通过**按需微调**，将预训练好的UIE模型适应特定的下游IE任务。论文发现，经过预训练的UIE模型为不同IE任务之间的知识获取、**共享**和**迁移**提供了坚实的基础，并且由于UIE学习了一般的IE能力，新的IE任务可以得到有效解决。\n\n关系抽取、实体抽取、实体+关系联合抽取、事件抽取、情感分析。\n\n----------------\n\n## 语义检索系统\n\n### 召回阶段\n\n在召回阶段，最常见的方式是通过双塔模型，学习Document(简写为Doc)的向量表示，对Doc端建立索引，用ANN召回。\n\nSimCSE模型是一种简单的对比句向量表征的框架，包含无监督和有监督两种方法。\n\n无监督学习：会采用Dropout技术，对原始文本进行数据增强，构造出正样本，用于对比学习训练；\n\n监督学习：由于本身有正样本（相近样本），故无需使用Dropout技术，直接训练即可。\n\n**核心是对比学习，对比学习是通过拉近相似数据的距离，拉远不相似数据的距离为目标，更好地学习数据的表征。使得其在文本匹配任务中产生更好的效果。**\n\n我们在这种方式的基础上，引入**有监督的**语义索引策略 [In-batch Negatives](https://arxiv.org/abs/2004.04906)，以如下Batch size=4的训练数据为例：\n\n> In-batch Negatives 策略的训练数据为语义相似的 Pair 对，策略核心是在 1 个 Batch 内同时基于 N 个负例进行梯度更新，将Batch 内除自身之外其它所有 Source Text 的相似文本 Target Text 作为负例，例如: 上例中“我手机丢了，我想换个手机” 有 1 个正例(”我想买个新手机，求推荐“)，3 个负例(1.求秋色之空全集漫画，2.手机学日语的软件，3.侠盗飞车罪恶都市怎么改车)。\n\n实质上就是计算两段文本的相似度，能够做的功能有：1、抽取文本的语义向量 2、计算文本pair的语义相似度\n\n\n\n### 排序阶段\n\n对召回训练好的pair-wise模型，将文本对的相似度作为特征之一输入到特定的排序规则中进行重新排序。\n\n双塔模型，使用ERNIE-Gram预训练模型，使用margin_ranking_loss训练模型。 排序损失函数。\n\n或者Domain-adaptive Pretraining领域自适应学习。\n\n\n\n然后放在milvus进行语义向量的快速检索。使用模型的推理，然后在召回库里进行匹配排序，即使用ann knn 进行快速检索了，提升了速度。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["信息抽取","UIE"],"categories":["AI"]},{"title":"【大数据】elasticSearch笔记","url":"/2023/04/07/ElasticSearch笔记/","content":"\n#   ElasticSearch顶尖高手系列——基础课程\n\n> 大数据技术栈：Haddop、spark、storm、hive、hbase、zookeeper、elasticsearch\n\n## 核心知识篇\n\n课程特点：\n\n使用了elasticsearch5.2版本进行讲解，市面上的书籍和视频几乎都停留在2.x版本\n\n深入浅出es核心工作原理、全部进行了手工画图讲解、完全不同于市面上已有视频的ppt讲解。\n\n涵盖elasticsearch所有核心知识点，系统化，体系完整详细，有一定深度，包括完整的java开发示范。\n\n1. 全面知识体系，包括了工作原理、文档管理、索引管理、搜索、聚合分析、分词、数据建模、java api等知识。\n2. 知识足够深入和细节、完全秒杀市面上已有的书籍和视频，比如index segment merge原理，乐观锁并发控制，索引别名与零停机，相关度评分算法与定制，近似聚合算法，doc values和fielddata机制原理，父子关系数据建模、java api执行scoll search等各种复杂操作，等等。\n\n全程每讲必练，大量的案例实战和上机实验，实战岀真知，实战中学习知识，没有任何一讲是干讲ppt的。\n\n包含一个实战项目，运用学到的知识，开发一个小型门户网站的搜索引擎和数据分析系统，运用了es几乎所有的核心知识，不像市面上的demo项目。\n\n课程学完之后，学员可以掌握es所有的核心知识点，理解es核心原理，并且能够熟练动手操作所有学到的知识和功能，并且能够掌握es集群的基本部署，并且基于java开发一个适用于中小型应用系统的搜索引擎以及数据分析系统，达到学完即可上手到中小型醒目中使用的程度。\n\n\n\n## 高手进阶篇\n\n1. 使用elasticsearch5.2版本讲解。\n2. 包含市面上几乎没有的所有leasticsearch高级知识点：包含地理位置搜索与聚合分析，term vector、suggester search，搜索模版定制，query执行剖析，数十种最全面的聚合分析，span query，shard分配定制，es插件开发，等等高级的知识点，这些知识市面上已有的书籍或者视频几乎没有。\n3. 全程每讲必练，大量的案例实战和上机实验。\n4. 包含一个复杂实战项目，运用学到的知识，开发一个复杂的基于地理位置的智能餐厅app的搜索引擎和数据分析系统，运用es从核心篇到高级篇的所有高阶知识点\n5. 课程学完之后，学员可以掌握es从核心到高阶的所有知识点，掌握完整的有深度的es知识体系，同时能够动手操作所有的知识点和功能，最后通过项目实战，能够在中小型公司中，基于java开发一个可以基于地理位置进行搜索的高级搜索引擎，以及使用复杂聚合操作进行分析的高级实时数据分析系统。\n\n\n\n## 大型集群运维优化篇\n\n1. 最全面的elasticsearch运维、管理、调优、故障处理的知识体系：企业级监控体系的搭建，企业级集群部署，集群日常管理策略，集群版本升级方案，集群基准压测方案，集群数据的备份和恢复，系统核心配置参数，性能调优方案，故障处理方案。\n2. 全程每讲必练，大量上机实验，所有的运维、管理、部署、优化，全部上机实验。\n3. 从零开始，逐步搭建出一个大型可扩展、高性能、监控体系完善、管理体系健全的分布式集群。\n4. 学完课程之后，除了可以开发复杂的es搜索/分析系统之外，还可以掌握任何一个公司里，从零开始搭建一个分布式的大型es集群，并制定完善的监控，运维，管理，优化等方案。\n\n## 大型项目架构篇\n\n1. 涵盖elasticsearch目前最核心的两盒应用领域，垂直搜索引擎，实时数据分析。\n2. 开发出3个企业级的大型复杂项目，是完全真实的大型企业项目，电商搜索引擎，电商实时数据分析平台，\n   1. 包括了检索、数据更新、排序、分词、query分析等各个核心模块，同时架构上实现了复杂的缓存机制，热启动机制，防雪崩机制，自动降级高可用机制，等等。\n   2. 大型电商实时数据分析平台，完整、复杂而且大型的电商数据分析，包括了完善的数据分析指标体系（运营指标、流量指标、销售转化指标、客户价值指标、商品指标、营销指标、风险控制指标、市场竞争指标），一站式构建出复杂的，企业级的，电商领域数据分析平台。\n   3. 之所以单独拉出一篇做大型项目实战，是因为之前几篇讲的项目，重点是采用大公司的大型复杂项目作为背景，可以掌握基于es技术的大型项目架构达到架构是的水平，比如说大型电商搜索引擎，主要运用es来实现，但是es之外还有大型系统的复杂架构需要讲解，还有大型的电商实时数据分析平台，主要特点是业务繁琐复杂，需要基于es来构建出大型的数据分析平台架构。\n3. 学完大型项目架构篇之后，在之前课程中掌握了es的企业级集群运维管理，复杂搜素引擎和数据分析引用开发的技术基础之上，现在可以运用所学知识，结合电商的领域知识，开发出业务真实而且复杂的，大型的搜索、分析等电商系统以及相关架构，彻底掌握运用es和elk相关技术栈在中大型企业中，开发大型项目架构的经验和能力。\n\n\n\n## ELK深入浅出篇\n\n1. 电商系统日志检索平台，采用ELK技术栈，会详细讲解logstash和kibana两个技术，包括logstash的插件机制，监控方案，大规模扩展方案，升级方案，性能调优方案，kibana的可视化展现方案，同时讲解如何使用ELK技术栈开发出大规模日志存储和检索平台。\n2. 最后需要真正深入重点讲解logstash和kibana两门技术，并结合es技术，采用elk技术栈，实现大型企业级的日志采集和检索平台。\n3. 学完课程后，应该可以彻底惊精通ELK技术栈，并能够使用ELK技术栈快速搭建日志检索平台，以及数据的可视化平台。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h58yfqcfoij20dc074jrq.jpg)\n\n\n\n# 一、什么是elasticSearch\n\n## 1、什么是搜索？\n\n百度：如果我们想要寻找任何想要感兴趣的相关信息，\n\n百度 = 搜索 ，这是不对的。\n\n\n\n垂直搜索（站内搜索）\n\n互联网的搜索：电商网站，招聘网站，新闻网站，各种app\n\nit系统的搜索：OA软件，办公自动化软件，会议管理，日程管理，项目管理，员工管理，搜索“张三”，“张三儿”；如电商网站，卖家，后台管理系统，搜索“牙膏”的订单，搜索到关于牙膏相关的订单。\n\n\n\n搜索，就是在任何场景下，寻找你想要的信息，这时候，就会输入你要搜索的关键字，然后期望找到这个关键字相关的信息。\n\n## 2、如果用数据库做搜索会怎么样？\n\n做软件开发的话，数据都是存储在数据库里面的，比如电商网站商品信息，招聘网站的职位信息，新闻网站的新闻信息，等等，从技术角度考虑的话电商网站内部的搜索功能的话就可以考虑使用数据库进行搜索。\n\n分表分字段去做，数据库的话就是一条一条的全本扫描。\n\n-  \t比方说每条记录的指定字段的文本，可能会很长，比如说“商品描述”字段的长度，有长达数千个甚至是上万个字符，这个时候每次都要对每条记录的所有文本进行扫描，懒判断说你包含不包含我指定的这个关键词（“牙膏”）\n-  \t并且还不能将搜索词拆分开来，尽可能的去搜索更多的符合你的期望的结果，比如输入“生化危” 就搜索不出来“生化危机”相关的信息。\n\n用数据库来实现搜索的话，是不太靠谱的，而且效果会很差。性能上也会很差。\n\n## 3、什么是全文检索和Lucene？\n\n全文检索：倒排索引\n\nlucene ：就是一个jar包，里面包含了封装好的各种建立倒排索引，以及进行搜索的代码，包括各种算法。我们就用java开发的时候，引入lucene jar，然后基于lucene的api进行曲开发就可以了，用lucene，我们可以去将已有的数据建立索引，lucene会在本地磁盘上面给我们组织索引的数据结构。另外的话，我们也可以用lucene提供的一些功能和api来针对磁盘上的索引数据，进行搜索。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0pojz1n76j21k40u00xr.jpg)\n\n## 4、什么是Elasticsearch？\n\nelasticsearch是把lucene封装起来了，当数据部署在单台机器上的时候，如果数据量很大需要部署在多台机器上如果遇到高可用、数据备份、高性能的建立索引、或者跟多台机器进行通信的时候就会特别麻烦，如果必须使用多台机器去进行数据存储和搜索，如果我们自己去实现这些的话，会很麻烦，于是elasticsearch就是做这个工作的。\n\nes本身实现了一些性能的优化，以及分布式的东西，以及提供了很多lucene不能提供的东西：\n\n1. 自动维护数据的分布到多个节点的索引的建立，还有搜索请求分布到多个节点的执行。\n2. 自动维护数据的冗余副本，保证说，一些机器宕机了，不会丢失任何的数据。\n3. 封装了更多的高级功能，以给我们提供更多高级的支持，让我们快速的开发应用；开发更加复杂的代码；复杂的搜索功能；聚合分析的功能，基于地理位置的搜素（距离我当前位置1公里以内的烤肉店）\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0pqq6e8blj21m30u0gqv.jpg)\n\n# 二、elasticsearch的功能、应用场景以及特点介绍\n\n## elasticsearch的功能\n\n1、分布式的搜索引擎和数据分析引擎\n\n​\t\t\t搜索：百度、网站的站内搜索、it系统的检索。\n\n​\t\t\t数据分析：电商网站，比如最近7天牙膏这种商品销量排行前10的商家有哪些；新闻网站，最近一个月访问量排名前3的新闻版块是哪些。\n\n​\t\t\t分布式：搜索，数据分析\n\n2、全文检索，结构化检索，数据分析：\n\n​\t\t全文检索：我想搜索商品名称包含牙膏的商品， select * from products where product_name like \"%牙膏%\"\n\n​\t\t结构化检索： 我想搜索商品分类为日用品的商品都有哪些， select * from products where category_id =\"日化用品\"\n\n​\t\t数据分析：我想分析每一个商品分类下都有哪些商品，select catagory_id , count(*) from products group by category_id\n\n3、对海量数据进行近实时的处理\n\n​\t\t分布式：es自动可以将海量数据分散到多台服务器上去存储和检索\n\n​\t\t海量数据的处理：分布式以后，就可以采用大量的服务器去存储和检索数据，自然而然就可以处理海量数据处理了。\n\n​\t\t近实时：检索数据要1分钟（这就不叫近实时，可以称之为离线批处理batch-processing）；在秒级别对数据进行搜索和分析\n\n​\t\t而跟分布式/海量数据相反的：lucene，单机应用，只能在单台服务器上使用，最多只能处理单台服务器可以处理的数据量。\n\n## elasticsearch的适用场景\n\n国外：\n\n- 维基百科，类似百度百科，牙膏的维基百科，全文检索，代码高亮显示，搜索推荐。\n- The Guardian（国外新闻网站），类似搜狐新闻，用户行为日志（点击、浏览、收藏、评论）+ 社交网络数据（对xx新闻的相关看法），数据分析，给每篇新闻文章的作者，让他知道他的文章的公众反馈（好、坏、热门、垃圾、崇拜、鄙视等）\n- Stack Overflow（程序异常讨论网站），全文检索，搜索相关问题和答案。\n- Github（开源代码管理），搜索上千亿行代码。\n- 电商网站，检索商品，亚马逊等。\n- 日志数据分析，logstash采集日志，es进行复杂的数据分析。  （ELK技术，es+logstash+kibana）\n- 商品价格监控网站，用户设定某商品的价格阈值，当低于该阈值的时候，发送通知消息给用户。\n- BI系统（Business intelligence，es执行数据分析和挖掘）（比如大型商场集团分析xx区域近三年的用户消费金额的趋势以及用户群体的组成构成，产出相关的数张报表，**区，最近3年每年消费金额呈100%增长，）es执行数据分析和挖掘，kibana进行数据可视化。\n\n国内：站内搜索（电商、招聘、门户，等等），IT系统搜素（oa、crm、erp等等），数据分析（es热门的一个使用场景）\n\n## elasticsearch的特点\n\n- 可以作为一个大型分布式的集群（数百台服务器）技术，处理PB级别数据，服务大公司；也可以运行在单机上，服务小公司。\n- elasticsearch不是什么新的技术，主要是把全文检索、数据分析以及分布式技术，合并在了一起，才形成了独一无二的es；具体是lucene（全文检索），商用的数据分析软件（现有现成的），分布式数据库（mycat）\n- 对于用户而言，开箱即用，非常简单，作为中小型的应用，直接3分钟部署一下就可以在生产环境的系统来使用了，数据量不大，操作不是太复杂。\n- 相比较于数据库而言面对很多领域都是不够用的，（比如事物、还有各种联机事务型的操作）特殊的功能，比如全文检索，同义词处理，相关度排行，复杂数据分析，海量数据的近实时处理；elasticserch作为传统数据库的补充，提供了数据库所不能提供的很多其他的功能。\n\n# 三、手工画图剖析elasticsearch的核心概念：NRT、索引、分片、副本等\n\n## 1、lucene和elasticsearch的前世今生\n\nlucene，最先进、功能最强大的搜索哭，直接基于lucene开发的话非常复杂，api复杂（如果要是心一些简单的功能，需要写大量的java代码）需要深入理解原理以及（各种索引结构）\n\nelasticsearch，基于lucene，隐藏复杂性，提供简单易用的restful api接口、java api接口（还有其他）\n\n- 分布式的文档存储引擎\n- 分布式的搜索引擎和分析引擎\n- 分布式，支持PB即数据\n\n开箱即用，优秀的默认参数，不需要任何额外设置，完全开源。\n\n\n\n## 2、elasticsearch的核心概念\n\n- Near Realtime（NRT）：近实时，两个意思，从数据写入到可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级。\n- Cluster（集群）：包含多个节点，每个节点属于哪个集群是通过一个配置（集群名称：默认是elasticsearch）来决定的，对于一个中小型企业来说，刚开始一个集群就一个节点很正常。\n- Node（节点）：集群中的一个节点，节点也有一个名称（默认是随机分配的），节点名称很重要（在执行运维管理操作的时候），默认节点会去加入一个名称为“elasticsearch”的集群，如果直接启动一堆节点，那么他们就会自动组成一个elasticsearch集群，当然一个节点也可以组成一个elasticsearch集群。\n- Document（文档）：es中最小的数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以存储多个document（不过在最新的版本中type已经慢慢被移除了）。一个document里面有多个field，每个field就是一个数据字段。一个document里面有多个field，每个field就是一个数据字段。\n\n```\nproduct document\n{\n\t\"product_id\":\"1\",\n\t\"product_name\":\"高露洁牙膏\",\n\t\"product_desc\":\"高效美白\",\n\t\"category_id\":\"2\",\n\t\"category_name\":\"日化用品\"\n}\n```\n\n- Index（索引）：包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品分类索引，订单索引，索引有一个名称。一个index名称包含很多document，一个index就包含了一类类似的或者相同的document。比如说建立一个product index，商品索引，里面可能就存放了所有的商品数据，所有的商品document。\n\n- type（类型）：每个索引index中都可以有一个或者多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field，比如博客系统，有一个索引，可以定义用户type，博客数据type，评论数据type。\n\n  商品index，里面存放了所有的商品数据，商品document\n\n  但是商品有分为很多类，每个种类的document的field可能不太一样，比如说电器用品，可能还包含一些诸如售后时间范围这样特殊的field；生鲜商品还包含一些生鲜保质期之类的特殊field。\n\n  type：日化商品type；电器商品type；生鲜商品type\n\n  日化商品type： product_id，product_name，product_desc，category_id，category_name\n\n  电器商品type： product_id，product_name，product_desc，category_id，category_name，**service_period**\n\n  生鲜商品type： product_id，product_name，product_desc，category_id，category_name，**eat_period**\n\n  \n\n  每个type里面，都会包含一堆document\n\n  {\n  \t\"product_id\":\"2\",\n  \t\"product_name\":\"长虹电视\",\n  \t\"product_desc\":\"4k高清\",\n  \t\"category_id\":\"3\",\n  \t\"category_name\":\"电器\"，\n\n  ​\t\"service_period\": \"1年\"\n\n  }\n\n  {\n  \t\"product_id\":\"3\",\n  \t\"product_name\":\"基围虾\",\n  \t\"product_desc\":\"纯天然，冰岛产\",\n  \t\"category_id\":\"4\",\n  \t\"category_name\":\"生鲜\"，\n\n  ​\t\"eat_period\": \"7天\"\n\n  }\n\n  \n\n- shard（分片）：单台机器无法存储大量数据，es可以将一个索引中的数据切分为多个shard，分布在多台服务器上存储。有了shard就可以横向扩展存储更多的数据了，让搜索和分析等操作分布到多台服务器上进行执行，提升吞吐量和性能。每个shard都是一个lucene index。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0qqlk50w8j22100s044g.jpg)\n\n\n\n- replica（副本）： 任何一个服务器随时可能出现故障或者宕机，此时shard可能就会丢失，因此可以为每个shard创建多个replica副本。replica可以在shard故障时提供备用服务，保证数据的不丢失。多个replica可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能进行修改，默认5个），replica shard（随时修改参数，默认1个（意思是每个shard都有一个replica）），默认每个索引10个shard，5个primary shard ，5个replica shard，最小的高可用配置，是两台服务器。\n\n\n\n## 3、elasticsearch核心概念 vs. 数据库核心概念\n\nelasticsearch\t\t\t\t\t\t\t\t\t数据库\n\n---------------------------------------------------------\n\nDocument\t\t\t\t\t\t\t\t\t   行\n\nType\t\t\t\t\t\t\t\t\t\t\t\t 表\n\nIndex\t\t\t\t\t\t\t\t\t\t\t    数据库\n\n# 三、安装和启动elasticsearch\n\n1、安装JDK，至少1.8.0_73以上版本，java -version\n2、下载和解压缩Elasticsearch安装包，目录结构\n3、启动Elasticsearch：bin\\elasticsearch.bat，es本身特点之一就是开箱即用，如果是中小型应用，数据量少，操作不是很复杂，直接启动就可以用了\n\n4、检查ES是否启动成功：http://localhost:9200/?pretty\n\nname: node名称\ncluster_name: 集群名称（默认的集群名称就是elasticsearch）\nversion.number: 5.2.0，es版本号\n\n{\n  \"name\" : \"4onsTYV\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"nKZ9VK_vQdSQ1J0Dx9gx1Q\",\n  \"version\" : {\n    \"number\" : \"5.2.0\",\n    \"build_hash\" : \"24e05b9\",\n    \"build_date\" : \"2017-01-24T19:52:35.800Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"6.4.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n\n5、修改集群名称：elasticsearch.yml\n6、下载和解压缩Kibana安装包，使用里面的开发界面，去操作elasticsearch，作为我们学习es知识点的一个主要的界面入口\n7、启动Kibana：bin\\kibana.bat\n8、进入Dev Tools界面\n9、执行GET _cluster/health\n\n# 四、快速入门案例实战之电商网站集群健康检查、文档CRUD\n\n## 1、document数据格式\n\n面向文档的搜索分析引擎\n\n（1）应用系统的数据结构都是面向对象的，复杂的\n（2）对象数据存储到数据库中，只能拆解开来，变为扁平的多张表，每次查询的时候还得还原回对象格式，相当麻烦\n（3）ES是面向文档的，文档中存储的数据结构，与面向对象的数据结构是一样的，基于这种文档数据结构，es可以提供复杂的索引，全文检索，分析聚合等功能\n（4）es的document用json数据格式来表达\n\npublic class Employee {\n\n  private String email;\n  private String firstName;\n  private String lastName;\n  private EmployeeInfo info;\n  private Date joinDate;\n\n}\n\nprivate class EmployeeInfo {\n\n  private String bio; // 性格\n  private Integer age;\n  private String[] interests; // 兴趣爱好\n\n}\n\nEmployeeInfo info = new EmployeeInfo();\ninfo.setBio(\"curious and modest\");\ninfo.setAge(30);\ninfo.setInterests(new String[]{\"bike\", \"climb\"});\n\nEmployee employee = new Employee();\nemployee.setEmail(\"zhangsan@sina.com\");\nemployee.setFirstName(\"san\");\nemployee.setLastName(\"zhang\");\nemployee.setInfo(info);\nemployee.setJoinDate(new Date());\n\nemployee对象：里面包含了Employee类自己的属性，还有一个EmployeeInfo对象\n\n两张表：employee表，employee_info表，将employee对象的数据重新拆开来，变成Employee数据和EmployeeInfo数据\nemployee表：email，first_name，last_name，join_date，4个字段\nemployee_info表：bio，age，interests，3个字段；此外还有一个外键字段，比如employee_id，关联着employee表\n\n{\n    \"email\":      \"zhangsan@sina.com\",\n    \"first_name\": \"san\",\n    \"last_name\": \"zhang\",\n    \"info\": {\n        \"bio\":         \"curious and modest\",\n        \"age\":         30,\n        \"interests\": [ \"bike\", \"climb\" ]\n    },\n    \"join_date\": \"2017/01/01\"\n}\n\n我们就明白了es的document数据格式和数据库的关系型数据格式的区别\n\n\n\n\n\n## 2、电商网站商品管理案例背景介绍\n\n有一个电商网站，需要为其基于ES构建一个后台系统，提供以下功能：\n\n（1）对商品信息进行CRUD（增删改查）操作\n（2）执行简单的结构化查询\n（3）可以执行简单的全文检索，以及复杂的phrase（短语）检索\n（4）对于全文检索的结果，可以进行高亮显示\n（5）对数据进行简单的聚合分析\n\n\n\n## 3、简单的集群管理\n\n（1）快速检查集群的健康状况\n\nes提供了一套api，叫做cat api，可以查看es中各种各样的数据\n\nGET /_cat/health?v\n\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1488006741 15:12:21  elasticsearch yellow          1         1      1   1    0    0        1             0                  -                 50.0%\n\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1488007113 15:18:33  elasticsearch green           2         2      2   1    0    0        0             0                  -                100.0%\n\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1488007216 15:20:16  elasticsearch yellow          1         1      1   1    0    0        1             0                  -                 50.0%\n\n如何快速了解集群的健康状况？green、yellow、red？\n\ngreen：每个索引的primary shard和replica shard都是active状态的\nyellow：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态\nred：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了\n\n为什么现在会处于一个yellow状态？\n\n我们现在就一个笔记本电脑，就启动了一个es进程，相当于就只有一个node。现在es中有一个index，就是kibana自己内置建立的index。由于默认的配置是给每个index分配5个primary shard和5个replica shard，而且primary shard和replica shard不能在同一台机器上（为了容错）。现在kibana自己建立的index是1个primary shard和1个replica shard。当前就一个node，所以只有1个primary shard被分配了和启动了，但是一个replica shard没有第二台机器去启动。\n\n做一个小实验：此时只要启动第二个es进程，就会在es集群中有2个node，然后那1个replica shard就会自动分配过去，然后cluster status就会变成green状态。\n\n\n\n（2）快速查看集群中有哪些索引\n\nGET /_cat/indices?v\n\nhealth status index                           uuid                   \t\t\t\t\t\t\t\tpri \trep \tdocs.count \tdocs.deleted \tstore.size pri.store.size\ngreen  open   .geoip_databases                RH_elHLcR-CAPrs61JpkOg   1   \t0         44            \t\t5     \t\t\t\t\t41.4mb  41.4mb\n\n(3) 简单的索引操作\n\n创建索引： PUT /test_index?pretty\n\n删除索引： DELETE /test_index?pretty\n\n\n\n## 4、 商品的CRUD操作\n\n \t1、新增商品： 新增文档，建立索引\n\n\n\n> **PUT /index/type/id**\n\n```\nPUT /ecommerce/product/1\n{\n  \"name\":\"gaolujie yagao\",\n  \"desc\":\"gaoxiao meibai\",\n  \"price\": 30,\n  \"producer\": \"gaolujie producer\",\n  \"tags\":[\"meibai\", \"fangzhu\"]\n}\n\n\n\nPUT /ecommerce/product/2\n{\n  \"name\":\"jiajieshi yagao\",\n  \"desc\":\"youxiao fangzhu\",\n  \"price\": 25,\n  \"producer\": \"jiajieshi producer\",\n  \"tags\":[\"fangzhu\"]\n}\n\n\n\nPUT /ecommerce/product/3\n{\n  \"name\":\"zhonghua yagao\",\n  \"desc\":\"caoben zhiwu\",\n  \"price\": 40,\n  \"producer\": \"zhonghua producer\",\n  \"tags\":[\"qingxin\"]\n}\n```\n\n\n\n**es会自动的建立index和type，不需要提前创建，而且es默认会对document每个field都建立倒排索引，让其可以被搜索到**\n\n\n\n​\t2、查询商品：检索文档\n\n\n\n```\nGET /ecommerce/product/1\n```\n\n```\n结果：\n\n{\n  \"_index\" : \"ecommerce\",\n  \"_type\" : \"product\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"_seq_no\" : 0,\n  \"_primary_term\" : 1,\n  \"found\" : true,\n  \"_source\" : {\n    \"name\" : \"gaolujie yagao\",\n    \"desc\" : \"gaoxiao meibai\",\n    \"price\" : 30,\n    \"producer\" : \"gaolujie producer\",\n    \"tags\" : [\n      \"meibai\",\n      \"fangzhu\"\n    ]\n  }\n}\n```\n\n​\t3、修改商品：替换文档(必须带上所有的field信息才可以进行修改)**注意这里是put请求**\n\n```\nPUT /ecommerce/product/1\n\n{\n\t\"name\":\"jiaqiangban gaolujie yagao\",\n\t\"desc\": \"gaoxiao meibai\",\n\t\"price\":30,\n\t\"producer\":\"gaolujie producer\",\n\t\"tags\":[\"meibai\",\"fangzhu\",\"qingxin\"]\n}\n```\n\n返回的结果：\n\n```\n{\n  \"_index\" : \"ecommerce\",\n  \"_type\" : \"product\",\n  \"_id\" : \"1\",\n  \"_version\" : 2,\n  \"result\" : \"updated\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 3,\n  \"_primary_term\" : 1\n}\n```\n\n​\t4、修改商品：更新文档(局部数据)。**修改商品 这里是post请求**\n\n```\nPOST /ecommerce/product/1/_update\n{\n\t\"doc\":{\n\t  \"name\":\"jiaqiangban gaolujie yagao gengxin\"\n\t}\n}\n```\n\n结果：\n\n```\n{\n  \"_index\" : \"ecommerce\",\n  \"_type\" : \"product\",\n  \"_id\" : \"1\",\n  \"_version\" : 3,\n  \"result\" : \"updated\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 4,\n  \"_primary_term\" : 1\n}\n```\n\n​\t5、删除商品\n\n```\nDELETE /ecommerce/product/1?pretty\n```\n\n\n\n# 五、快速入门案例实战之电商网站商品管理：多种搜索方式\n\n## 1、query string search\n\n搜索全部商品： GET /ecommerce/product/_search\n\n结果：\n\n```\n{\n  \"took\" : 3,   took耗费了几毫秒\n  \"timed_out\" : false,  是否超时\n  \"_shards\" : {    数据拆分成了1个分片，所以对于搜索请求，会发送到所有的primary shard（或者它的某个relica shard中也可以）\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"skipped\" : 0,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 3,      查询结果的数量，3个document\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 1.0,   score的含义，就是document对于一个search的相关度的匹配分数，越相关，就越匹配，分数也越高\n    \"hits\" : [     包含了匹配搜索结果document的详细数据\n      {\n        \"_index\" : \"ecommerce\",\n        \"_type\" : \"product\",\n        \"_id\" : \"2\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"jiajieshi yagao\",\n          \"desc\" : \"youxiao fangzhu\",\n          \"price\" : 25,\n          \"producer\" : \"jiajieshi producer\",\n          \"tags\" : [\n            \"fangzhu\"\n          ]\n        }\n      },\n      {\n        \"_index\" : \"ecommerce\",\n        \"_type\" : \"product\",\n        \"_id\" : \"3\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"zhonghua yagao\",\n          \"desc\" : \"caoben zhiwu\",\n          \"price\" : 40,\n          \"producer\" : \"zhonghua producer\",\n          \"tags\" : [\n            \"qingxin\"\n          ]\n        }\n      },\n      {\n        \"_index\" : \"ecommerce\",\n        \"_type\" : \"product\",\n        \"_id\" : \"1\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"gaolujie yagao\",\n          \"desc\" : \"gaoxiao meibai\",\n          \"price\" : 30,\n          \"producer\" : \"gaolujie producer\",\n          \"tags\" : [\n            \"meibai\",\n            \"fangzhu\"\n          ]\n        }\n      }\n    ]\n  }\n}\n```\n\n\n\n搜索商品名称中包含yagao的商品： GET /ecommerce/product/_search?q=name:yagao&sort=price:desc\n\n适用于临时的在命令行使用的一些工具，比如crul，快速的发出请求，来检索想要的答案；但是如果查询请求比较复杂，是很难通过去构建在生产环境中的，几乎很少使用query srting search\n\n-------------\n\n## 2、query DSL\n\nDSL: Domain Specified Language，特定领域的语言\n\nhttp request body： 请求体，可以用json格式来构建查询语法，比较方便，可以构建各种复杂的语法 比query string search强大多\n\n```\nGET /ecommerce/product/_search\n{\n  \"query\":{\n    \"match_all\":{}\n  }\n}\n```\n\n**查询名称包含yagao的商品，同时按照价格降序排序：**\n\n```\nGET /ecommerce/product/_search\n{\n  \"query\":{\n    \"match\":{\n      \"name\": \"yagao\"\n    }\n  },\n  \"sort\":[\n    {\"price\":\"desc\"}\n  ]\n}\n```\n\n**分页查询商品**\n\n```\nGET /ecommerce/product/_search\n{\n  \"query\":{\n    \"match_all\":{}\n  }, \n  \"from\":1,    从哪一个开始\n  \"size\":1     查询几个\n}\n```\n\n**指定要查询的结果的筛选字段使用_source**\n\n```\nGET /ecommerce/product/_search\n{\n  \"query\":{\n    \"match_all\":{}\n  },\n  \"_source\":[\"name\",\"price\"]\n}\n```\n\n​\t\n\n## 3、query filter\n\n搜索名称中包含牙膏，而且售价大于25的商品\n\n```\nGET /ecommerce/product/_search\n{\n  \"query\":{\n    \"bool\":{\n      \"must\":{\n          \"match\":{\n            \"name\":\"yagao\"\n          }\n      },\n      \"filter\": {\n        \"range\": {\n          \"price\": {\n            \"gt\": 25\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n## 4、full-text search全文检索\n\n \n\n```\nget /ecommerce/product/_search\n{\n  \"query\":{\n    \"match\":{\n      \"producer\": \"yagao producer\"\n    }\n  }\n  \n}\n```\n\nproducer这个字段，会先被拆解，建立倒排索引\n\nspecial 4\n\nyagao  4\n\nproducer 1，2，3，4\n\ngaolujie  1\n\nzhonghua 3\n\njiajieshi 2\n\n\n\n## 5、phrase search（短语搜索）\n\n跟全文检索相对应，相反，去倒排索引里面去一一匹配，只要能匹配上任意一个拆解后的单词，就可以作为结果返回。\n\npyrase search ，要求输入的搜索串，必须在指定的字段中完全包含一模一样的，才可以算作匹配，才能结果返回\n\n```\nget /ecommerce/product/_search\n{\n  \"query\":{\n    \"match_phrase\":{\n      \"producer\": \"yagao producer\"\n    }\n  }\n  \n}\n```\n\n## 6、highlight search（高亮搜索）\n\n```\nget /ecommerce/product/_search\n{\n  \"query\":{\n    \"match\":{\n      \"producer\": \"producer\"\n    }\n  },\n  \"highlight\":{\n    \"fields\":{\n      \"producer\": {}\n    }\n  } \n}\n```\n\n# 六、快速入门案例实战之电商网站商品管理：聚合嵌套、下钻分析、聚合分析\n\n第一个分析需求：计算每个tag下的商品数量\n\n首先需要将文本field的fielddata属性设置为true，\n\n> ElasticSearch创建索引报错 **Types cannot be provided in put mapping requests, unless the include_type_name parameter is set to true**\n>\n> 原因是elasticsearch7.x版本不支持type（低版本写法）所致，所以在高版本使用type，需要传入include_type_name参数，值为true。\n\n```\nPUT /ecommerce/_mapping/product?include_type_name=true\n{\n    \"properties\":{\n      \"tags\":{\n        \"type\":\"text\",\n        \"fielddata\":true\n      }\n    }\n}\n```\n\n返回结果：\n\n```\n{\n  \"acknowledged\" : true\n}\n\n```\n\n然后再进行聚合聚合分析\n\n```\nGET /ecommerce/product/_search\n{\n  \"aggs\":{\n    \"group_by_tags\":{\n      \"terms\":{\n        \"field\":\"tags\"  \n      }\n    }\n  }\n}\n```\n\n结果：\n\n```\n{\n  \"took\" : 39,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"skipped\" : 0,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 4,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"ecommerce\",\n        \"_type\" : \"product\",\n        \"_id\" : \"2\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"jiajieshi yagao\",\n          \"desc\" : \"youxiao fangzhu\",\n          \"price\" : 25,\n          \"producer\" : \"jiajieshi producer\",\n          \"tags\" : [\n            \"fangzhu\"\n          ]\n        }\n      },\n      {\n        \"_index\" : \"ecommerce\",\n        \"_type\" : \"product\",\n        \"_id\" : \"3\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"zhonghua yagao\",\n          \"desc\" : \"caoben zhiwu\",\n          \"price\" : 40,\n          \"producer\" : \"zhonghua producer\",\n          \"tags\" : [\n            \"qingxin\"\n          ]\n        }\n      },\n      {\n        \"_index\" : \"ecommerce\",\n        \"_type\" : \"product\",\n        \"_id\" : \"4\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"special yagao\",\n          \"desc\" : \"special meibai\",\n          \"price\" : 50,\n          \"producer\" : \"special yagao producer\",\n          \"tags\" : [\n            \"meibai\",\n            \"gaoxiao\"\n          ]\n        }\n      },\n      {\n        \"_index\" : \"ecommerce\",\n        \"_type\" : \"product\",\n        \"_id\" : \"1\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"gaolujie yagao\",\n          \"desc\" : \"gaoxiao meibai\",\n          \"price\" : 30,\n          \"producer\" : \"gaolujie producer\",\n          \"tags\" : [\n            \"meibai\",\n            \"fangzhu\"\n          ]\n        }\n      }\n    ]\n  },\n  \"aggregations\" : {\n    \"group_by_tags\" : {\n      \"doc_count_error_upper_bound\" : 0,\n      \"sum_other_doc_count\" : 0,\n      \"buckets\" : [\n        {\n          \"key\" : \"fangzhu\",\n          \"doc_count\" : 2\n        },\n        {\n          \"key\" : \"meibai\",\n          \"doc_count\" : 2\n        },\n        {\n          \"key\" : \"gaoxiao\",\n          \"doc_count\" : 1\n        },\n        {\n          \"key\" : \"qingxin\",\n          \"doc_count\" : 1\n        }\n      ]\n    }\n  }\n}\n```\n\n但是此时把很多原始的数据给查询出来了，结果只是我们想要的aggregations这个列表中\n\n我们可以添加一个 size：0来解决这个问题。\n\n```\nGET /ecommerce/product/_search\n{\n  \"size\":0,\n  \"aggs\":{\n    \"group_by_tags\":{\n      \"terms\":{\n        \"field\":\"tags\"  \n      }\n    }\n  }\n}\n```\n\n这样结果就只有聚合分析的结果了。\n\n------------\n\n第二个聚合分析的需求：对名称中包含yagao的商品，计算每个tag下的商品数量\n\n就是在聚合结果的基础上添加一个搜索。就是按照tags进行分组然后在此基础上筛选name为gaolujie的结果，其中aggs下一级：group_by_tags可以自己定义。\n\n```\nGET /ecommerce/product/_search\n{\n  \"query\":{\n    \"match\":{\n      \"name\":\"gaolujie\"\n    }\n  },\n  \"size\":0,\n  \"aggs\":{\n    \"group_by_tags\":{  #可以自定义起名字\n      \"terms\":{\n        \"field\":\"tags\"  \n      }\n    }\n  }\n}\n```\n\n第三个聚合分析的需求：先分组，在计算每组的平均值，计算每个tag下的商品的平均价格\n\n通过嵌套聚合这类语法来进行：\n\n```\nGET /ecommerce/product/_search\n{\n  \"size\":0,\n  \"aggs\":{\n    \"group_by_tags\":{\n      \"terms\":{\n        \"field\":\"tags\"  \n      },\n      \"aggs\":{  #在聚合的基础上再进行聚合，这里聚合了平均值\n        \"avg_price\":{\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n\n\n结果如下：\n\n```\n{\n  \"took\" : 6,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"skipped\" : 0,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 4,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : null,\n    \"hits\" : [ ]\n  },\n  \"aggregations\" : {\n    \"group_by_tags\" : {\n      \"doc_count_error_upper_bound\" : 0,\n      \"sum_other_doc_count\" : 0,\n      \"buckets\" : [\n        {\n          \"key\" : \"fangzhu\",\n          \"doc_count\" : 2,\n          \"avg_price\" : {\n            \"value\" : 27.5\n          }\n        },\n        {\n          \"key\" : \"meibai\",\n          \"doc_count\" : 2,\n          \"avg_price\" : {\n            \"value\" : 40.0\n          }\n        },\n        {\n          \"key\" : \"gaoxiao\",\n          \"doc_count\" : 1,\n          \"avg_price\" : {\n            \"value\" : 50.0\n          }\n        },\n        {\n          \"key\" : \"qingxin\",\n          \"doc_count\" : 1,\n          \"avg_price\" : {\n            \"value\" : 40.0\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n\n\n需求：聚合类后再对价格进行聚合分析，然后再对聚合分析后的平均价格进行order排序。\n\n```\nGET /ecommerce/product/_search\n{\n  \"size\":0,\n  \"aggs\":{\n    \"group_by_tags\":{\n      \"terms\":{\n        \"field\":\"tags\",\n        \"order\": {\n          \"avg_price\": \"desc\"\n        }\n      },\n      \"aggs\":{\n        \"avg_price\":{\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n以上都是使用es的restful api方式进行学习知识点和使用，并没有使用一些编程语言（java），原因如下：restful api才是最基础的\n\n\n\n第五个数据分析需求：按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格。\n\n```\nGET /ecommerce/product/_search\n{\n  \"size\":0,\n  \"aggs\":{\n    \"group_by_range\":{\n      \"range\":{\n        \"field\":\"price\",\n        \"ranges\":[\n          {\n            \"from\":0,\n            \"to\": 20\n          },\n          {\n            \"from\":20,\n            \"to\":40\n          },\n          {\n            \"from\":40,\n            \"to\":50\n          }\n        ]\n      },\n      \"aggs\":{\n        \"group_by_tags\":{\n          \"terms\": {\n            \"field\": \"tags\"\n          },\n          \"aggs\":{\n            \"average_price\":{\n              \"avg\": {\n                \"field\": \"price\"\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n结果如下：\n\n```\n{\n  \"took\" : 12,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"skipped\" : 0,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 4,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : null,\n    \"hits\" : [ ]\n  },\n  \"aggregations\" : {\n    \"group_by_range\" : {\n      \"buckets\" : [\n        {\n          \"key\" : \"0.0-20.0\",\n          \"from\" : 0.0,\n          \"to\" : 20.0,\n          \"doc_count\" : 0,\n          \"group_by_tags\" : {\n            \"doc_count_error_upper_bound\" : 0,\n            \"sum_other_doc_count\" : 0,\n            \"buckets\" : [ ]\n          }\n        },\n        {\n          \"key\" : \"20.0-40.0\",\n          \"from\" : 20.0,\n          \"to\" : 40.0,\n          \"doc_count\" : 2,\n          \"group_by_tags\" : {\n            \"doc_count_error_upper_bound\" : 0,\n            \"sum_other_doc_count\" : 0,\n            \"buckets\" : [\n              {\n                \"key\" : \"fangzhu\",\n                \"doc_count\" : 2,\n                \"average_price\" : {\n                  \"value\" : 27.5\n                }\n              },\n              {\n                \"key\" : \"meibai\",\n                \"doc_count\" : 1,\n                \"average_price\" : {\n                  \"value\" : 30.0\n                }\n              }\n            ]\n          }\n        },\n        {\n          \"key\" : \"40.0-50.0\",\n          \"from\" : 40.0,\n          \"to\" : 50.0,\n          \"doc_count\" : 1,\n          \"group_by_tags\" : {\n            \"doc_count_error_upper_bound\" : 0,\n            \"sum_other_doc_count\" : 0,\n            \"buckets\" : [\n              {\n                \"key\" : \"qingxin\",\n                \"doc_count\" : 1,\n                \"average_price\" : {\n                  \"value\" : 40.0\n                }\n              }\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n\n\n# 七、手工画图剖析es的基础分布式架构\n\n## 1、elasticsearch对复杂分布式机制的透明隐藏特性\n\nes是一套分布式的系统，分布式为了应对大数据量。\n\n隐藏了复杂的分布式机制：\n\n​\t分片机制（之前随随便便就把document插入到es集群中去了，并没有关注数据怎么进行分片的，数据到了哪个shard中去\n\n​\tcluster discovery：集群发现机制，当直接启动第二个进程作为node2，这时候就自动发现了集群，并且加入了进去还接受了部分数据，replica shard\n\n​\tshard负载均衡：假设现在有3个节点，总共有25个shard要分配到3个节点上面去，es会自动进行均匀分配，以保持每个节点的均衡的读写负载请求\n\n​\tshard副本：\n\n​\t请求路由：\n\n​\t集群扩容：\n\n​\tshard重分配：\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0s71hjtcqj20zm0n2myc.jpg)\n\n​\t\n\n## 2、elasticsearch的垂直扩容与水平扩容\n\n\n\n垂直扩容：采购更强大服务器，成本非常昂贵，有瓶颈\n\n水平扩容：业内常常采用的方案，采购越来越多普通的服务器，性能比较一般但是很多普通服务器组合在一起就能构成强大的计算和存储能力。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0s7rihy95j21hi0qun08.jpg)\n\n## 3、增加或减少节点时的数据rebalance\n\n增加或减少节点时的重新负载均衡\n\n## 4、master节点\n\n### \t管理es集群的元数据：\n\n​\t\t\t\t\t比如说索引的创建和删除、维护索引元数据；节点的增加和移除，维护集群的元数据，master节点实质上是做了一些轻量级的工作。\n\n​\t\t\t\t\t默认情况下，会自动选择出一台节点，作为master节点。\n\n## 5、节点平等的分布式架构\n\n### \t（1）节点对等，每个节点都能接收所有的请求。 \n\n### \t（2）自动请求路由\n\n​\t\t\t\t\t当通过对等节点进行请求时，会通过这个对等节点自动的请求路由到合适的shard上去进行搜索。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0syzagbiej21l00pyjvx.jpg)\n\n### \t（3）响应收集\n\n\n\n# 八、shard&replica机制再次梳理以及单node节点中创建index图解\n\n## 1、shard&replica机制再次梳理\n\n- index包含多个shard\n- 每个shard都是一个最小工作单元，承载部分数据，是一个lucene实例，完整的建立索引和处理请求的能力。\n- 增减节点时候，shard会自动在nodes中进行负载均衡。\n- primary shard和replica shard，每个document肯定只存在于某一个primary shard以及对应的replica shard中，不可能存在于多个primary shard中。\n- replica shard是primary shard的副本，负责容错，以及承担读请求负载。\n- primary shard数量在创建索引时就固定了，replica shard的数量可以随时修改。\n- primary shard的默认数量是5，replica默认是1，默认有10个shard，5个primary shard，5个replica shard。\n- primary shard不能和自己的replica shard放在同一个节点上，否则节点宕机，primary shard和副本都会丢失，起不到容错的作用，但是可以和其他primary shard的replica shard放在同一个节点上。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0t1dctceuj21w80p0diy.jpg)\n\n## 2、图解单node环境下创建index是什么样子\n\n- 单node环境下，创建一个index，有3个primary shard，3个replica shard。\n\n- 集群statsus是yellow的。\n\n- 这时候，只会将3个primary shard分配到仅有的一个node上去，另外3个replica shard是无法分配的。\n\n- 集群可以正常进行工作，但是一旦出现了节点宕机，那么数据就会全部丢失，而且集群不可用，无法承接任何的需求。\n\n  ```json\n  PUT /test_index\n  {\n    \"settings\": {\n      \"number_of_shards\": 3,\n      \"number_of_replicas\": 1\n    }\n  }\n  ```\n\n  ![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0t3d5n9eij20yk0d0dg7.jpg)\n\n  # 九、图解2个node环境下replica shard是如何分配的\n\n  ![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0t3i4unr0j212w0ny75p.jpg)\n\n- ​\treplica shard分配： 3个primary shard， 3个replica shard，1个node节点\n\n- primary shard 与 replica shard 同步 可以查询同样的功能操作\n\n- 读请求的话 一个java程序都可以对primry和replica进行读取操作。（关于存储不清楚是不是可以）\n\n# 九、分布式原理：图解横向扩容过程，如何超出扩容极限，以及如何提升容错性\n\n\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0t3ngtykqj21gm09g3z0.jpg)\n\n6个shard被平均分配到3个node上 对于每一份数据都会存储在不同的node上，如p0与r0 ，如果出现某一台节点宕机，并不会影响。尽量保证每个shard上面有相同的节点数量。\n\n\n\n- Primary&replica自动负载均衡，6个shard，3个primary，3个replica。\n\n- 每个node有更少的shard，IO/CPU/Memory资源给每个shard分配更多，每个shard性能更好。\n\n- 扩容的极限，6个shard（3primary，3replica），最多扩容到6台机器，每个shard可以占用单台服务器的所有资源，性能最好。\n\n- 超出扩容极限，可以动态的修改replica数量，9个shard（3个primary，6个replica），最多可以扩容到9台机器，比3台机器时，拥有3倍的读吞吐量。\n\n- 3台机器下，9个shard（3个primary，6个replica），资源更少，但是容错性更好，最多容纳两台的机器宕机，6个shard只能容纳1台机器宕机。\n\n- 这里的这些知识点，综合起来，就是说，一方面告诉你扩容的原理，怎么扩容，怎么提升系统整体的吞吐量；另一方面要考虑到系统的容错性，怎么保证提高容错性，让尽可能多的服务器宕机，保证数据尽可能的不丢失。\n\n  \n\n  ![image-20220331172132565](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0t6t2vysvj21q80pmgq1.jpg)\n\n  \n\n# 十、图解elastic的容错机制：master选举，replica容错， 数据恢复\n\n> （1） 9shard，3 node\n>\n> （2）master node宕机，自动master选举，red状态\n>\n> （3）replica容错：新master将replica提升为primary shard， yellow状态\n>\n> （4）重启宕机node，master copy replica到该node，使用原有的shard并同步宕机后的修改，green状态\n\n![image-20220331173815548](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0t7agjm14j21f80dc757.jpg)\n\nmaster node宕机的一瞬间，p0这个primary shard就没了，此时\n\n容错第一步：master选举，自动选举另外一个node成为新的master，承担起master的责任来。\n\n容错第二步：新master，将丢失掉的primary shard的某个replica shard提升为primary shard，此时cluster status会变为yellow。\n\n容错第三步：重启故障的node，new master，会将缺失的副本都copy一份到该node上去，而且该node会使用之前已有的shard数据，只是同步一下宕机之后发生过的修改。\n\n![image-20220331173514618](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0t7a9usplj21mg0pwjvv.jpg)\n\n## 十、初步解析document核心元数据以及图解剖析index创建范例\n\n```json\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"_seq_no\" : 0,\n  \"_primary_term\" : 1,\n  \"found\" : true,\n  \"_source\" : {\n    \"settings\" : {\n      \"number_of_shards\" : 3,\n      \"number_of_replicas\" : 1\n    }\n  }\n}\n```\n\n\n\n## 1、_index元数据\n\n![image-20220401103322149](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0u0mqju7kj21rq0ryjxh.jpg)\n\n类似的数据放在一个index，这批数据的功能和支持的需求，可能是很类似的，相同功能的数据放在一个index，在自己独立的shard中，与其他的数据不在一个shard中，就不会相互影响。\n\n- 代表一个document存放在哪个index中。\n- 类似的数据放在一个索引，非类似的数据放在不同的索引：product index（包含了所有的商品），sales index（包含了所有的商品销售数据），inventory index（包含了所有库存相关的数据）。如果把比如product，sales，human，resource全部都放在一个大的index里面，比如说company index中事不合适的。\n- index中包含了很多类似的document：类似指这些document的fields很大一部分是相同的，你说你放了3个document，每个document的fields都完全不一样，这就不是类似了，就不太适合放在一个index里面去。\n- **索引名称必须是小写**，不能用下划线开头，不能包含逗号。\n\n## 2、_type元数据\n\n- 代表document属于index中那个类别（type）\n- 一个索引通常会划分为多个type，逻辑上对index中有些许不同的几类数据进行分类：因为一批相同的数据，可能会有很多相同的fields，但是还是可能会有一些轻微的不同，可能会有少数的fields是不一样的，举个例子，商品可以划分为电子商品、生鲜商品、日化商品等等。\n- **type名称可以是大写或者小写**，但是同时不能用下划线开头，不能包含逗号。\n\n## 3、_id元数据\n\n- 代表document的唯一标识，与index和type一起，可以唯一标识和定位一个document\n- 可以手动指定document的id（put /index/type/id），也可以不指定，es自动给为我们创建一个id。\n\n# 十一、document中id的手动指定和自动生成两种策略解析\n\n- 手动指定document id\n\n  - 应当根据应用的情况来说，是否满足手动指定document id的前提：\n\n  一般来说，是从某些其他的系统中，导入一些数据到es时，会采取这种方式，就是使用系统中已有数据的唯一标识，作为es中document的id，举个例子，比如现在开发的一个电商网站做搜索功能，或者是oa系统做员工检索系统，这个时候数据首先在网站或者是it系统内部的数据库中先有一份，此时就会有一个数据库的primary key（自增长、uuid或者是业务编号），如果将数据导入到es中，此时就比较适合采用之前数据在数据库中的primary key。\n\n  - ​\tput /index/type/id\n\n- 自动生成document id\n\n```json\npost /test_index/test_type\n{\n  \"test_content\":\"my test noid\"\n}\n```\n\n这里使用的是post 并非手动指定id的put\n\n```json\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"dvw0438BQ1kM2qC8Hjfc\",   #这里是自动生成的id\n  \"_version\" : 1,\n  \"result\" : \"created\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 1,\n  \"_primary_term\" : 1\n}\n```\n\n- 自动生成的id，长度为20个字符，url安全，bse64编码，GUID方式进行生成的，保证分布式系统生成的时候并不会产生冲突。\n\n\n\n![image-20220401115233015](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0u2x2qj2cj21f60nsgo2.jpg)\n\n# 十二、document的source元数据以及定制返回结果解析\n\n- _source元数据\n\n```json\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"1\",\n  \"_version\" : 2,\n  \"_seq_no\" : 1,\n  \"_primary_term\" : 1,\n  \"found\" : true,\n  \"_source\" : {\n    \"test_field1\" : \"test field1\",\n    \"test_field2\" : \"test field2\"\n  }\n}\t\n```\n\n在创建一个document的时候，使用的那个放在request body中的json串，默认情况下，在get的时候，会原封不动的返回回来。\n\n- 定制返回结果\n\n  \n\n```json\nGET /test_index/test_type/1?_source=test_field2\n```\n\n返回多个field 使用逗号隔开，\n\n```json\nGET /test_index/test_type/1?_source=test_field1,test_field2\n```\n\n当其不存在时候的发送put是创建，当存在的时候再发送就是全量替换了\n\n# 十三、document的全量替换、强制创建、删除\n\n## 1、document的全量替换\n\n- 语法与创建文档是一样的，如果document id不存在，那么就是创建；如果document id存在，那么就是全量替换操作，替换document的json串内容。\n- document是不可变的，如果要修改document的内容，第一种方式就是全量替换，直接对document重新建立索引，替换里面的所有内容。\n- es会将老的document标记为deleted，然后新增我们给定的一个document，当我们创建越来越多的document的时候，es会在适当时机在后台自动删除标记为deleted的document。\n\n![image-20220402105847047](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0v6zhe8wzj217m0pyabx.jpg)\n\n## 2、document的强制创建\n\n- 创建文档与全量替换的语法是一样的，有时我们只是想创建文档，而不想替换文档，如果强制进行创建呢？\n\n- ```\n  PUT /test_index/test_type/1?_create   \n  ```\n\n  ```\n  PUT /test_index/test_type/1?op_type=create\n  ```\n\n  > **这里在7.16版本中不行，未做解决。**\n\n## 3、document的删除\n\n```\nPUT /test_index/test_type/1?op_type=create\n```\n\n并不会物理删除，而是标记为deleted ，只有当数据越来越多的时候，在后台自动删除\n\n# 十四、深度图解剖析elasticsearch并发冲突问题\n\n> 举例子：电商场景下，假设我们有个程序，工作流程是如下：\n>\n> 1、读取商品信息（包含了商品库存）\n>\n> 2、用户下单购买\n>\n> 3、更新商品信息（主要讲库存减1）\n\n比如我们的程序是多线程，所以可以能有多个线程并发的去执行上述的3步操作。\n\n又一个牙膏，库存100件，现在同时有两个人都过来读取了牙膏的数据，然后下单购买了该管牙膏，此时两个线程并发的服务于两个人，同时再进行商品库存数据的修改/\n\n![image-20220402135058407](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0vbyltpn0j21wa0rcwlp.jpg)\n\n# 十五、深度剖析乐观锁与悲观锁两种并发控制方案\n\n悲观锁并发控制方案，就是在各种情况下，都上锁。都上锁之后就只有一个线程可以操作这一条数据了，当然不同的场景下，上的锁不同，有行级锁、表级锁、读锁、写锁。\n\n![image-20220402155415715](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0vfiw2ocfj21zw0r2n55.jpg)\n\n1、悲观锁的优点：方便，直接加锁，对应用程序来说，透明不需要做额外的操作；缺点，并发能力很弱，同一时间只能有一条线程操作数据。\n\n2、乐观锁的优点：并发能力很高，不给数据加锁，大量线程并发操作；缺点：麻烦，每次更新的时候都要先对比版本号，然后可能需要重新\n\n# 十六、图解elasticsearch内部如何基于_version进行乐观锁并发控制\n\n第一次创建document的时候，它的_version内部版本号就是1；以后每次对这个document执行修改或者删除操作，都会对这个version版本号自动加1，哪怕是删除，也会对这条数据的版本号加1\n\n在删除一个document之后，从侧面验证他不是立即物理删除掉的，因为她的一些版本号信息还是保留着的，先删除一个document，然后再创建这个条document，其实会在delete version的基础上，再把version+1\n\n![image-20220402173716684](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h0vii2divoj21sm0pe0xi.jpg)\n\n# 十七、上机器动手实战演练基于_vesion进行乐观锁并发控制\n\n- 先构造一条数据出来\n\n```json\nPUT /test_index/test_type/7\n{\n  \"test_field\":\"test test\"\n}\n```\n\n- 模拟两个客户端，都获取到了同一条数据\n\n```\nGET test_index/test_type/7\n```\n\n```\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"7\",\n  \"_version\" : 1,\n  \"_seq_no\" : 0,\n  \"_primary_term\" : 2,\n  \"found\" : true,\n  \"_source\" : {\n    \"test_field\" : \"test test\"\n  }\n}\n```\n\n- 其中一个客户端先更新了这个数据，同时带上了数据的版本号，确保地说es中的数据的版本号，跟客户端中的数据的版本号是相同的，才能修改\n\n```json\nPUT /test_index/test_type/7?version=1\n{\n  \"test_field\":\"test client 1\"\n}\n```\n\n**但是这样在旧版本中可行，在es新版本中注意：一些老的版本es使用version，但是新版本不支持了，会报这个错误，提示我们用if_seq_no和if_primary_term。**\n\n**版本元数据**\n**_seq_no：文档版本号，作用同_version（相当于学生编号，每个班级的班主任为学生分配编号，效率要比学校教务处分配来的更加高效，管理起来更方便）**\n**_primary_term：文档所在位置（相当于班级）**\n\n如下修改：\n\n```json\nPUT /test_index/test_type/7?if_seq_no=0&if_primary_term=2\n{\n  \"test_field\":\"test client1\"\n}\n```\n\n返回的结果：\n\n```json\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"7\",\n  \"_version\" : 2,\n  \"result\" : \"updated\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 1,\n  \"_primary_term\" : 2\n}\n```\n\n- 另外一个客户端，尝试基于version=1的数据去进行修改，同样带上version版本号，进行乐观锁的并发控制\n\n```json\nput test_index/test_type/7?if_seq_no=0&if_primary_term=2\n{\n  \"test_field\":\"test client 3 \"\n}\n```\n\n这时候报错就如下：\n\n```\n{\n  \"error\" : {\n    \"root_cause\" : [\n      {\n        \"type\" : \"version_conflict_engine_exception\",\n        \"reason\" : \"[7]: version conflict, required seqNo [0], primary term [2]. current document has seqNo [2] and primary term [2]\",\n        \"index_uuid\" : \"ps1w8fp5RmO4wdaUqLuc1A\",\n        \"shard\" : \"0\",\n        \"index\" : \"test_index\"\n      }\n    ],\n    \"type\" : \"version_conflict_engine_exception\",\n    \"reason\" : \"[7]: version conflict, required seqNo [0], primary term [2]. current document has seqNo [2] and primary term [2]\",\n    \"index_uuid\" : \"ps1w8fp5RmO4wdaUqLuc1A\",\n    \"shard\" : \"0\",\n    \"index\" : \"test_index\"\n  },\n  \"status\" : 409\n}\n```\n\n- 在乐观锁成功阻止并发问题之后，尝试正确的完成更新(控制版本号正确无误)\n\n```json\nput test_index/test_type/7?if_seq_no=2&if_primary_term=2\n{\n  \"test_field\":\"test client 3 \"\n}\n```\n\n# 十八、上机动手实战演练基于external version进行乐观锁并发控制\n\nexternal version\n\n> es提供了一个feature，就是说可以不用它提供的内部verison版本号进行并发控制。举个例子，假如数据在mysq里有一份，然后在应用系统本身就维护了一个版本号，无论是什么自己生成的，还是程序控制的。这个时候，进行乐观锁并发控制的时候，可能并不是想要es内部的_version来进行控制，而是用自己维护的那个version来进行控制。\n\n\n\n- ?version=1\n- ?version=1&version_type=external\n\n\n\n其中version_type=external,唯一的却别在于：\n\n​\t_version：只有当你提供的version与es中的version一模一样的时候，才可以进行修改，只要不一样就不会报错。\n\n​\tversion_type=external的时候，只有当你提供的version比es中的version大的时候，才能完成修改。\n\n\n\n比如es中，version=1；  在修改的时候version=1，才能更新成功。\n\n比如es中，version=1； 在修改的时候version>1&version_type=external，才能成功，比如说?version=2&version_type=external\n\n```json\n#先构造一条数据\nput test_index/test_type/8\n{\n  \"test_field\":\"test test\"\n}\n```\n\n```\n#使用version_type=external\nput test_index/test_type/8?version=2&version_type=external\n{\n  \"test_field\":\"test client 2\"\n}\n```\n\n返回值：\n\n```json\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"8\",\n  \"_version\" : 2,\n  \"result\" : \"updated\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 4,\n  \"_primary_term\" : 2\n}\n```\n\n\n\n# 十九、图解partial update实现原理以及动手实战演练\n\n## 1、什么是partial update？\n\nPUT /index/type/id，创建文档&替换文档，就是一样的语法\n\n一般对应到应用程序中，每次的执行流程基本是这样的：\n\n1. 应用程序先发起一个get请求，获取到document，展示到前台界面，供用户查看和修改。\n2. 用户在前台界面修改数据，发送到后台。\n3. 后台代码，会将用户修改的数据在内存中进行执行，然后封装好修改后的全量数据。\n4. 然后发送put请求，到es中，进行全量替换。\n5. es将老的document标记为deleted，然后重新创建一个新的document。\n\n\n\n**partial update**\n\npost /index/type/id/_update\n\n{\n\n​\t\"doc\": {\n\n​\t\t\"要修改的几个field即可，不需要全量的数据\"\n\n​\t}\n\n}\n\n看起来好像方便了很多，每次只需要传递几个发生修改的field即可，不需要将全量的document数据发送过去。\n\n![image-20220406165443906](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h103r3y8toj21py0r2n24.jpg)\n\n## 2、代码演练\n\n```json\nPOST /test_index/test_type/10/_update\n{\n  \"doc\":{\n    \"test_filed2\":\"test2 copy\"\n  }\n}\n```\n\n上面的代码中是partial update 请求是post，后面添加了_update\n\n# 二十、上机动手实战演练基于groovy脚本进行partial update\n\n## 1、创建数据\n\n```json\nPUT /test_index/test_type/11\n{\n  \"num\":0,\n  \"tags\":[]\n}\n```\n\n## 2、内置脚本\n\n```json\npost /test_index/test_type/11/_update\n{\n  \"script\":\"ctx._source.num+=1\"\n}\n```\n\n结果：\n\n```json\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"11\",\n  \"_version\" : 3,\n  \"_seq_no\" : 7,\n  \"_primary_term\" : 2,\n  \"found\" : true,\n  \"_source\" : {\n    \"test_field\" : \"test groovy\",\n    \"num\" : 1,\n    \"tags\" : [ ]\n  }\n}\n```\n\n## 3、外部脚本\n\n```\nPOST /test_index/test_type/11/_update\n{\n  \"script\":{\n    \"lang\": \"groovy\", \n    \"file\":\"test-add-tags\",\n    \"params\": {\n      \"new_tag\":\"tag1\"\n    }\n  }\n}\n#这样在es7版本中是错误的\n```\n\n[elasticsearch](http://zpycloud.com/archives/tag/elasticsearch/)7.3报错[script] [unknow](http://zpycloud.com/archives/tag/unknow/)n field [[file](http://zpycloud.com/archives/tag/file/)], parser not found\n\n原因：高版本的elasticsearch不支持[file](http://zpycloud.com/archives/tag/file/)类型，也就是说不支持从外部[文件](http://zpycloud.com/archives/tag/文件/)读取[脚本](http://zpycloud.com/archives/tag/脚本/)。\n\n### 1.需要先将脚本[post](http://zpycloud.com/archives/tag/post/)到es的库中\n\n```json\nPOST _scripts/new_tags\n{\n  \"script\":{\n    \"lang\":\"painless\",\n    \"source\": \"ctx._source.tags.add(params.new_tag)\"\n  }\n}\n#返回结果：\n{\n  \"acknowledged\" : true\n}\n\n\n```\n\n现document有一个字段tags是一个[数组](http://zpycloud.com/archives/tag/数组/)['a','b']。\n\n现在要往这个tags里新增一个tag，[脚本](http://zpycloud.com/archives/tag/脚本/)里写的是ctx._source.tags+=params.[new_tag](http://zpycloud.com/archives/tag/new_tag/)。实际上这个写法在5.x里是没有问题的，而高版本里会报错Cannot cast java.lang.String to java.util.[ArrayList](http://zpycloud.com/archives/tag/arraylist/)\n\n正确的写法应该是**ctx._source.tags.[add](http://zpycloud.com/archives/tag/add/)(params.new_tag)**\n\n### 2.根据id来调用\n\n```json\nPOST test_index/product/11/_update\n{\n  \"script\":{\n    \"id\":\"new_tags\",\n    \"params\": {\n      \"new_tag\": \"tag1\"\n    }\n  }\n}\n#返回结果\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"11\",\n  \"_version\" : 4,\n  \"result\" : \"updated\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 8,\n  \"_primary_term\" : 2\n}\n#查看document，多了tags中的tag1：\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"11\",\n  \"_version\" : 4,\n  \"_seq_no\" : 8,\n  \"_primary_term\" : 2,\n  \"found\" : true,\n  \"_source\" : {\n    \"test_field\" : \"test groovy\",\n    \"num\" : 1,\n    \"tags\" : [\n      \"tag1\"\n    ]\n  }\n}\n```\n\n如果不是arraylist的话 就可以是下面这种定义：\n\n```\nPOST _scripts/new_price\n{\n  \"script\":{\n    \"lang\":\"painless\",\n    \"source\": \"ctx._source.price+=params.new_price\"\n  }\n}\n#这样就可以直接定义一个\n{\n  \"script\":{\n    \"id\":\"new_price\",\n    \"params\": {\n      \"new_price\": 22\n    }\n  }\n}\n```\n\n## 4、用脚本来删除文档\n\n### 1、将脚本post到es的库中\n\n```json\nPOST _scripts/delete\n{\n  \"script\":{\n    \"lang\":\"painless\",\n    #错误\"source\": \"ctx.op =ctx._source.num==count?'delete':'none'\"\n    \"source\": \"ctx.op =ctx._source.num==params.count?'delete':'none'\"\n  }\n}\n```\n\n原因是语法改变了，需要用新的语法写，或者是\n\n```json\nPOST /test_index/test_type/11/_update\n{\n \n   \"script\": {\n    \"source\" : \"ctx.op = ctx._source.num == params.count ? 'delete' : 'none'\",\n    \"params\" : {\n        \"count\": 0\n    }\n   }\n}\n#当脚本参数值正确的时候，结果如下：\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"11\",\n  \"_version\" : 2,\n  \"result\" : \"deleted\", #标记为deleted\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 11,\n  \"_primary_term\" : 2\n}\n#当脚本对不上的时候 结果如下：\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"11\",\n  \"_version\" : 1,\n  \"result\" : \"noop\",  #标记为no 操作\n  \"_shards\" : {\n    \"total\" : 0,\n    \"successful\" : 0,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 10,\n  \"_primary_term\" : 2\n}\n\n```\n\n\n\n### 2、根据id来调用\n\n```json\nPOST /test_index/test_type/11/_update\n{\n  \"script\":{\n    \"id\":\"delete\",\n    \"params\": {\n      \"count\":0\n    }\n  }\n}\n```\n\n\n\n## 5、upsert操作\n\n当document不存在的时候 如果执行post update操作\n\n```json\nPOST /test_index/test_type/11/_update\n{\n  \"doc\":{\n    \"num\":1\n  }\n}\n```\n\n结果如下：\n\n```json\n{\n  \"error\" : {\n    \"root_cause\" : [\n      {\n        \"type\" : \"document_missing_exception\",\n        \"reason\" : \"[test_type][11]: document missing\",\n        \"index_uuid\" : \"ps1w8fp5RmO4wdaUqLuc1A\",\n        \"shard\" : \"2\",\n        \"index\" : \"test_index\"\n      }\n    ],\n    \"type\" : \"document_missing_exception\",\n    \"reason\" : \"[test_type][11]: document missing\",\n    \"index_uuid\" : \"ps1w8fp5RmO4wdaUqLuc1A\",\n    \"shard\" : \"2\",\n    \"index\" : \"test_index\"\n  },\n  \"status\" : 404\n```\n\n执行：\n\n```json\nPOST /test_index/test_type/11/_update\n{\n  \"script\":\"ctx._source.num+=1\",\n  \"upsert\":{\n    \"num\":0,\n    \"tags\":[]\n  }\n}\n```\n\n结果如下：\n\n```json\n{\n  \"_index\" : \"test_index\",\n  \"_type\" : \"test_type\",\n  \"_id\" : \"11\",\n  \"_version\" : 1,\n  \"_seq_no\" : 12,\n  \"_primary_term\" : 2,\n  \"found\" : true,\n  \"_source\" : {\n    \"num\" : 0,\n    \"tags\" : [ ]\n  }\n}\n```\n\n**如果指定的document不存在，就执行upsert中的初始化操作；如果指定的documeng存在，就执行doc或者script指定的partial update操作**\n\n再次执行上面的post 请求：\n\n```json\nPOST /test_index/test_type/11/_update\n{\n  \"script\":\"ctx._source.num+=1\",\n  \"upsert\":{\n    \"num\":0,\n    \"tags\":[]\n  }\n}\n```\n\n结果中num就变成了1，因为已经有document存在了\n\n# 二十一、图解partial update内置乐观锁并发控制原理及相关操作\n\n![image-20220407005044569](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h10hictuw8j21cs0q8n05.jpg)\n\n# 二十二、上机动手实战演练之mget批量查询api\n\n## 1、批量查询的好处：\n\n​\t就是一条条的查询，比如要查询100条数据，那么就要发送100次网络请求，这个开销还是很大的。如果进行批量查询的话，查询100条数据，就只要发送1次网络请求，网络请求的性能开销缩减100倍。\n\n## 2、mget的语法\n\n一条一条的查询\n\nGET /test_index/test_type/1\n\nGET /test_index/test_type/2\n\n\n\n```json\nGET _mget\n{\n  \"docs\":[\n    {\n      \"_index\":\"test_index\",\n      \"_type\":\"test_type\",\n      \"_id\":11\n    },\n   {\n    \"_index\":\"test_index\",\n    \"_type\":\"test_type\",\n    \"_id\":2\n   }\n  ]\n}\n```\n\n结果：\n\n```json\n{\n  \"docs\" : [\n    {\n      \"_index\" : \"test_index\",\n      \"_type\" : \"test_type\",\n      \"_id\" : \"11\",\n      \"_version\" : 2,\n      \"_seq_no\" : 13,\n      \"_primary_term\" : 2,\n      \"found\" : true,\n      \"_source\" : {\n        \"num\" : 1,\n        \"tags\" : [ ]\n      }\n    },\n    {\n      \"_index\" : \"test_index\",\n      \"_type\" : \"test_type\",\n      \"_id\" : \"2\",\n      \"_version\" : 1,\n      \"_seq_no\" : 0,\n      \"_primary_term\" : 1,\n      \"found\" : true,\n      \"_source\" : {\n        \"test_content\" : \"my test\"\n      }\n    }\n  ]\n```\n\n## 3、如果查询的是一个\\_index下不同\\_type的话可以使用以下的语法\n\n```json\nGET /test_index/_mget\n{\n  \"docs\":[\n    {\n      \"_type\":\"test_type\",\n      \"_id\":11\n    },\n   {\n    \"_type\":\"test_type\",\n    \"_id\":2\n   }\n  ]\n}\n```\n\n## 4、如果查询的是一个\\_index下相同\\_type的可以使用以下的语法：\n\n```json\nGET /test_index/test_type/_mget\n{\n \"ids\":[11,2]\n}\n#或者\nGET /test_index/test_type/_mget\n{\n  \"docs\":[\n    {\n      \"_id\":11\n    },\n   {\n      \"_id\":2\n   }\n  ]\n}\n```\n\n## 5、mget的重要性\n\n可以说mget是很重要的，一般来说，进行查询的时候，如果一次性查询多条数据，一定要用batch操作的api，尽可能的减少网络开销次数，可能可以将性能提升数倍。甚至数十倍。\n","tags":["大数据","elasticSearch"],"categories":["AI"]},{"title":"【算法】实现复杂数据源中小区信息的准确归一化","url":"/2022/12/12/实现复杂数据源中小区信息的准确归一化/","content":"\n\n\n# 实现复杂数据源中小区信息的准确归一化\n\n## 背景\n\n小区是租房业务中很重要的信息，它能够反映房源的位置和品质。对租客而言，能否浏览到准确的小区信息是高效找房的关键。因此，收集和展示准确的小区信息是提高用户找房效率的重要方面。为了获得全面的小区信息，租房业务通常会依赖多种数据源获得小区数据，这些数据格式不一，信息杂乱无章，含有很多冗余信息。为了提高找房效率，必须把同一个小区的不同数据聚合到一起并理清小区信息之间的从属关系。本文抓住小区的独有特征并利用相似度算法，设计了一种基于文本匹配的方法来解决这个问题。\n\n## 目标\n\n现有的小区数据中重复小区很多，比如“福鼎家园”、“福鼎家园晓风苑”、“福鼎家园2幢3单元”、“西溪北苑西区”和“西溪北苑东区”等等。这些小区名虽然不完全一样，但是其中一些表示的同一个小区或者同一小区的子小区，我们把这些小区名叫做同义小区，比如“福鼎家园”、“福鼎家园晓风苑”、“福鼎家园2幢3单元”。表示整个小区的是父小区，比如福鼎家园、西溪北苑。表示小区下面的部分区域的叫做子小区，比如福鼎家园晓风苑、西溪北苑东区。而像“福鼎家园2幢3单元”这样的小区地址，称为楼栋地址。\n\n为了房源信息搜索和展示的准确与效率，我们需要把解析出每条小区数据对应的小区信息，以及小区的层级关系，甚至补充一些小区信息。具体来说，一是把现有小区统一到子小区：子小区是期、区、苑一级，比如‘福鼎家园晓风苑’，‘福鼎家园雨露苑’：1. 子小区是单元、栋、幢的上一级：像单元、栋、幢、xx号楼这样的名称，属于小区楼栋；2. 每个子小区都有唯一的父小区，比如子小区“福鼎家园晓风苑”的父小区是“福鼎家园”；二是能够补充父小区、子小区信息：对于小区库中不存在的父小区或者子小区信息，可以补充进来；\n\n## 思路\n\n小区作为一个独特的地址单位，有下面这些特征： *子小区的父小区通常是多个子小区的最长公共前缀：小区的命名是一个层级结构，同一个父小区的子小区通常具有相同的前缀，这符合人对于位置的命名习惯；* 子小区和小区楼栋的名称有独特的特征：比如子小区大多符合这样的模式：“PP[ww|xx|yy|zz]区”，“PP[ww|xx|yy]期”，“PP[ww|xx|yy]号”以及“PP[ww|xx|yy]座”等等。其中PP是公共前缀，也就是确定的虚拟父小区名，ww表示数字，xx表示代表数字的汉字，yy表示大写或者小写字母，zz表示方位词（比如东、西、南、北、东北、西北等）。而小区楼栋地址通常是下面的形式：“PP[ww|xx|yy]幢”，“PP[ww|xx|yy]楼”，“PP[ww|xx|yy]栋”，“PP[ww|xx|yy]单元”以及“PP[ww|xx|yy]号楼”等等 （PP是子小区名，ww,xx和yy代表的含义与上面一样）； *小区作为一个相对小的地址单位，范围是较小的，而且同一个小区的不同子小区的距离不可能太远；* 同一个小区的不同子小区名称通常非常相似。\n\n基于上述观察，我们提出一种以前缀匹配和文本相似度为基础的小区归一化方案。基本的想法是 1. 使用前缀匹配算法对小区实施初步聚类， 2. 然后通过计算文本相似度附加距离权重做进一步筛选， 3. 最后识别父子小区识别。\n\n## 数据预处理\n\n我们按照市、区、小区名和经纬度信息确定一个小区。所有小区数据存储在一个表plot里：小区id、市、区、小区名称、小区gps、来源source（标记出小区的来源），类型type（0表示父小区，1表示虚拟父小区，2表示子小区，3表示楼栋地址），父小区id。我们需要对原始的小区数据做预处理： *需要对原始数据做数据处理：市区的格式，整理成类似：杭州，余杭区；有些小区gps是非高德gps，需要转换为高德gps* 有些小区数据只有省市街道小区名，没有具体的区域和经纬度信息，需要使用地图提示进行校正，尽量补全区域和经纬度信息 * 小区名里还会夹杂很多标点符号，干扰我们的分析，我们会首先清除掉这些标点，只对中文字符进行匹配分析\n\n## 归一化算法\n\n小区信息归一化流程如下图所示：\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h911au0xnbj30ia0ccgm2.jpg\" alt=\"image-20221212145109222\" style=\"zoom:67%;\" />\n\n\n\n主要的思路是先利用前缀匹配算法匹配小区得到近似小区树，然后在同一个近似小区树里面筛选掉不适合的小区，然后在小区树之间根据相似度算法匹配，接着合并同义小区树，得到最终的归一化小区树，在每一棵小区树中可以使用模式匹配识别父子小区。下面重点介绍前四步。\n\n## 前缀匹配聚集小区\n\n前面提到同一个父小区的大多数子小区都拥有相同的前缀。我们以这个作为切入点来识别近似的小区，具体方法如下： 对于同一城市同一区域里的每一个小区，开始搜索所有以其名称的前两个字开头的所有小区，这些小区拥有公共前缀，称为以该小区为根的近似小区树。找到所有的近似小区树，不断增加前缀的长度，近似小区树不断分裂成更小的树，在合适的时候停止前缀长度的增加，最后得到的每个小区树里面的小区都是近似小区，可以提取出父小区和子小区。但是如何确定前缀长度的最大值呢？分下面几种情况：\n\n1. 如果是能够判断出本小区的名称是子小区或者楼栋的，直接抽取出父小区名，如果没有同名的父小区则新建一个父小区名，然后搜索所有以该父小区名为前缀的小区，构成一棵以父小区为根的近似小区树；判断一个小区名代表的是不是子小区或者小区楼栋的方法是通过前文阐释的小区名进行正则匹配完成。\n2.  如果存在其他小区以本小区为前缀，那么认为本小区是一个父小区，并且把以本小区为前缀的所有小区都聚集在一起。下图展示了1、2两种情况的例子，其中蓝色是父小区，红色是楼栋地址，黄色是子小区：\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h9115sovylj30u011b76q.jpg\" alt=\"image-20221212145319426\" style=\"zoom: 50%;\" />\n\n3. 对于其他的小区，那么前缀长度的确定有以下原则：以该长度为前缀的小区不超过20（除去重复的小区，以及市、区和小区名完全一样的小区）。一般来说一个小区的子小区不会太多，一个父小区的子小区数量超过20个是非常少见的。举例来说，如下图，小区“翡翠城木兰苑”无法被识别为子小区或者楼栋地址，依靠前缀得到了它的近似小区树，树的数量小于20，停止前缀增长。\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h9116oh1apj30p00mo760.jpg\" alt=\"image-20221212145410948\" style=\"zoom:50%;\" />\n\n对于每一个小区都做这样一个操作，就会得到每个小区对应的前缀树，也称为小区树（使用前缀树实现），这个小区称为小区树的根。容易知道一个小区可能在多个小区树中。在这个过程中也能够识别一部分小区楼栋、子小区和父小区。 本质上讲，这一步是文本聚类的过程。如下图所示，文本聚类通常对文本分词，然后使用TF-IDF(Term Frequency-Inverse Document Frequency，词频-逆文本频率)统计出词频、设置词语权重，构建VSM(Vector Space Model，向量空间模型), 为每个文本构建等长向量，最后设置度量标准（欧氏距离、余弦相似度等），使用聚类算法对文本聚类。\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h91178aqg5j30pc0a03z0.jpg\" alt=\"image-20221212145443034\" style=\"zoom:50%;\" />\n\n这种方法不太适合我们的场景：1. 这种方法通常是针对具有多个特征词的文档进行聚类，然而小区名称短，很难提取出有效的特征词；2. 小区名称有一个特征比较明显，即父子小区以及楼栋号的名称按照顺序是有层级关系的，然而这个特征却不能在文本向量化聚类算法中体现并利用；3. 常用的聚类算法如K-Means等选择合适的k值需要技巧和摸索，而我们的方法利用子小区数量不会太多这一点规避了这个问题。\n\n## 非同义小区过滤\n\n上一步中得到的是拥有相同前缀的小区树。依靠前缀，我们圈选的小区是比较多的，很多不属于同一个父小区的小区被圈选到同一棵小区树中了。一般来说，一个小区的不同子小区之间的距离不会太远。因此，我们过滤掉地理位置较远的小区。具体来讲就是，如果近似小区树里面小区A和小区树的根小区的距离大于2km，那么把这个小区从小区树中删除。\n\n## 近似小区再聚集\n\n并不是所有的同义小区都有相同的后缀。所以，我们也通过文本相似度，来补充一些被遗漏的同义小区： 计算近似树前缀的编辑距离和gps距离，对于gps距离小于1km，而且相似度大于2的近似树可以合并为一棵同义小区树，其中相似度的计算如下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h91198gm1kj30om03ojrj.jpg\" alt=\"image-20221212145551158\" style=\"zoom:67%;\" />\n\n其中a和b分别是两棵小区树的根小区的名称，max(a,b)是a和b的长度最大值，LevenshteinDistance(a, b)是编辑距离，similarity(a, b)越大表示a和b越相似。\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h9118y0awuj31280a8t9r.jpg\" alt=\"image-20221212145621821\" style=\"zoom: 50%;\" />\n\n如上图所示，“西溪风情”不是“大华西溪风情”的前缀，所以在第一步的前缀匹配聚集中，没有被加入到“大华西溪风情”的近似小区树。针对这种前缀覆盖不到的情况，我们计算“西溪风情”和“大华西溪风情”的相似度为3，这个相似度表示文本总长度是文本差异的倍数，越大说明相对差别越小。当这个相似度大于2时，我们把两棵相似树合并。\n\n## 归一化\n\n在以每个小区为根，聚集得到相应的小区树之后，小区树之间会有很多重叠。这一步把有交集的小区树合并得到最终的小区归一化结果。合并后的小区树中的小区可以认为是同义的小区，这一步可以说是最终的小区归一化。\n\n## 评估\n\n使用高德地图中准确区分了同义小区的数据以及人工识别，衡量小区归一化算法的准确度。主要从两个方面： - 正误识（false positives): 本来归于父小区的子小区，未能被识别出来； - 负误识（false negatives): 不归某一父小区的子小区，却被误识为这个父小区的子小区；\n数据显示本文介绍的算法正误识比率低于8%，负误识比率低于5%，说明这种归一化方法的准确度有一定保证。\n\n## 总结\n\n本文通过观察小区名称和层级关系的规律，提出了一种使用文本匹配和近似度分析解决小区信息归一化问题的方法。方法实现简单，有较高的准确度，能够快速识别近似的小区，为提高房源搜索的效率和房源发布的准确度提供了基础数据保障。\n","tags":["信息抽取"],"categories":["AI"]},{"title":"基于HanLP的Elasticsearch分词插件","url":"/2022/12/05/基于HanLP的Elasticsearch分词插件/","content":"\n\n\n# 基于HanLP的Elasticsearch分词插件\n\n## 版本对应\n\n| Plugin version | Branch version |\n| -------------- | -------------- |\n| 7.x            | 7.x            |\n| 6.x            | 6.x            |\n\n## 安装步骤\n\n### 1. 下载安装ES对应Plugin Release版本\n\n安装方式：\n\n方式一\n\na. 下载对应的release安装包（https://github.com/KennFalcon/elasticsearch-analysis-hanlp）\n\nb. 执行如下命令安装，其中PATH为插件包绝对路径：\n\n```\n./bin/elasticsearch-plugin install file://${PATH}\n```\n\n方式二\n\na. 使用elasticsearch插件脚本安装command如下：\n\n```\n./bin/elasticsearch-plugin install https://github.com/KennFalcon/elasticsearch-analysis-hanlp/releases/download/v6.5.4/elasticsearch-analysis-hanlp-6.5.4.zip\n```\n\n### 2. 安装数据包\n\nrelease包中存放的为HanLP源码中默认的分词数据，若要下载完整版数据包，请查看[HanLP Release](https://github.com/hankcs/HanLP/releases)。\n\n数据包目录：*ES_HOME*/plugins/analysis-hanlp\n\n**注：因原版数据包自定义词典部分文件名为中文，这里的hanlp.properties中已修改为英文，请对应修改文件名**\n\n### 3. 重启Elasticsearch\n\n**注：上述说明中的ES_HOME为自己的ES安装路径，需要绝对路径**\n\n### 4. 热更新\n\n在本版本中，增加了词典热更新，修改步骤如下：\n\na. 在*ES_HOME*/plugins/analysis-hanlp/data/dictionary/custom目录中新增自定义词典\n\nb. 修改hanlp.properties，修改CustomDictionaryPath，增加自定义词典配置\n\nc. 等待1分钟后，词典自动加载\n\n**注：每个节点都需要做上述更改**\n\n## 提供的分词方式说明\n\nhanlp: hanlp默认分词\n\nhanlp_standard: 标准分词\n\nhanlp_index: 索引分词\n\nhanlp_nlp: NLP分词\n\nhanlp_crf: CRF分词\n\nhanlp_n_short: N-最短路分词\n\nhanlp_dijkstra: 最短路分词\n\nhanlp_speed: 极速词典分词\n\n## 样例\n\n```shell\nPOST http://localhost:9200/twitter2/_analyze\n{\n  \"text\": \"美国阿拉斯加州发生8.0级地震\",\n  \"tokenizer\": \"hanlp\"\n}\n```\n\n```json\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"美国\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 2,\n      \"type\" : \"nsf\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"阿拉斯加州\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 5,\n      \"type\" : \"nsf\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"发生\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 2,\n      \"type\" : \"v\",\n      \"position\" : 2\n    },\n    {\n      \"token\" : \"8.0\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 3,\n      \"type\" : \"m\",\n      \"position\" : 3\n    },\n    {\n      \"token\" : \"级\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 1,\n      \"type\" : \"q\",\n      \"position\" : 4\n    },\n    {\n      \"token\" : \"地震\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 2,\n      \"type\" : \"n\",\n      \"position\" : 5\n    }\n  ]\n}\n```\n\n## 远程词典配置\n\n配置文件为*ES_HOME*/config/analysis-hanlp/hanlp-remote.xml\n\n```xml\n<properties>\n    <comment>HanLP Analyzer 扩展配置</comment>\n\n    <!--用户可以在这里配置远程扩展字典 -->\n    <entry key=\"remote_ext_dict\">words_location</entry>\n\n    <!--用户可以在这里配置远程扩展停止词字典-->\n    <entry key=\"remote_ext_stopwords\">stop_words_location</entry>\n</properties>\n```\n\n### 1. 远程扩展字典\n\n其中words_location为URL或者URL+\" \"+词性，如：\n\n```\n1. http://localhost:8080/mydic\n\n2. http://localhost:8080/mydic nt\n```\n\n第一个样例，是直接配置URL，词典内部每一行代表一个单词，格式遵从[单词] [词性A] [A的频次] [词性B] [B的频次] ... 如果不填词性则表示采用词典的默认词性n。\n\n第二个样例，配置词典URL，同时配置该词典的默认词性nt，当然词典内部同样遵循[单词] [词性A] [A的频次] [词性B] [B的频次] ... 如果不配置词性，则采用默认词性nt。\n\n### 2. 远程扩展停止词字典\n\n其中stop_words_location为URL，如：\n\n```\n1. http://localhost:8080/mystopdic\n```\n\n样例直接配置URL，词典内部每一行代表一个单词，不需要配置词性和频次，换行符用 \\n 即可。\n\n**注意，所有的词典URL是需要满足条件即可完成分词热更新：**\n\n- 该 http 请求需要返回两个头部(header)，一个是 Last-Modified，一个是 ETag，这两者都是字符串类型，只要有一个发生变化，该插件就会去抓取新的分词进而更新词库。\n- 可以配置多个字典路径，中间用英文分号;间隔\n- URL每隔1分钟访问一次\n- 保证词典编码UTF-8\n\n## 自定义分词配置\n\nHanLP在提供了各类分词方式的基础上，也提供了一系列的分词配置，分词插件也提供了相关的分词配置，我们可以在通过如下配置来自定义自己的分词器：\n\n| Config                             | Elastic version    |\n| ---------------------------------- | ------------------ |\n| enable_custom_config               | 是否开启自定义配置 |\n| enable_index_mode                  | 是否是索引分词     |\n| enable_number_quantifier_recognize | 是否识别数字和量词 |\n| enable_custom_dictionary           | 是否加载用户词典   |\n| enable_translated_name_recognize   | 是否识别音译人名   |\n| enable_japanese_name_recognize     | 是否识别日本人名   |\n| enable_organization_recognize      | 是否识别机构       |\n| enable_place_recognize             | 是否识别地名       |\n| enable_name_recognize              | 是否识别中国人名   |\n| enable_traditional_chinese_mode    | 是否开启繁体中文   |\n| enable_stop_dictionary             | 是否启用停用词     |\n| enable_part_of_speech_tagging      | 是否开启词性标注   |\n| enable_remote_dict                 | 是否开启远程词典   |\n| enable_normalization               | 是否执行字符正规化 |\n| enable_offset                      | 是否计算偏移量     |\n\n注意： 如果要采用如上配置配置自定义分词，需要设置enable_custom_config为true\n\n例如：\n\n```shell\nPUT test\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_hanlp_analyzer\": {\n          \"tokenizer\": \"my_hanlp\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_hanlp\": {\n          \"type\": \"hanlp\",\n          \"enable_stop_dictionary\": true,\n          \"enable_custom_config\": true\n        }\n      }\n    }\n  }\n}\n```\n\n```shell\nPOST test/_analyze\n{\n  \"text\": \"美国,|=阿拉斯加州发生8.0级地震\",\n  \"analyzer\": \"my_hanlp_analyzer\"\n}\n```\n\n结果：\n\n```shell\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"美国\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 2,\n      \"type\" : \"nsf\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \",|=\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 3,\n      \"type\" : \"w\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"阿拉斯加州\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 5,\n      \"type\" : \"nsf\",\n      \"position\" : 2\n    },\n    {\n      \"token\" : \"发生\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 2,\n      \"type\" : \"v\",\n      \"position\" : 3\n    },\n    {\n      \"token\" : \"8.0\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 3,\n      \"type\" : \"m\",\n      \"position\" : 4\n    },\n    {\n      \"token\" : \"级\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 1,\n      \"type\" : \"q\",\n      \"position\" : 5\n    },\n    {\n      \"token\" : \"地震\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 2,\n      \"type\" : \"n\",\n      \"position\" : 6\n    }\n  ]\n}\n```\n\n\n\n","tags":["大数据"],"categories":["HanLP"]},{"title":"【场景落地】基于HanLP数据智能在二手车业务场景中的探索与沉淀","url":"/2022/12/05/数据智能在二手车业务场景中的探索与沉淀 - 业务标签挖掘/","content":"\n\n\n# 数据智能在二手车业务场景中的探索与沉淀 - 业务标签挖掘\n\n> **导语**：标签为用户提供了一种新的检索方式，用户和信息通过标签进行关联，信息的标签化、行为的标签化，在提供个性召回能力的同时，也有助于帮我梳理和挖掘业务品类的特征，做相关业务属性的聚合。\n\n## **背景**\n\n当下全连接的信息场景，所有对于有助于连接转化效率提升的内容和考虑尤为重要，这就要求我们在实际业务场景中，结合用户行为的每个关键的行为节点进行深入的分析和思考。通过用户全链路的分析并结合平台用户业务数据的不断沉淀，在不同的行为环节进行辅助和提效，我们将feed（信息流）场景中用户的关键行为分拆为4个阶段：1流量的接入、2检索、3点击、4电话，首先来整体看下，我们在用户的四个关键行为节点都做了哪些内容？（内容摘要参考图1）\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8szdv1xi2j30w30ct76c.jpg)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（图 1 - 关键节点的技术内容摘要)\n\n1. 在流量的接入环节，我们希望具备两种能力：一种是对用户身份的预估与分层，另一种是我们需要对已有历史行为的用户进行相对准确的兴趣描述。用户身份的预估是后续多个业务场景策略干预的基础，兴趣的准确描述是提供准确召回能力的基础。\n2. 在检索的这个环节我们尝试了一种新的方式，对信息（车）进行标签化的表述，信息通过标签聚合，可以帮助提升长尾信息的曝光能力，通过用户的行为我们可以将用户 - 标签 - 信息串联，做到标签的个性化展示，通过持续完善标签的质量（召回能力）、标签 - 车系的关联准确度，进而带来对转化效率提升的正向刺激。\n3. 在点击环节 - feed场景下ctr的提升是各业务的重点，在持续优化基础的体验的同时会加入智能的干预，通用的做法是介入模型的干预，对ctr进行预估，并结合召回模块在下一个阶段做信息的精排，过程中会加入策略、规则及算法的融合，另外也需要考虑的一个问题是ctr导向与商业的收入导向的目标需要结合考虑。\n4. 电话这部分主要是考虑系统的高可用与提升号码资源的使用效率，并降低不同spm（渠道）的接入成本，同时针对不同的数据纬度进行分析。今年围绕电话这个环节我们也做了一些赋能的场景，结合用户的身份预估做智能AxB的绑定，结合用户的身份及历史行为进行模型的训练 - 做cvr拨打的预估，这些在提升号码资源的使用效率&降低串号率方面都获得的不错的效果。\n\n 今天文章整理的是关于第二个行为环节的内容 - 关于信息标签的挖掘。\n\n## **为什么要挖掘业务品类的标签？**\n\n前面提到，在检索的这个环节我们尝试了一种新的方式，对信息（车）进行标签化的表述，信息通过标签聚合，这有别于传统按业务规则固定的检索方式，可以帮助我们提升长尾信息的曝光能力；通过用户的历史行为我们可以将「用户 - 标签 - 信息」串联，做到标签的个性化展示，通过持续完善标签的质量、标签 - 车系的准确度，进而带来对ctr提升的正向刺激。\n\n信息的标签化，在提升信息个性化展示、体验的同时，也有助于帮助我们发掘品类基本检索属性之外的个性内容，如描述感官的、描述价值的、描述体验的，如：“年轻化、耐造、买菜、动力澎湃、外形炫酷”，这也会极大的丰富我们商品特征的表现形式，结合不同人群兴趣、特征的划分，考虑从信息特征的维度将用户的兴趣进行聚合（这个思路在前面关于用户兴趣描述中也提到过），可以做相对精准的信息召回，进而也会带来对ctr提升的正向刺激。\n\n搭建二手车的标签系统需要对业务有相对深入地了解，在前期语料处理、分词部分需要投入一定的人工成本进行标注、分类和挑选，并结合场景实验的反馈进行持续的优化。标签的种类可以按照业务品类的属性进行区分，比如：“描述外观”、“描述配置”、“描述操控”、“描述情感”、或其他的属性词。\n\n二手车业务标签挖掘这部分包含几块主要的内容：语料的获取、分词、车品类分词模型的训练、词向量、及分层的实验并结合场景的反馈提升标签的质量（召回能力、ctr）标签-车系的准确度，分词模型与词向量的计算很大程度上受限与语料的质量，所以在前期对于语料的处理也会投入不小的精力。\n\n## **具体内容部分**\n\n### **1、我们先来整体了解标签与业务场景的结合点：如下图**\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8szebdul1j30w70mo41c.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t（图 2 - 标签与业务场景的结合方式）\n\n场景：\n\n1. 为列表页固定位置提供个性筛选标签区域 - 此区域可以理解为穿插在feed中的一块固定的位置，如图所示该部分标签内容的组成包括：用户的筛选条件、热门标签、个性标签、创意标签等。该场景实验的指标是对标上下车源信息的点击率，目前二手车第一版标签的实验也是在这个场景上进行的，通过持续的迭代观测效果。\n2. 发布场景 - 个性标签选择&单元参数，在规范和格式化的发布场景中，我们希望针对目标车系、车型召回个性的标签，用于车源信息的补充说明，在丰富信息个性展示的同时，这些标签也会作为单元参数，同上面提到的用户兴趣描述-召回的场景进行协同，关注用户在某个业务属性上的兴趣专注度，高效的召回相关的信息。\n3. item信息的标签展示（个性内容）- feed场景中item标签的展示可以体现信息的个性，个性的标签也会match用户的属性，做个性的召回展示。\n4. 优质的标签或无命中的检索词，进行阶段性的汇总，提供给中台检索部门支持搜索框检索，这部分是对高频、业务特殊属性词、无命中检索能力的一个补充和支持。\n\n### **2、处理阶段（语料的处理 -> 词向量 -> 关联关系 -> 实验）：**\n\n1. 语料的处理 - 标签工程对抓取的语料做分词处理，分词的过程也是训练车品类分词模型的过程（需要先期人工的标注），对分词的结果进行人工的筛选，选定备用的标签，存储标签库，关于标签库的设计在后续内容中进行说明。\n2. 词向量 - 对标签做词向量的计算，这有助于帮助我们扩展标签的召回能力（这个地方我们起初犯过一个错误，我们过分依赖词向量对召回的扩展能力，但不知标签词向量的计算是依赖语料的，我们先期选定的语料是某口碑的数据，语料本身的形式、内容会有很多的局限性，这就造成计算的词向量并不具备广泛认知的代表性，也就造成了最初线上标签召回能力差和准确度低的问题）。\n3. 关联关系 - 前面的内容已经提到用户兴趣的描述，在标签的召回环节也会依赖用户的兴趣基础做标签子集的召回，信息 - 标签 - 人存在稳定的映射关系，做到标签的个性展示。\n4. 实验 - 第四部分是关于实验的对照，标签工程的两个核心的指标是：1 标签的召回能力 2 标签到车系的对应关系准确度，所以通过线上用户在标签点击和信息点击环节的数据回溯，可以帮助我们去观测这批选定标签的质量，这也是实验的重点。\n\n### **3、二手车标签工程详细流程展开如下：**\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8szf191qrj30ty0cxmzu.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t（图 3 - 二手车标签工程整体流程）\n\n流程描述：\n\n阶段1 - 语料的收集\n\n语料的收集可以有多种渠道，我们可以挖掘站内车源信息本身的内容、可以从车相关的新闻/媒体文章中截取相关的内容、可以统计站内搜索的内容/热词、也可以收集公开的口碑/评价信息进行标签的提取，受限于资源和精力的投入，目前我们通过最后一种方式进行语料的收集和完善（这边也会涉及到一些爬虫性能的优化，不做展开）。\n\n阶段2 - 分词\n\n分词是提取标签的过程，是NLP的一个环节，起初关于分词工具的选择，我们对标了jieba、HanLP和LTP，针对相同的语料分词系效果基本一致，受限于商业应用的限制及开发语言的支持，最终考虑使用HanLP作为我们的分词工具，分词的结果经过规则的过滤产出粗选词，这边是人工标注的环节，将业务类的词组进行标注，避免分词过度，也会摒弃问题语料或干扰词，处理后的语料会放回到模型中继续参与分词，分词的过程本身也是训练车品类分词模型的过程。\n\n关于词向量的计算前面也稍微提了一下，词向量的计算结果很大程度上受限于语料的来源，如word2Vec会通过固定窗口长度的上下文预测词条的概率，计算词向量，相同的词条在媒体类文章和口碑类评论两种语料中计算出的词向量是差别很大的，所以后面我们也弱化了通过词向量计算扩展标签的召回能力这种方式，并加入一定的人力成本去做词条的整理和归类（先验知识）。\n\n阶段3 - 标签库\n\n标签库的内容是个持续更新和完善的过程，接收分词的结果，并针对备选词作人工的筛选与分类，去掉无用的词条、错误的词条。\n\n我们会将不同品类的词条进行划分，相近含有的词条会参考线上实验的效果（标签ctr）做取舍，如 “动力嗷嗷 0.00048、动力澎湃 0.00167、动力强劲 0.001164”，参考ctr最终会保留词条“动力澎湃”。\n\n我们也会去掉一些ctr很高但词意表述或者召回含义并不清晰的词条，如“小轿车、颜值、稳重”这类的词条，这些词条不利于我们后续从标签到车系准确关系的建立，词条召回的含义过于发散，相对优质的标签是可以准确表述一类或几类商品，并且这种表述符合大多数人的认知。\n\n从标签的质量角度考虑，我们也尽量不会选择2个字词条，主要以4个字及以上的词条为主，如“动力澎湃、外观拉风、城市穿梭利器、路感清晰、省心省油”等。\n\n标签的筛选需要4个阶段：1 从标签的个性代表度进行筛选、2 从标签到信息的召回能力进行筛选、3 对标签按属性进行分类，比如我们依赖这些词条的描述内容进行划分：描述驾驶体验、感官、配置、商品特征等、4 最后阶段做标签的检查处理。\n\n总体来说，标签库的工作需要对业务有相对深入的理解，带有业务特性的先验知识和决策在这个环节中起到了非常关键的作用。\n\n阶段4 - 场景的反馈\n\n在标签的召回环节，我们同之前第一部分用户兴趣的描述-模型相结合，依托中台的日晷平台进行多组的线上实验，日晷系统支持多层的多策略的实验组合，与工程的接入有良好的切合，配置成本也相对不高。实验的结果结合曝光和用户的点击行为对不同标签的线上效果进行反馈，主要是围绕标签的召回能力和准确率进行的，这个环节的反馈会帮助我们不断迭代标签库内容，在项目较成熟后期，我们也会考虑将这个环节开放产品配置的模块。\n\n### **4、分词效果的迭代**\n\n关于分词部分，大体经历了三个阶段的迭代「语料 - 宝马品牌业务口碑数据」，如下图4描述：\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t086ucqlj30wm0lrtbv.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（图 4 - 分词的迭代）\n\n第一版的分词实验是按词性进行的，观测结果并不理想，分词效果如下：\n\n「颜/ 值/ ，/ 还是/ 颜/ 值/ 。/ 新/ X3/ 的/ 前/ 脸/ 很/ 凌厉/ ，/ 侧面/ 比例/ 和/ 腰/ 线/ 也/ 不错/ 。/ 操控/ 让/ 人/ 极其/ 满意/ ，/ 尤其/ 高速/ 时/ 的/ 表现/ ，/ 简直/ 就是/ 越/ 快/ 越/ 稳/ ，/ 开/ 宝马/ 看来/ 的确/ 不是/ 吹/ 的/ 。」\n\n已有的分词器是单纯按照词性进行分词的，接下来，我们需要针对不同词性的组合来进行第二版的分词。\n\n第二版的分词引入了结构化感知机标注框架进行辅助分词，该框架是通过感知机做序列标注任务，配置起来也相对简单和实用，针对类似语料分词的效果如：\n\n「颜值/nz ，/w 还是/c 颜值/nz 。/w 新/a X3/n 的/u 前脸/n 很/d 凌厉/a ，/w 侧面/f 比例/n 和/c 腰线/n 也/d 不错/a 。/w 操控/v 让/v 人/n 极其/d 满意/v ，/w 尤其/d 高速/d 时/Ng 的/u 表现/vn ，/w 简直/d 就/d 是/v 越/d 快/a 越/d 稳/a ，/w 开宝/v 马看来/nr 的确/d 不/d 是/v 吹/v 的/u 。/w」。\n\n从分词的效果来看，如“新X3”、“开宝马”、“高速时”等这类的领域词汇没有被标注出来，从而认为降低了分词的效果。\n\n分词效果的提升是个不断迭代的过程，在标准模型的基础上加大人工标注的投入，分阶段筛选出可用词条，做有放回的语料补充，或通过词性做词组的扩展。在hanlp的基础上我们新加了chelib（车型库）中的品牌车系和汽车的专用词库，并对不影响句子整体表达的副词、连词等词性的分词做了过滤，同时也增加了对负面词语的判定逻辑，在分词质量上有了较大的提升。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8szif2llmj30tv072tbc.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（图 5 - 业务品类的标签词）\n\n### **5. 关于分词与工程部分的结合流程，如下图6：**\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8szj66tjgj30ma0lc0uy.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（图 6 - 与工程部分的结合的流程）\n\n说明：\n\n1. 基础的语料经过格式的规整存储mysql，规整后的语料数据包含：id、time、city、品牌、车系、内容。\n2. 通过人工标注的方式将业务品类相对固定的词条生成字典，提供hanlp初始化。\n3. 经过初步的分词后的结果产出10w量级的词条，经过去重、过滤及业务侧个性的筛选规则要求，有效分词为5.7w。\n4. 通过词性的标注和词组的扩展流程，产出可用词组，针对这些词组做业务相关性的分析和筛选，并针对筛选后的结果集做聚合，这部分可以认定为基础标签，基础标签结合先线上实验效果的反馈最终会进行评估，筛选稳定优质的线上使用标签。\n5. 配合阶段4也会做词向量的计算，产出词向量矩阵，扩展标签的召回能力，并加入一定量的人工成本进行干预。\n\n### **6、关于词向量部分**\n\n关于词向量：\n\n1. 词向量描述了目标词组在固定语料基础集上训练的空间位置（多维空间）。\n2. 可以通过欧式或cos计算标签之间的空间距离，帮助我们找到距离相近的标签。\n3. 距离相近的标签一定程度上可以帮助我们扩展标签词的信息召回能力，提升召回率，并在信息返回上提供了一定新颖信息的探索能力。\n4. 目前我们采用word2vec进行词向量的计算，word2vec是目前比较普遍的词向量算法，word2vector 输出的结果可以理解为当前的词在固定语料中的空间位置，很多人都有一些了解，它包含CBOW/Skip-gram两种训练模式，是一个结构简单的神经网络，通过上下文预测当前词、或者通过当前词预测上下文，感兴趣的可以参考下word2vector的原理、训练过程。\n\n【词向量的生成过程】：\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8szsjism3j30vo05jdga.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（图 7 - 词向量的生成）\n\n前面提到词向量可以理解为：目标词条在当前语料中的空间位置，以“我”字为例在我们语料中训练后100维的词向量可表示为如下：\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t08w1cw8j30sj03mjte.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（图 8）训练后的词向量\n\n通过词向量扩展标签的召回能力：\n\n通过对已选标签词向量的计算，生成词向量矩阵，这可以帮助我们发现当前语料基础上的相似标签，以扩展标签的召回能力，如下图9：标签3与标签2、标签4词向量接近，那么我们可以认识车型2、车型3、车型6也在标签3的召回能力中，当然这个过程也需要介入一定量的人力查验的工作，check相关车系、车型的符合度，进而调整标签同车、车型的映射关系。\n\n![image-20221205160828746](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8szzuk0yjj30py08rjs9.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（图 9 - 标签相似性的扩展）\n\n### **7、标签库**\n\n标签库的数据层级的设计对应了二手车的车型库的基本层级结构，映射关系分了3层：品牌 → 车系 → 车型，从数据的层级结构来看，标签依次对应了品牌、车系、车系，可以按不同的维度层级进行聚合，生成相关的检索条件，从标签数据的存储格式：ID | name | 车型ids | 车系ids | 品牌ids 可以发现标签的召回顺序依次是车型、车系、品牌，在后来的实际应用中，可以认定准确描述车型级别的标签很少，并且不具备良好的召回能力和泛化能力，所有我们目前考虑的召回层级只到车系这一级别。如图10:\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t00i3bdaj30vj0aimyv.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（图 10 - 标签库的层级关系）\n\n标签库本身是个不断迭代和自我学习的过程，清晰的层级映射服务于线上的召回场景，并结合线上效果的反馈提升和优化标签子集，主要参考的指标仍然是召回能力和准确率，本身用户的行为就是一个很好的反馈，所以标签库的实验也是一个ctr和质量提升相互促进的过程。另外目前二手车标签挖掘的内容相对比较基础，初步搭建工程的同时实现了关于上面全流程的几个核心的节点，但可参考的指标还没有明显的提升，后续如果资源允许，我们也考虑加大这部分的投入，将这部分配置工具化，提供给产品同学可操作与观察的配置平台，这这部分实验的工作快速有效的流程化进行。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t027w1j5j30u608raar.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（图 11 - 标签库的更新）\n\n### **8、从场景、标签服务、用户模型三个模块来简单说明目前我们线上标签召回的时序：**\n\n用户的基础行为topN会映射为标签，通过相似度扩展标签的范围，这样我们将人和info通过标签建立关联，标签映射ses查询条件，直接用于相关info的召回，召回时序如下图12：\n\n![image-20221205161129149](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t02y6qsmj30v60q1tas.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（图 12 - 标签召回的时序）\n\n说明：\n\n1. 流量入口我们选取的是二手车m页的第二页曝光内容，在feed流中插入实验的标签区域，该区域的曝光会携带用户信息（cookie、设备信息等）首先从标签服务拉取标签信息，用户的身份信息会结合用户的模型进行兴趣召回，这个点是同之前我们的说到的用户兴趣描述相结合的，基础用户数据是IDMapping与相关设备信息、cookie、tel的映射，保存了IDMapping到用户兴趣的模型，标签服务通过用户兴趣描述的topN（检索条件）反向查询标签，作为标签的召回子集反馈给场景。针对无行为用户（新客）或行为较少的用户，就需要考虑标签的补足，补足的标签主要是从按地域热门标签中抽取。\n2. 标签曝光后，用户通过标签的点击做车源信息的召回，召回的顺序是从车型 → 车系 → 品牌，这边的信息召回也会结合广告贴策略有所侧重，召回能力不足时，同样需要考虑补足的能力，比如通过词向量召回相似或热门召回更多的信息，相同属性及语意相似的标签，我们也会保存关联的关系，如：动力澎湃 - 动力强劲、外观拉风 - 外观拉轰、外形威猛 - 肌肉车，后者虽然不在线上召回的子集，但却可以帮助提升召回的能力。\n3. 车源信息的曝光结合用户的点击行为作为我们回溯模型的数据进行记录和分析。\n\n基于场景的反馈始终也是以提升ctr为核心的，所以标签应用部分提高标签的质量和召回能力本质上就是提高对ctr的转化效率，目前我们正在做的手段：1 选取优质标签子集 2 人工干预和调整，分析看标签召回能力分布 3 结合用户特征做标签的展示策略（历史查询行为 形容词 高热词 相关高匹配次 探索词） 4 监控效果行程反馈-快速实验 5 提高位置争取资源。\n\n并且在后面的实验场景中，我们也需要建立从：1 标签召回 → 2标签点击 → 3 信息检索 → 4 信息点击 → 5 电话转化完整链路的数据分析能力。\n\n标签是2019年团队新启动的技术项目，目前不是特别的成熟，从标签的召回到信息的曝光到ctr提升的指向完整链路的数据在后面会和大家分享。\n\n## **总结分析与设计**\n\n整体来看，标签的挖掘为用户提供了一种新的检索方式，用户和车源通过标签进行关联，这有别于传统固定规则的检索方式，信息的标签化、行为的标签化，在提供个性召回能力的同时，也有助于帮我梳理和挖掘业务品类的特征，这些内容和后面的业务品类的「特征工程」相辅相成。\n\n至此关于《数据智能在二手车业务场景中的探索与沉淀 - Part2 关于业务标签挖掘》部分的内容整理结束，在后面的分享中机会针对第三个关键行为节点“用户的点击”进行技术内容的整理与分享，涉及的技术内容：业务品类特征工程的搭建、排序、相关算法的应用（lr、XGBOOST、FM、DeepFM、GBDT等）。\n\n## **作者简介**\n\n李晓东 - 2015年1月加入58集团二手车技术团队，现任二手车技术部高级技术经理，负责二手车商业技术团队、拍卖技术团队，开发及管理工作。\n\n穆文斌 - 58同城ABG资深研发工程师，负责二手车电话、声纹、NLP等技术在二手车业务中的实践与应用。","tags":["大数据"],"categories":["AI"]},{"title":"HanLP分词工具应用案例：商品图自动推荐功能的应用","url":"/2022/09/26/HanLP分词工具应用案例：商品图自动推荐功能的应用/","content":"\n\n\n## HanLP分词工具应用案例：商品图自动推荐功能的应用\n\n> 本篇分享一个hanlp分词工具应用的案例，简单来说就是做一图库，让商家轻松方便的配置商品的图片，最好是可以一键完成配置的。\n\n先看一下效果图吧：\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t1rd0ji2g30bk0kh7dt.gif)\n\n商品单个推荐效果：匹配度高的放在最前面\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t1sat0r0g30bk0khwjl.gif)\n\n这个想法很好，那怎么实现呢？分析了一下解决方案步骤：\n\n1. 图库建设：至少要有图片吧，图片肯定要有关联的商品名称、商品类别、商品规格、关键字等信息。\n2. 商品分词算法：由于商品名称是商家自己设置的，不是规范的，所以不可能完全匹配，要有好的分词库来找出关键字。还有一点，分词库要能够自定义词库，最好能动态添加。如果读者不知道什么是分词，请自行百度，本文不普及这个。\n3. 推荐匹配度算法：肯定要最匹配的放在前面，而且要有匹配度分数。商家肯定有图库没有的商品，自动匹配的时候，不能随便配置不相关的图片。\n\n先说明一下，本文企业没有搜索引擎之类的工具，所以本质就靠的是数据库检索。\n\n首页让我们先分析一下图库，下面是图库的设置界面。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t1suxy7qj30v90ksmyt.jpg)\n\n让我们先贴一下图库的表结构：\n\n```sql\nCREATE TABLE `wj_tbl_gallery` (\n\n  `gallery_id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键',\n\n  `fileid` int(11) NOT NULL COMMENT '文件服务器上的文件ID',\n\n  `ptype` tinyint(4) NOT NULL DEFAULT '0' COMMENT '图片类型，0 点歌屏点餐图片',\n\n  `materialsort` varchar(50) DEFAULT NULL COMMENT '商品分类',\n\n  `materialbrand` varchar(50) DEFAULT NULL COMMENT '商品品牌',\n\n  `materialname` varchar(100) NOT NULL COMMENT '商品名称',\n\n  `material_spec` varchar(50) DEFAULT NULL COMMENT '商品规格',\n\n  `material_allname` varchar(200) DEFAULT NULL COMMENT '商品完整名称',\n\n  `status` tinyint(4) NOT NULL DEFAULT '0' COMMENT '状态，0正常，1停用，2删除',\n\n  `updatedatetime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',\n\n  `keyword` varchar(200) DEFAULT NULL COMMENT '商品关键字，用逗号隔开',\n\n  `bstorage` tinyint(4) NOT NULL DEFAULT '0' COMMENT '关键字是否入库 0没有，1有',\n\n  PRIMARY KEY (`gallery_id`),\n\n  KEY `idx_fileid` (`fileid`)\n\n) ENGINE=InnoDB AUTO_INCREMENT=435 DEFAULT CHARSET=utf8 COMMENT='图库信息表';\n```\n\n**数据示例：**\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t1tj2g8kj30v90120su.jpg)\n\n简单说一下material_allname是干什么用的呢，主要就是拼接商品名称、规则 、关键字字段。用来写sql的时候比较方便。关键字字段是干什么用的呢，作用有两个。1是商品可能有多个名字，补充名称的。二是给分词库动态添加词库。图库简单说到这。\n\n再说一下分词库，笔者选择的是开源的汉语言分词库-hanlp分词工具\n\n优点是词库大，有词性分析，可以自定义词库。缺点当然也有，就是不支持数据库方法动态读取词库。后面说一下我自己的解决办法。\n\n上代码：\n\n分词代码,这其中去掉一些没用字符。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t1uip0drj30ow1ppn0l.jpg)\n\n我们分词，就是调用SegmentUtils.segmentTerm(materialname);\n\n动态添加词库方法：\n\n```java\nprivate void addCustomerDictory(){\n\n        Integer max = galleryRepository.getMaxGallery();\n\n        if(CommonUtils.isNotEmpty(max) && max > 0 && max > SegmentUtils.CACHE_GALLERY_ID){\n\n            int oldid = SegmentUtils.CACHE_GALLERY_ID;\n\n            SegmentUtils.CACHE_GALLERY_ID = max;\n\n            Listgallery = galleryRepository.getGallery(oldid,max);\n\n            if(CommonUtils.isNotEmpty(gallery)){\n\n                MapdicMap = new HashMap<>();\n\n                for(String w : gallery){\n\n                    if(CommonUtils.isNotEmpty(w)){\n\n                        String[] array = w.split(\",\");\n\n                        if(CommonUtils.isNotEmpty(array)){\n\n                            for(String item : array){\n\n                                String value = item.trim();\n\n                                if(CommonUtils.isNotEmpty(value)){\n\n                                    dicMap.put(value, true);\n\n                                }\n\n                            }\n\n                        }\n\n                    }\n\n                }\n\n                Setkeys = dicMap.keySet();\n\n                if(CommonUtils.isNotEmpty(keys)){\n\n                    SegmentUtils.insertCustomDictory(keys);\n\n                }\n\n            }\n\n        }\n\n    }\n\n    /**\n\n     * 获取关键字\n\n     *\n\n     * @author deng\n\n     * @date 2019年3月13日\n\n     * @param galleryId\n\n     * @return\n\n     */\n\n    @Query(\"select keyword from Gallery a where galleryId > ?1 and galleryId<=?2 and a.keyword !='' and bstorage=0\")\n\n    public ListgetGallery(int bgalleryId, int egalleryId);\n\n    @Cacheable(value = CacheConstants.CACHE_GALLERY, keyGenerator = CacheConstants.KEY_GENERATOR_METHOD)\n\n    @Query(value = \"select gallery_id from wj_tbl_gallery a where a.keyword !='' and bstorage=0 order by gallery_id desc limit 1\", nativeQuery = true)\n\n    public Integer getMaxGallery();\n```\n\n​\t说一下解决思路，由于HanLP文档上没有看到从mysql上动态添加词库方法，只有CustomDictionary.insert能动态添加单个实例词库，系统如果重启，就要重新添加。我就想出一个办法，就是分词的时候，查一下类的保存的最大图库表的主键是什么，如果跟数据库一样，就不动态添加。如果小于图库的主键，就把没有的那一段用CustomDictionary.insert添加进去。系统一般不重启，如果重启就在分词的时候重新添加一下。查询数据库当然都有缓存，编辑图库的时候，把对应缓存清除一下。这种方式也能支持分布式环境，多个实例都是一样处理的。每过一段时间，就把图库表的关键字词库搞成文件的词库，避免动态添加太多，占用太多内存。自定义词库其实是很重要的，任何分词库都不可能包含所有的词库，而分词算法是根据词库来展开的，可以说词库决定了分词结果的准确性。\n\n让我们看一下分词的效果:\n\n**商品名称为”雪碧（大）“的分词结果 雪碧/nz, 大/a ,其中nz表示专有词汇，a表示形容词。**\n\n再看一下不理想的分词结果：\n\n**商品品名称:”蕾芙曼金棕色啤酒“,类别名称:啤酒,**\n\n**分词结果:蕾/ng,芙/n,曼/ag,金/ng,棕色/n,啤酒/nz**\n\n很明显，分词结果不理想，蕾芙曼金棕色其实是一个商品名，不能分开。怎么办呢，这时候动态添加词汇功能就派上用场了。\n\n再图库关键字时差添加蕾芙曼金棕色啤酒，保存一下，再看一下分词效果：\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t1w6wmmvj30bv0d10t2.jpg)\n\n物品名称:蕾芙曼金棕色啤酒,类别名称:啤酒,分词结果:蕾芙曼金棕色/nz,啤酒/nz\n\n蕾芙曼金棕色被分到了一起，达到预期效果，这其实就是 CustomDictionary.insert(data, \"nz 1024\");再起作用。hanlp具体API功能，请参考官方文档，本文就不介绍了。\n\n最后重头戏来了，商品图片匹配度分析。作者就是采用了mysql的sql词句的方法搞定了，其实就用到了LOCATE函数，很简单。SQL示例如下:\n\n```sql\nSELECT gallery_id, fileid, materialname, material_allname, score\n\n, ROUND(score / 4 * 100, 0) AS rate\n\nFROM (\n\nSELECT a.gallery_id, a.fileid, materialname, material_allname\n\n, IF(LOCATE('雪碧', a.material_allname), 2, 0) + IF(LOCATE('大', a.material_allname), 1, 0) + IF(LOCATE('饮料', a.material_allname), 1, 0) AS score\n\nFROM wj_tbl_gallery a\n\nWHERE a.STATUS = 0\n\nAND (a.material_allname LIKE '%雪碧%'\n\nOR a.material_allname LIKE '%大%'\n\nOR a.material_allname LIKE '%饮料%')\n\n) b\n\nORDER BY score DESC, materialname\n\nLIMIT 0, 8\n```\n\n执行结果：\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8t1wycbjuj30gt03laa9.jpg)\n\n可以看出gallery_id是第一条，它的rate的是75，满分是100，匹配度蛮高的。\n\n说一下匹配度算法原则，如果完全匹配就是1百分，肯定就上了。然后去除某些关键字后，也匹配上了就是90分。最后采用分词算法，按照1百分打分，其中如果高于50分，可以算基本匹配，自动配置图片的时候，就可以当成匹配成功。总体原则就是匹配词汇越多，分数越多。但是两个字的词汇，和5个字的词汇，分数是不一样的。还有词性，专属词汇理论上应该比形容词分数高。详见下面的calculateWeight代码:\n\n```java\npublic List> queryList(String searchstr, int pagenumber, int pagesize, String materialsortname,\n\n            ListsegmentList) {\n\n        String name = \"%\" + searchstr + \"%\";\n\n        // 先简单搜索 ，完全匹配100分\n\n        List> list = queryList(name, pagenumber, pagesize, 100);\n\n        if (CommonUtils.isEmpty(list)) {\n\n            searchstr = searchstr.replaceAll(\"\\\\s\", \"\");\n\n            String regEx = \"(特价)|(/)|(\\\\()|(\\\\))|(（)|(）)|(\\\\d+ml)|(买.送.)|(/)|(\\\\*)\";\n\n            searchstr = searchstr.replaceAll(regEx, \"\");\n\n            if (CommonUtils.isNotEmpty(searchstr)) {\n\n                name = \"%\" + searchstr + \"%\";\n\n                // 简单过滤 90分\n\n                list = queryList(name, pagenumber, pagesize, 90);\n\n            }\n\n            // 剩下分词 靠计算\n\n            if (CommonUtils.isEmpty(list)) {\n\n                if (CommonUtils.isNotEmpty(segmentList)) {\n\n                    list = queryListTerm(pagenumber, pagesize, segmentList, materialsortname);\n\n                }\n\n                // 如果只有分类，先定10分\n\n                else if (CommonUtils.isNotEmpty(materialsortname))\n\n                    list = queryList(materialsortname, pagenumber, pagesize, 10);\n\n            }\n\n        }\n\n        return list;\n\n    }\n\n    private List> queryList(String name, int pagenumber, int pagesize, int rate) {\n\n        String sql = \"SELECT\\n\" + \"  a.gallery_id,\\n\" + \"  a.fileid,a.material_allname,a.materialname \\n, \" + rate\n\n                + \" rate FROM\\n\" + \"  wj_tbl_gallery a\\n\" + \"WHERE\\n\"\n\n                + \"  a.material_allname LIKE :searchstr and a.status = 0 order by length(materialname) LIMIT :pagenumber,:pagesize \";\n\n        Dto param = new BaseDto();\n\n        param.put(\"searchstr\", name).put(\"pagenumber\", pagenumber * pagesize).put(\"pagesize\", pagesize);\n\n        return namedParameterJdbcTemplate.queryForList(sql, param);\n\n    private List> queryListTerm(int pagenumber, int pagesize, ListsegmentList,\n\n            String materialsortname) {\n\n        Dto param = new BaseDto();\n\n        StringBuffer sb = new StringBuffer();\n\n        StringBuffer wsb = new StringBuffer(\" (\");\n\n        // 总权重\n\n        int tw = 0;\n\n        if (CommonUtils.isNotEmpty(segmentList)) {\n\n            for (int i = 0; i < segmentList.size(); i++) {\n\n                String str = segmentList.get(i).word;\n\n                int w = SegmentUtils.calculateWeight(segmentList.get(i));\n\n                str = StringUtils.escapeMysqlSpecialChar(str);\n\n                tw += w;\n\n                sb.append(\"if(LOCATE('\").append(str).append(\"', a.material_allname),\").append(w).append(\",0) \");\n\n                wsb.append(\" a.material_allname like '%\").append(str).append(\"%' \");\n\n                if (i < segmentList.size() - 1) {\n\n                    sb.append(\" + \");\n\n                    wsb.append(\" or \");\n\n                }\n\n            }\n\n            // 类别单独处理，目前权重较低\n\n            // 表示字符串是否为空\n\n            int emptylen = 3;\n\n            if (CommonUtils.isNotEmpty(materialsortname)) {\n\n                if (sb.length() > emptylen) {\n\n                    sb.append(\" + \");\n\n                    wsb.append(\" or \");\n\n                }\n\n                tw += SegmentUtils.DWEIGHT;\n\n                materialsortname = StringUtils.escapeMysqlSpecialChar(materialsortname);\n\n                sb.append(\" if(LOCATE('\").append(materialsortname).append(\"', a.material_allname),\")\n\n                        .append(SegmentUtils.DWEIGHT).append(\",0) \");\n\n                wsb.append(\" a.material_allname like '%\").append(materialsortname)\n\n                        .append(\"%' \");\n\n            }\n\n            if (sb.length() > emptylen) {\n\n                sb.append(\" as score \");\n\n                wsb.append(\") \");\n\n                String scoreSelect = sb.toString();\n\n                String scorewhere = wsb.toString();\n\n                String sql = \"select gallery_id,fileid,materialname,material_allname,score,ROUND(score/\" + tw\n\n                        + \"*100, 0) rate  from  (SELECT \" + \"  a.gallery_id, \"\n\n                        + \"  a.fileid,materialname,material_allname, \" + scoreSelect + \" FROM \"\n\n                        + \"  wj_tbl_gallery a \" + \"WHERE \" + \" a.status = 0 and \" + scorewhere\n\n                        + \" ) b order by score desc ,materialname LIMIT \" + pagenumber * pagesize + \",\" + pagesize;\n\n                param.put(\"pagenumber\", pagenumber * pagesize).put(\"pagesize\", pagesize);\n\n                logger.debug(\"商家搜索图库的SQL语句是{}\", sql);\n\n                List> list = namedParameterJdbcTemplate.queryForList(sql, param);\n\n                if (CommonUtils.isNotEmpty(list)) {\n\n                    return list;\n\n                }\n\n            }\n\n        }\n\n    /**\n\n     * 计算分词权重\n\n     * @author deng\n\n     * @date 2019年6月21日\n\n     * @param term\n\n     * @return\n\n     */\n\n    public static int calculateWeight(Term term) {\n\n        // 汉字数\n\n        int num = countChinese(term.word);\n\n        // 大于3个汉字，权重增加\n\n        int value = num >= 3 ? 2 + (num - 3) / 2 : DWEIGHT;\n\n        // 专属词，如果有两个字至少要最小分是2分\n\n        if (term.nature == Nature.nz && value <= DWEIGHT) {\n\n            value = DWEIGHT + 1;\n\n        }\n\n        return value;\n\n    }\n```\n\n总结一下，本文介绍的商品图片推荐和自动匹配方法，可以看出来是相当简单的，本质就是mysql的like%% 优化来的，依赖sql语句和hanlp分词库，做法简单，但是能满足专门商品的匹配，适合小图库。","tags":["大数据","HanLP","前后端"],"categories":["AI"]},{"title":"自然语言处理BERT模型","url":"/2022/07/19/自然语言处理BERT模型/","content":"\n\n\n# 自然语言处理BERT模型\n\n## 课程安排\n\n- 通俗讲解知识点，项目实战驱动 \n- 当下主流解决框架，一站式搞定NLP任务 \n- 环境配置：选一款IDE即可，基于谷歌开源项目 \n- 提供所有数据与代码，追随热点持续更新\n\n## BERT\n\n- 需要熟悉word2vec，RNN网络模型，了解词向量如何建模\n- 重要在于Transformer网络架构，BERT训练方法，实际应用\n- 开源项目，都是现成的，套用进去就ok\n- 提供预训练模型，基本任务拿过来直接用都成\n\n## Transformer\n\n要做一件什么事呢？\n\n- 基本组成依旧是机器翻译模型中常见的Seq2Seq网络\n- 输入输出都很直观，其核心架构就是中间的网络设计了\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h884nz6dxkj30ng05yaa6.jpg)\n\n\n\n对比传统的RNN网络：\n\n计算时有什么问题？\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h884pbn9wgj315d0dimy5.jpg)\n\n是依次串行处理的，现在处理的需要依赖上一步处理后的数据。\n\n而Self-Attention机制来进行并行计算（得益于QKV的矩阵），在输入和输出都相同，输出结果是同时被计算出来的，现在基本已经取代RNN了。\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h884sbjayej30xm0l40tv.jpg\" alt=\"image-20221117145723293\" style=\"zoom:50%;\" />\n\n传统的word2vec\n\n- 表示向量时有什么问题？ \n\n训练好的词向量是固定的，但是应用到不同的情景中原有的词含义并不合适\n\n- 如果‘干哈那’是一个词？\n\n不同语义中不同的含义\n\n- 不同语境中相同的词该如何表达\n- 预训练好的向量就永久不变了 \n\n\n\n## 整体架构\n\n- 输入如何编码？\n- 输出结果是什么？\n- Attention的目的？\n- 怎样组合在一起？\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8859gwec0j315z0n1djr.jpg)\n\n\n\n### Attention是什么意思呢？\n\n对于输入的数据，所关注的点是什么？不同的句子有不同的关注点。\n\n那么如何才能让计算机关注到这些有用的信息呢？\n\n### Self-attention是什么？\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h885yjha1cj30n103jaaj.jpg)\n\n在上面的句子中，不同的情景下it所代指的是不一样的。\n\nself-attention实质上就是当前词自己self，融合了该词所在句子中其他单词的含义，只不过对不同的词添加了不同的权重。\n\n> 所谓的各种模型，实际上都是在进行加权平均\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h885ued8naj30fj0endge.jpg)\n\n### self-attention如何计算？\n\n考虑如何获得一个句子中不同的单词对该词自身selt的影响，考虑添加一个query、key、value矩阵来，在开始的时候初始化，不断的训练中会改变，其实质上就是一个权重参数矩阵。\n\n1. 输入经过编码后得到向量\n2. 想得到当前词语上下文的关系，可以当作是是加权\n3. 构建三个矩阵分别来查询当前词跟其他词的关系，以及特征向量的表达。\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8862zxd7yj31320ncdgw.jpg\" alt=\"image-20221117154213948\" style=\"zoom:50%;\" />\n\n\n\n三个需要训练的矩阵：\n\nQ：query ，要去查询的\n\nK：key，等着被查询的\n\nV：value，实际的特征信息\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dwrlsu6hj30kb0mj74y.jpg\" alt=\"image-20221122145315126\" style=\"zoom: 50%;\" />\n\n\n\n首先：\n\nq与k的内积表示有多匹配，向量垂直，内积为0、相近的话内积接近于1。\n\n输入了两个向量，得到一个分值。q、k。得到的是两个矩阵查询出来的结果。\n\n其中k代表了等着被查的，v代表了q*k后的实际的特征信息。\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dwwx0j59j30ss0ggq3m.jpg\" alt=\"image-20221122145824269\" style=\"zoom:50%;\" />\n\n\n\n在Q * K转制后也就是得到了一个实际的内积值，因为多个计算结果不同的值之间方差比较大，所以我们去除以了根号下k的维度，然后做了一个softmax，结果可能是：\n\n【0.2、0.12、0.34、0.37 。。】\n\n这实际上对应的是当前词和和不同词之间的相关度（这里使用概率值来表示）然后和V矩阵相乘后得到z矩阵，也就是当前词根据上下文信息后得到的综合向量embedding。\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dx3rw1q2j30ob0c274i.jpg\" alt=\"image-20221122150458880\" style=\"zoom:33%;\" />\n\n最终的得分值经过softmax就是最终上下文结果\n\n除以根号的原因在于，不能让其分值随着向量维度的增大而增加，所以这里做了一个缩小除法。\n\nsoftmax公式回忆：\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dx66pirbj30s609z0tp.jpg\" alt=\"image-20221122150719368\" style=\"zoom:33%;\" />\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dx76lquij308f02o0sm.jpg)\n\n把一些输入映射为0-1之间的实数，并且归一化保证和为1。\n\n\n\n### 每个词的Attention计算\n\n- 每个词的Q会跟整个序列中每一个K计算得分，然后基于得分再分配特征。![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dxb16vpvj30mi09imxm.jpg)\n\n### Attention整体计算流程\n\n- 每个词的Q会跟每一个K计算得分\n- Softmax后（和V矩阵相乘后）就得到整个加权结果\n- 此时每个词看的不只是它前面的序列而是整个输入序列\n- 会在同一时间计算出所有词的表示结果\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dxf3ytzxj30pj0pzt9t.jpg\" alt=\"image-20221122151553690\" style=\"zoom:50%;\" />\n\n首先是输入文本，然后转换成embedding，然后新建Q、K、V三个矩阵并随机初始化，然后q1\\*k1、q2\\*k2等等得到一个相关内积，然后除以k矩阵的维度，再经过softmax后得到概率值，然后和v矩阵相乘，就得到了如：\n\nZ1 = 0.88v1 + 0.12 v2 这样的表示方式。\n\n\n\n### Multi-headed机制\n\n- 一组q、k、v得到了一组当前词的特征表达。\n- 我们设想类似cnn中的filter，能不能提取多种特征呢？\n- 多头注意力机制类似于我们从不同的视角去分析问题，对于selt-attention的信息抽取同样是包含这种思想在里面的。\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dy5np243j30j10cc74m.jpg\" alt=\"image-20221122154124438\" style=\"zoom: 67%;\" />\n\n实质上就是借鉴了：\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dyh4hhm5j30we07y3zv.jpg\" alt=\"image-20221122155225292\" style=\"zoom: 33%;\" />\n\n\n\n通过不同的head得到了多个特征表示 ，然后将所有特征拼接在一起，然后再进经过一层全链接来降维：\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dyijc607j30u00wb0tz.jpg\" alt=\"image-20221122155346655\" style=\"zoom: 33%;\" />\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dyiwz6mmj31yi0oyjuw.jpg)\n\n### Multi-headed结果\n\n> 不同注意力的结果，得到的特征向量表达也不相同。\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dyjyx3pqj30yu0u0afl.jpg\" alt=\"image-20221122155509768\" style=\"zoom: 50%;\" />\n\n### 堆叠多层\n\n一层不够，需要堆叠多层。计算方法都是相同的。\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dym8v26wj30wr0k5gmz.jpg\" alt=\"image-20221122155721316\" style=\"zoom: 50%;\" />\n\n### 位置信息表达\n\n在上文描述的self-attention中每个词都会考虑整个序列的加权，所以其出现位置对结果并不会产生什么影响，相当于放在哪里都是无所谓的，但是这就跟实际有些不相符了，所以希望模型对位置信息有一些额外的认识，于是引入了位置变量。\n\n\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dyqf7xxoj30n0088mxf.jpg)\n\n\n\n在Encoder层和Decoder层中都用到了Add&Norm操作，即残差连接和层归一化操作。\n\n### 什么是残差连接呢？\n\n> 残差连接就是把网络的输入和输出相加，即网络的输出为F(x) + x ，在网络结构比较深的时候，网络梯度反向传播更新参数时，容易造成梯度消失的问题，但是如果每层的输出都加上一个x的时候，就会变成F（x）+x，对x求导结果为1，所以就相当于每一层求导时都加上了一个常数项1，有效的解决了梯度消失的问题。\n>\n> 加入未学习的原向量使得到的结果的效果至少不弱于原来的结果\n\n### Norm操作\n\nNorm操作\n首先要明白Norm做了一件什么事，从刚开始接触Transformer开始，我认为所谓的Norm就是BatchNorm，但是有一天我看到了这篇文章，才明白了Norm是什么。\n\n假设我们输入的词向量的形状是（2，3，4），2为批次（batch），3为句子长度，4为词向量的维度，生成以下数据：\n\n[[w11, w12, w13, w14], [w21, w22, w23, w24], [w31, w32, w33, w34]\n[w41, w42, w43, w44], [w51, w52, w53, w54], [w61, w62, w63, w64]]\n1\n2\n如果是在做BatchNorm（BN）的话，其计算过程如下：BN1=(w11+w12+w13+w14+w41+\nw42+w43+w44)/8，同理会得到BN2和BN3，最终得到[BN1,BN2,BN3] 3个mean\n\n如果是在做LayerNorm（LN）的话，则会进如下计算：LN1=(w11+w12+w13+w14+w21+\nw22+w23+w24+w31+w32+w33+w34)/12，同理会得到LN2，最终得到[LN1,LN2]两个mean\n\n如果是在做InstanceNorm（IN）的话，则会进如下计算：IN1=(w11+w12+w13+w14)/4，同理会得到IN2，IN3，IN4，IN5，IN6，六个mean，[[IN1，IN2，IN3],[IN4，IN5，IN6]]\n下图完美的揭示了，这几种Norm\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8e24bu8drj311t0ahacj.jpg)\n\n1. 通过解决ICS（[内部协变量偏移](http://www.baidu.com/link?url=NzFNnRmBgJpmycgNhEUB2zE2KdI-YD7F5gT7Q-8mKmZvRU9ojcu3ioiHRRRUovkT)）的问题，使得每一层神经网络的输入分布稳定，在这个基础上可以使用较大的学习率，加速了模型的训练速度 \n2. 起到一定的正则作用，进而减少了dropout的使用。当我们通过BN规整数据的分布以后，就可以尽量避免一些极端值造成的overfitting的问题\n3. 使得数据不落入饱和性激活函数（如sigmoid，tanh等）饱和区间，避免梯度消失的问题。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8e2k7r6lsj31q50u0448.jpg)\n\n\n\n\n\n---------\n\n### Decoder\n\nAttention计算不同\n\n并且加入了MASK机制\n\n\n\n### 最终的输出结果\n\n得出最终预测结果\n\n损失函数使用cross-entropy即可\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8e2z0jgvtj30hb0c3dg7.jpg\" alt=\"image-20221122182800177\" style=\"zoom: 67%;\" />\n\n\n\n### 整体梳理一下\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8ewkskjmcj30ge0oqq4t.jpg\" alt=\"img\" style=\"zoom:67%;\" />\n\n首先输入Embedding和positional Encoding，然后输入到Self-attention，并且使用multi-head多层堆叠，位置编码，然后使用Add & Norm 残差连接和归一化，并且使用多个transformer块进行训练。\n\n在decoder方面，为了使得中间隐层向量的解码带有注意力，而不是一视同仁地统一解码，所以需要某种计算机制来生成一种记录训练过程中语义倾向的值。\n\n\n\nencoder-decoder attention\n用到的策略的名称叫做encoder-decoder attention，\n用原博中的两个gif就能解释，首先通过最末尾的一个encoder保留KV，初始化Q得到decoder的第一个输出\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8ewkmb050j30n30d23z9.jpg)\n\n图示中的I就是decoder的第一个输出，\n接下来这个输出当做下一个decoder的Q，然后再获取之前的encoder的KV，继续做self attention\n如下图所示：\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8evrbaucnj30oz0d3wfh.jpg)\n\n这样一来就能够通过transformer得到对应的一个不带位置编码的输出\n一句话总结就是：\n**encoder-decoder attention中，Q来自于decoder的上一个输出，KV来自于encoder的输出**。\n\n\n\n\n\n## 源码\n\n## Pre-trained models\n\n预训练的模型包含如下：\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8f1va35f6j310h07aabq.jpg)\n\nREADME中介绍.zip包含3大块：\n\n- A TensorFlow checkpoint (`bert_model.ckpt`) containing the pre-trained weights (which is actually 3 files).  **权重参数和偏置**\n- A vocab file (`vocab.txt`) to map WordPiece to word id. **词表**\n- A config file (`bert_config.json`) which specifies the hyperparameters of the model.**超参数**\n\n\n\n\n\n| 这种表格 |      |      |\n| -------- | ---- | ---- |\n|          |      |      |\n|          |      |      |\n|          |      |      |\n\n","tags":["BERT","NLP"],"categories":["AI"]},{"title":"博士这五年-李沐","url":"/2022/06/13/博士这五年-李沐/","content":"\n## 前言\n\n12年8月提着一个行李箱降落在匹兹堡机场。没找住的地方，也不知道CMU应该怎么去。对未来一片迷茫，但充满乐观。 现在，刚完成了博士期间最后的一场报告，在同样的机场，不过是在等待离开的航班。\n\n回想过去的五年，是折腾的五年，也是自我感悟和提升的五年。这里我尝试记录这五年主要做过的事情和其中的感想，希望对大家有所启发。\n\n## 第0年：3/11-8/12\n\n我第一次申请美国的博士是在11年，但拿到的offer并没有特别合适的导师，于是就北上投奔文渊去了。 我当时在百度商务搜索部门做广告的点击预估。具体是使用机器学习来预测一个广告是不是会被用户点击。 这时候离“大数据”这个词流行还有两年，但百度那时候的数据即使现在来看仍然是大的。我的任务是如何高效的利用数百台机器快速的在数十T的数据上训练出模型。\n\n当时产品用的算法基于LBFGS，我于是想是不是可以换个收敛更快的算法。没几天就找到个不错 。但实现上发现了各种问题，包括性能，收敛，和稳定性。而且那时有的就是一个裸的Linux和很老版本的GCC，什么都是需要从头开始写。花了大量时间做系统优化，算法改动，和线上实验，最后一年后在整个广告流量上上了线。\n\n现在再回顾会觉得整个一年时间都在打磨各种细节上，有时候为了5%的性能提升花上上千行代码。这些都导致算法过于复杂，有过度设计之嫌。但深入各个细节对个人能力提升很大，而且很多遇到的问题成为了之后研究方向的来源。一些算法上的思考曾写在[这里](https://link.zhihu.com/?target=http%3A//mli.github.io/2013/03/24/the-end-of-feature-engineering-and-linear-model/)，当时候深度学习刚刚出来，冥冥中觉得这个应该是大规模机器学习的未来，不过真正开始跟进是好几年以后了。\n\n11年12月中的时候突然心血来潮随手把材料重新寄了一遍，就选了CMU和MIT，结果意外收到了CMU的offer。有天在百度食堂同凯哥（余凯）和潼哥（张潼）吃饭，我说收了CMU offer，在纠结去不去。他们立马说去跟Alex Smola啊，他要要加入CMU了，我们给你引荐下。\n\n记得是离开的前一天才开始打包行李，早上去公司开完会，中午离职，跟小伙伴打招呼说出个国，然后就奔机场了。那天北京天气特别好，完全不记得前一天雾霾刚爆了表。\n\n## 第一年：9/12-8/13\n\n第一年的主要事情是熟悉环境和上课。CMU课程比较重，博士需要学8门课，每门课工作量巨大。而且要求做两门课助教，做助教比上课更累。\n\n这一年上的课中对我最有用的是“高级分布式系统”。之前在上交ACM班的时候已经学过很多质量都还不错课，纯知识性的课程一般对我帮助不大。但这门课主要是读论文，然后大家讨论。不仅仅是关于知识，很多是对设计理念的领悟。大家知道对于系统而言，设计是一门艺术而不是科学，这是设计者审美和哲学理念的体现。同时系统界历史也是由一波又一波的潮流组成，了解历史的发展以及其中不断重复的规律非常有意义。\n\n那年这门课上课老师是Hui Zhang（神人之一，20多岁就在CMU任教了，学生包括了Ion Stoica，他是Spark作者Matei的导师），他有非常好的大局观，对于“Why”这个问题阐述非常到位。我是通过这门课才对分布式系统有了比较清晰的认识。两年之后我偶然发现我的一篇论文也在这门课的阅读列表里了，算是小成就达成 。\n\n除了上课，更重要是做研究。我去CMU的时候Alex那时还在Google，而且没经费，所以把我丢给了 Dave Andersen。于是我有了两个导师，一个做机器学习，一个做分布式系统。\n\n前面半年都是在相互熟悉的过程。我们每周会一起聊一个小时。前半年因为Alex不在，所以我们只能视频。Alex那边信号经常不好，而且他有德国和澳大利亚口音，外加思维跳跃，经常我听不懂他说啥只能卖萌傻笑。还是靠着Dave不断的打字告诉我Alex说了什么才度过了前几次的会。\n\n两个导师风格迥异。Alex是属于反应特别快，通常你说一点，他已经想好了接下来十点，要跟上他节奏很难。一般抛出问题的时候他就想好了好几个解决方法。这时候要证明自己的想法比他的更好不容易，需要大量的沟通和实验数据支撑。我想我大概是花了两年证明了在某些方向上我的方案一般更好，所以这时候他就不那么hands-on了。\n\nDave不会给很多想法，但会帮助把一个东西理解透，然后讲得很清楚。因为我研究方向主要是机器学习上，基本上前两年基本都是我在教Dave什么叫机器学习，而且是尽量不用公式那种教法。\n\n我的第一个研究工作是关于如果划分数据和计算使得减少机器学习求解中的网络通讯量。Alex体现了他的强项，几分钟就把问题归纳成了一个优化问题，然后我们三各自提出一个解法。我做了做实验发现Dave的算法更好。接下来两个月把算法做了很多优化，然后又做了点理论分析就把论文写了。\n\n可惜这个想法似乎有点超前，虽然我们一遍又一遍的改进写作，但投了好几个会审稿人就是不理解，或者觉得这个问题不重要。那个时候学术界已经开始吹嘘“大数据”，但我觉得其实大部分人是不懂的，或者他们的“大数据”仍然是几个GB的规模，烤U盘需要十来分钟的那种。\n\n这是我在CMU的一个工作，我觉得挺有用，但却是唯一没能发表的。\n\n当时跟我坐同一个办公室的是Richard Peng，他做的是理论研究。我经常跟他讨论问题，然后有了些想法合作了一个工作。大体思想是把图压缩的快速算法做到矩阵的低秩近似上。这个工作写了三十页公式但没有任何实验，我主要当做写代码间隙的悠闲娱乐，不过运气很好的中了FOCS。\n\n坦白说我不是特别喜欢纯理论这种，例如在bound的证明中很多大量的项直接丢掉了，导致我觉得bound特别的近似。对于做系统的人来说，最后拼的是常数。这个工作中这种大开大合的做法我觉得很不踏实。所以我觉得以后还是应该做更实在点的东西。\n\n在CMU回到了去百度前的一周七天工作无休的节奏。每周至少80个小时花在学校。如果累了就去健身房，我一般晚上12点去。不仅是我一个人，大家都很努力，例如凌晨的健身房，早3点的办公室，四处都可以见到中国或者印度学生。我那时候的室友田渊栋花在学校的时候比我多很多。\n\n那一阵子有读了很多关于优化的文章。其中对我启发最大的是Bertsekas写于80年代末的那本关于分布式计算的书。此书可以认为是MIT控制领域黄金一代研究成果总结，换到现在仍然不过时。\n\n受启发我转去研究异步算法，就是分布式下不保证数据的及时性来提升系统性能。我基于在百度期间做的算法，做了一些改进和理论分析，然后投了NIPS。\n\n投完NIPS就动身去了Google Research实习。那时候Google Brain成立不久，在“宇宙的答案”42楼，包括Jeff Dean，Geoffrey Hinton，Prabhakar Raghavan好些大牛挤在一起，加起来论文引用率能超80万。\n\nAlex跟我说，你去读读Jure Leskovec的文章，学学人家怎么讲故事。我在Google也尝试用了些用户GPS数据来对用户行为建模。可是写文章的时候怎么也写不出Jure的那种故事感，发现自己不是那块料。这篇文章因为用了用户数据，恰逢Snowden让大家意识到隐私的重要性，历经艰辛删了一半结果Google才允许发出来。有些累觉不爱。\n\n不过在Google期间我主要时间花在研究内部代码和文档上。Google的基础架构很好，文档也很健全。虽然没有直接学到了什么，但至少是开了眼界。\n\n## 第二年：9/13-8/14\n\n这学期上了Tuomas Sandholm的机制设计，此乃另一大神，例如最近德州扑克赢了专业选手，之前开公司也卖了上亿。不过这门课我是完完全全没学懂，连承诺的课程大作业都没怎么做出来。之后的两年里我一遇到Tuomas他都会问下有什么进展没。我只能远远看见他就绕开。\n\nNIPS被拒了，发现审稿人不懂线程和进程的区别，有点沮丧。隔壁实验室一篇想法类似但简单很多的论文倒是中了oral，所以那阵子压力很大。Alex安慰说这种事情常有发生，看淡点，然后举了很多自己的例子。\n\n之后想了想，一篇好文章自然需要有足够多的“干货”，或者说信息量， 但一篇能被接受的文章需要满足下面这个公式：\n\n> 文章的信息量 / 文章的易读性 < 审稿人水平 * 审稿人花的时间\n\n对于机器学习会议，因为投稿量大，所以审稿人很多自然平均水平就会下降。而且很多审稿人就花半个小时到一个小时来读文章，所以公式右边数值通常是很小，而且不是我们能控制。\n\n如果文章的信息量不大，例如是改进前面工作或者一些简单的新想法，那么公式成立的概率很大。而对于信息量大的文章，就需要努力提升易读性，包括清晰的问题设定，足够的上下文解释等等。而前面投的那篇NIPS，以及更早的那个被拒工作，就是因为我们假设了审稿人有足够多的相关专业知识，而我们塞进了太多干货使得大家都读糊涂了。\n\n即使对于已经发表的文章，上面那个公式同样可以用来衡量一篇论文的引用率。例如经常见到干货很多的文章没有什么人引用，而同时期的某些工作就是考虑了其中简单特殊情况结果被大引特引。\n\n接下来的半年我主要在做一个通用的分布式机器学习框架，是想以后做实验方便些。名字就叫parameter server，沿用了Alex 10年论文提出的名字。花了很多时间在接口设计上，做了好几个版本实现，也跑了些工业界级别的大规模的实验。\n\n不过真正花了我大量时间的是在写论文上。目标是把这个工作投到OSDI上，OSDI是系统界两大会之一。我们预计审稿人跟Dave两年前状态差不多，不会有太多机器学习和数学背景，所以需要尽量的少用公式。整整一个月就花在写论文上，14页的文章满满都是文字和示意图。不过努力没有白费，最终论文被接受了。随后又花了好几周准备大会报告上。相对于平时花一周写论文，两三天准备报告，这次在写作和报告水平上有了很大的提升。没有放进去的公式和定理投了接下来的NIPS，这次运气很好的中了。\n\n有了文章后稍微心安了点可以更自由的做些事情。\n\n寒假回了趟国，跑去百度找了凯哥和潼哥。潼哥说他最近有个想法，于是快糙猛的把实验做了然后写了篇论文投了KDD。同时期Alex一个学生也把他一个一直想让我做但我觉得这个小trick不值得我花时间的想法投了KDD，结果中了最佳论文。作报告那天我在的会场稀稀疏疏几个人，他们隔壁会场人山人海。这个使得好长一段时间我都在琢磨是不是还是要跟着导师走比较好。\n\n那时凯哥在百度搞少帅计划，觉得蛮合适就加入了。这时凯哥正带着一大帮兄弟轰轰烈烈的搞深度学习，我自然也是跳坑了。试过好几个想法后，我觉得做做分布式的深度学习框架比较对胃口。我挑了CXXNet作为起点，主要是因为跟天奇比较熟。同时也慢慢上手跑一些Alexnet之类的实验。\n\n我是因为少帅计划才开始开始做深度学习相关项目，凯哥也很支持我做开源开发回馈社会而不是只做公司内部的产品。但在少帅期间并没有做出什么对公司有帮助的事，很是惭愧。\n\n## 第三年：9/14-8/15\n\n回CMU后Alex看见深度学习这么火，说我们也去买点GPU玩玩。但我们比较穷，只能去newegg上掏点便宜货。这个开启了轰轰烈烈的机器折腾之旅。整个一年我觉得我都在买买买装装装上。最终我们可能就花了小几万刀攒出了一个有80块GPU的集群。现在想想时间上花费不值得，而且为了图便宜买了各种型号的硬件导致维护成本高。但当时候乐在其中。具体细节可以看这篇[blog](https://link.zhihu.com/?target=http%3A//mli.github.io/gpu/2016/01/17/build-gpu-clusters/)\n\n这一年写了很多parameter server代码，同时花了很时间帮助用户使用这些代码。很难说做得很成功，现在想想有几个原因。写代码时我会优先考虑性能和支持最多的机器学习算法。但正如前面的错误，忽略了代码的易读性，从而导致只有少部分人能理解代码从而做一些开发。例如我尝试让Alex组的学生来使用这些代码，但其中的各种异步和callback让他们觉得很是难懂。其次是没有人能一起审核代码接口，导致这些接口有浓浓的个人味道，很难做到对所有人都简单明了。\n\n不过幸运的是找到一帮志同道合的小伙伴。最早是我发现天奇在写xgboost的分布式启动脚本，我看了看发现挺好用，就跟他聊了聊。聊下的发现有很多基础部件例如启动脚本，文件读取应该是可以多个项目共同使用，而不是每个项目都造一个轮子。于是跟天奇在Github上创建了一个叫DMLC的组织，用来加强合作和沟通。第一个项目是dmlc-core，放置了启动和数据读取代码。\n\nDMLC的第二个新项目叫wormhole。想法是提供一系列分布式机器学习算法，他们使用差不多相同的配置参数来统一用户体验。我把parameter server里面的机器学习相关算法移植了过来，天奇移植了xgboost。Parameter server原有的系统代码简化到了ps-lite。\n\n中途我听百度同学说factorization machine（FM）在广告数据上效果不错，所以在wormhole上实现了下。针对分布式做了一些优化，然后投了WSDM。前后没有花到一个月，但神奇的竟然拿了最佳论文提名。\n\n在wormhole的开发中发现一个问题，就是各个算法还是挺不一样，他们可以共用一些代码，但又有各自的特点，需要特别的优化来保证性能。这样导致维护有些困难，例如对共用代码的改动导致所有项目都要检查下。总结下来觉得一个项目最好只做一件事情。所以天奇把xgboost代码放回原来项目，我也把FM独立出来一个项目叫difacto。\n\n通过一系列的项目，我学到的一点是，以目前的水平和人力，做一个通用而且高效的分布式机器学习框架是很难的一件事情。比较可行的是针对一类相似的机器学习算法做针对性的项目。这个项目的接口必须是符合这类算法结构，所以做算法开发的同学也能容易理解，而不是过多暴露底层系统细节。\n\n真正的让DMLC社区壮大的项目是第三个，叫做MXNet。当时的背景是CXXNet达到了一定的成熟度，但它的灵活性有局限性。用户只能通过一个配置项来定义模型，而不是交互式的编程。另外一个项目是zz和敏捷他们做的Minerva，是一个类似numpy的交互式编程接口，但这个灵活的接口对稳定性和性能优化带来很多挑战。我当时候同时给两个项目做分布式的扩展，所有都有一定的了解。然后一个自然的想法是，把两个项目合并起来取长补短岂不是很好。\n\n召集了两个项目的开发人员讨论了几次，有了大致的眉目。新项目取名MXNet，可以叫做mixed-net，是前面两个名字（Minerva和CXXNet）的组合。放弃开发了几年的项目不是容易的决定，但幸运的是小伙伴都愿意最求更好，所以 MXNet进展挺顺利。很快就有了可以跑的第一个版本。\n\n## 第四年：9/15-8/16\n\n前半年为difacto和MXNet写了很多代码。其实一开始的时候我觉得difacto更重要些，毕竟它对于线性算法的提升非常显著而且额外的计算开销并不大，这对广告预估之类的应用会有非常大的提升。但有次遇到Andrew Ng，我跟他说我同时在做这两个项目，他立即告诉我我应该全部精力放在MXNet上，这个的未来空间会大很多。我一直很佩服Andrew的眼光，所以听了他的建议。\n\n11月的时候MXNet就有了很高的完成度。写了个小论文投去了NIPS的workshop也算是歇了口气。但随后就听到了TensorFlow（TF）开源的消息。由 Jeff Dean领导大量全职工程师开发，Google庞大的宣传机器支持，不出意料迅速成为最流行的深度学习平台。TF对我们压力还是蛮大，我们有核心开发者转去用了TF。不过TF的存在让我领悟到一点，与其过分关心和担忧对手，不如把精力集中在把自己的做得更好。\n\nNIPS的时候MXNet的小伙伴聚了一次，有好几个我其实是第一次见面。随后Nvidia的GTC邀请我们去做报告。在这两次之间大家爆发了一把，做了很多地方的改进。同时用户也在稳步增长。我们一直觉得MXNet是小开发团队所以做新东西快这是一个优势，但随着用户增加，收到抱怨说开发太快导致很多模块兼容性有问题。有段时间也在反思要在新技术开发速度和稳定性之间做一些权衡。\n\n这时一夜之间大数据不再流行，大家都在谈深度学习了。\n\n我也花了很多力气在宣传MXNet和争取开发者上。包括微博知乎上吼一吼，四处给报告。在大量的点赞声中有些陶醉，但很多中肯的批评也让我意识到重要的一点，就是应该真诚的分享而不是简单的吹嘘。\n\n因为大量的媒体介入，整个深度学习有娱乐化的趋势。娱乐化的报道很多都只是一些简单信息，（有偏见）的观点，而没有太多干货。不仅对别人没营养，对自己来说也就是满足虚荣心。与其写这些简单的水文，不如静下心做一些有深度的分享，包括技术细节，设计思路，和其中的体会。\n\n此类分享一个容易陷入的误区是只关注自己做了什么，结果多么好。这些确实能证明个人能力，对于想重复这个工作的人来说会有很大帮助。但更多的人更关心的是适用范围在哪里，就是什么情况下效果会减弱；为什么结果会那么好；insight是什么。这个需要更多深入的理解和思考，而不是简单的展示结果。\n\n这个对写论文也是如此。只说自己的结果比基线好多少只能说明这是不错的工作，但结果再好并不能意味这个工作有深度。\n\n深度学习的火热导致了各种巨资收购初创司不断。Alex也有点按耐不住， 结果是他，Dave，Ash（曾经是YahooCTO）和我合伙弄了一家公司，拿了几十万的天使投资就开工了。Alex写爬虫，Dave写框架，我跑模型，风风火火干了好一阵子。可惜中途Dave跑路去跟Jeff做TF了。后来这个公司卖给了一个小上市公司。再后来我们觉得这个公司不靠谱也就没考虑跟他们干了。\n\n第一次创业不能说很成功，从中学到几点：一是跟教授开公司一定要注意有太多想法但没死死的掐住一个做，二是找一堆兼职的博士生来干活不是特别靠谱，尤其是产品不明确的时候，三是即使要卖公司也一定要做一个产品出来。我们卖的时候给很多人的感觉是团队人太强但产品太弱，所以他们只想要人而已。四是试图想要通过技术去改变一个非技术公司是很难的事情，尤其是过于新的技术。\n\n然后我们就奔去折腾下一个公司。Ash早财务自由所以想做一个大的想法，但这时Alex刚在湾区买了个房，有还贷压力，他选择去了Amazon。于是算是胎死腹中。\n\n随后收到Jeff的邮件说有没有兴趣加入Google，自然这是一个很诱人的机会。同时我觉得小的创业技术性强的公司是不错的选择。但从MXNet的发展上来书，去Amazon是最好选择之一。自己挖的坑，总是要自己填的。所以我以兼职的身份去了Amazon，领着一帮小弟做些MXNet开发和AWS上深度学习的应用。\n\n## 第五年：9/16-2/17\n\n早在15年初Alex就表示我可以毕业了，但作为拖延晚期患者，迟迟没开始准备。这时候感觉不能再拖了，于是窝在湾区写毕业论文。Alex觉得毕业论文应该好好写，但我对把前面都做完的东西再捣鼓写写实在是没兴趣，尤其是加州太阳那么好，大部分时间我都是躺在后院晒太阳。此时B站已经完全被小学生占领，这边买书也不方便，无聊之余刷了很多起点。然后还写了篇[炼丹文](https://zhuanlan.zhihu.com/p/23781756)。\n\nCMU要求答辩委员会需要有三个CMU老师和一个学校外的。除了两个导师外，我找了Jeff Dean和刚加入CMU的Ruslan Salakhutdinov. 结果Russ随后就加入了Apple，整个委员会的人都在湾区了。Jeff开玩笑说可以来Google答辩。可惜跟CMU争吵了好多次，还是不允许在校外答辩，而且必须要三个人委员会成员在场。这些限制导致答辩一拖再拖，而且临时加了Barnabas Poczos来凑人数。最后是Jeff的助理快刀斩乱麻的协调好了时间把所有东西定好了。没有她估计我还可以拖几个月。\n\n答辩的时候是一个比较奇异的状态，委员会里有Google, Amazon, Apple的AI负责人，剩下两个和我又分别在这三家公司兼职。这个反应了当下AI领域学术界纷纷跑去工业界的趋势。\n\n不过答辩这个事情倒是挺简单，跟平常做个报告没什么太多区别。一片祥和，即使Russ问了MXNet和TensorFlow哪家强这个问题也没有打起来。\n\n答辩后我问委员会说，我在考虑找个学术界的工作，有什么建议没。大家介绍了一大堆经验，不过大家都强调的一个重点是：学术界好忙好忙，而且好穷好穷，工业界的薪水（就差指自己脸了）分分钟秒掉CMU校长。你要好好想。\n\n## 总结\n\n答辩前一天的晚上，我想了两个问题，一个是“博士收获最大的是什么”，另一个是“如果可以重来会怎么办”。对于第一个问题，这五年时间自然学到了很多东西，例如系统的学习了分布式系统，紧跟了机器学习这五年的发展，写文章做幻灯片做报告水平有提升，代码能力也加强了些。自信上有所提高，觉得既可以做一流的研究，也可以写跟大团队PK的代码。只要努力，对手没什么可怕的。\n\n但更重要的是博士的五年的时间可以专注的把一些事情从技术上做到最好，做出新的突破，这个氛围没有其他地方能给予。\n\n第二个问题的一个选项是当年留在国内会怎么样？ 当年百度的伙伴们多数现在都做得很好，都在引领这一波AI的潮流，甚至有好几个创造了上亿价值的公司。所以从金钱或者影响力角度来看，一直在工业界也不差，说不定现在已经是土豪了。\n\n不过我觉得还是会选择读博。赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次。站在这个领域的高点会发现世界虽然很大，但其实其他领域也使用差不多的技术，有着同样的发展规律。博士期间领悟到的学习的方法可以在各个方向上都会大有作为。\n\n更重要的是理想和情怀。人一生要工作五十年，为什么不花五年来追求下理想和情怀呢？\n","tags":["李沐","读博"],"categories":["AI"]},{"title":"【推荐算法】推荐系统之评估方法和评价指标PR、ROC、AUC","url":"/2022/06/10/推荐系统之评估方法和评价指标PR、ROC、AUC/","content":"\n\n\n# 推荐系统文档\n\n## 1、推荐系统之评估方法和评价指标PR、ROC、AUC\n\n> 链接：https://www.jianshu.com/p/5b0bc79e3d75\n\n# 简介\n\n推荐系统的评估相关的知识比重在整个推荐系统的知识框架中占比不大，但是其重要程度不言而喻，因为采用的评价指标直接影响到了推荐系统的优化方向是否正确。**评价指标主要用于评价推荐系统各方面的性能**，按照应用场景可以分为离线评估和线上测试。其中离线评估的主要方法包括**Holdout检验、交叉检验、留一验证、自助法**等，评价指标主要包括**用户满意度、预测准确度、召回率、覆盖率、多样性、新颖性、流行度、均方根误差、对数损失、P-R曲线、AUC、ROC曲线**等等。线上测试的评估方法主要包括**A/B测试、Interleaving方法**等，评价指标主要包括**点击率、转化率、留存率、平均点击个数**等等。本文将着重介绍**离线评估相关方法和指标**，尤其是**P-R曲线、AUC、ROC曲线**等，这些评价指标是最常用的也是最基本的，出现在各类推荐相关的论文中，因此需要重点掌握。\n\n-------------\n\n# 离线评估方法和评价指标\n\n在推荐系统的评估过程中，离线评估往往被当做最常用也是最基本的评估方法。顾名思义，离线评估是指在将模型部署于线上环境之前，在离线环境中进行的评估。由于不用部署到生产环境，离线评估没有线上部署的工程风险，也无须浪费宝贵的线上流量资源，而且具有测试时间短，同时进行多组并行测试、能够利用丰富的线下计算资源等诸多优点。\n\n## 离线评估的主要方法\n\n- Holdout检验\n  Holdout检验是基础的离线评估方法，它将原始的样本集合随机划分为训练集和测试集两部分。举例来说，对于一个推荐模型，可以把样本按照70%~30%的比例随机分成两部分，70%的样本用于模型的训练，30%的样本可以用于模型的评估。\n  在实际应用中，有很多方便的库可以帮助我们进行样本集合的划分，比如scikit-learn中提供的train_test_split函数，下面进行个简单展示：\n\n\n\n```python\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nx,y = np.arange(10).reshape((5,2)), range(5)\nprint(\"data: \\n\", x)\nprint(\"labels: \", list(y))\n\n# 对数据集进行划分，设置测试集占比30%，训练集占比70%\nX_train, X_test,Y_train,Y_test = train_test_split(x, y, test_size=0.3, random_state=100)\nprint(\"Train Data: \", X_train, Y_train)\nprint(\"Test Data: \", X_test, Y_test)\n```\n\n输出：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xahrs25j20cc0bkglx.jpg)\n\nHoldout检验的缺点也很明显，即在验证集上计算出来的评估指标与训练集合验证集的划分有直接关系，如果仅仅进行少量Holdout检验，则得到的结论存在较大的随机性。为了消除这种随机性，“交叉检验”的思想被提出。\n\n- 交叉验证\n  **k-fold交叉验证**：先将全部的样本划分为k个大小相等的样本子集；依次遍历这k个子集，每次都把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后将所有的k次的评估指标的平均值作为最终的评估指标。在实验中，k经常取10。同样,scikit-learn中提供了KFold函数可以使用，例子如下：\n\n\n\n```python\nfrom sklearn.model_selection import KFold\n\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([1, 2, 3, 4])\n\nkf = KFold(n_splits=4)\nkf.get_n_splits(X)\n\nfor train_index, test_index in kf.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n```\n\n输出：![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xaws6m1j20bo04e74g.jpg)\n\n**留一验证**：每次留下1个样本作为验证集，其余所有样本作为测试集。样本总数为n，依次遍历所有n个样本，进行n次验证，在将评估指标求平均得到最终指标，在样本总数较多的情况下，留一验证法的时间开销极大。事实上，留一验证是留p验证的特例。留p验证是指每次留下p个样本作为验证集，而从n个元素中选取p个元素共有<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xbd6tkkj201m0183y9.jpg\" alt=\"image-20220609132353358\" style=\"zoom:50%;\" />种可能，因此它的时间开销远超留一验证，故很少在实际工程中使用。\n同样，scikit-learn中提供了LeaveOneOut方法可使用，例子如下：\n\n```swift\nimport numpy as np\n\nfrom sklearn.model_selection import LeaveOneOut\nX = np.array([[1, 2], [3, 4], [5,6]])\ny = np.array([1, 2, 3])\n\nloo = LeaveOneOut()\nloo.get_n_splits(X)\n\nfor train_index, test_index in loo.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    print(X_train, X_test, y_train, y_test)\n```\n\n结果：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xbt3o5oj20cs090q3d.jpg)\n\n- 自助法\n  不管是Holdout检验还是交叉检验，都是基于划分训练集和测试集的方法进行模型评估的。然而，当样本规模比较小的时候，将样本集进行划分会让训练集进一步减小，这可能会影响到模型的训练效果。有没有能维持训练集样本规模的验证方法呢？“自助法”可以在一定程度上解决这个问题。\n  自助法是基于自助采样法的检验方法：对于总数为n的样本集合，进行n次有放回的随机抽样，得到大小为n的训练集。在n次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集进行模型验证，这就是自助法的验证过程。\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xdawo9tj210q098dhc.jpg\" alt=\"image-20220609132533595\" style=\"zoom:50%;\" />\n\n## 离线评估的指标\n\n- 均方根误差\n  很多推荐网站都会提供一个让用户给物品打分的功能，如果知道了用户对物品的历史评分数据，就可以从中学习到用户的兴趣模型，并预测该用户在将来浏览到未曾见过的物品时，会给这个物品打多少分。评分预测可以看做是回归问题，评分预测的预测准确度一般是通过均方差误差(RMSE)和平均绝对误差(MAE)计算。对于测试集中的一个用户<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xdtnvikj20120180hl.jpg\" alt=\"image-20220609132615154\" style=\"zoom:50%;\" />和物品<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xeak7anj201201e0jd.jpg\" alt=\"image-20220609132641992\" style=\"zoom:50%;\" />，我们令<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xf1hsscj201m01e0r9.jpg\" alt=\"image-20220609132724384\" style=\"zoom:50%;\" />代表用户<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xdtnvikj20120180hl.jpg\" alt=\"image-20220609132615154\" style=\"zoom:50%;\" />对物品<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xeak7anj201201e0jd.jpg\" alt=\"image-20220609132641992\" style=\"zoom:50%;\" />的实际评分，而<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xgkgib7j201q01i0s9.jpg\" alt=\"image-20220609132853052\" style=\"zoom:50%;\" />代表我们的推荐算法给出的预测评分。\n  综上，可以得出RSME的定义为：    \n\n  ​\t\t\t\t\t\t\t\t\t\t\t ![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31xh51u1cj207802jq2s.jpg)     \n\n​\t同理，MAE采用绝对值计算预测误差，定义如下：:\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31yosny0bj206t02jweb.jpg)\n\n其中|T|代表测试集的大小。\n一般情况下，RMSE能够很好滴反映回归模型预测值与真实值的偏离程度。但在实际应用时，如果存在个别偏离程度非常大的离群点，那么即使离群点数量非常少，也会让RSME指标变得很差。为了解决这个问题，可以使用鲁棒性更强的平均绝对百分比误差（Mean Absolute Percent Error，MAPE）进行类似的评估，MAPE的定义如下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31yps1eklj20io05eaa2.jpg\" alt=\"image-20220609141219926\" style=\"zoom:50%;\" />\n\n相比RSME，MAPE相当于把每个点的误差都进行了归一化，降低了个别离群点带来的绝对误差的影响。\n\n- 覆盖率\n\n  覆盖率是描述一个推荐系统对物品长尾的发掘能力，即推荐系统做到了雨露均沾，对商城中的每一个物品都有涉及，而不是只推荐那些热门的商品。据此，覆盖率的一个简单定义为推荐系统能够推荐出来的物品占总物品集合的的比例。假设系统的用户集合<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31yqp5ujvj201601a0ml.jpg\" alt=\"image-20220609141312721\" style=\"zoom:50%;\" />，推荐系统给每个用户推荐了一个长度为N的物品列表<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31yr312qdj202g01emwx.jpg\" alt=\"image-20220609141333960\" style=\"zoom:50%;\" />，那么覆盖率的计算公式为：:\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h32g5rg0m5j20e2052jrb.jpg\" alt=\"image-20220609141354307\" style=\"zoom:50%;\" />\n\n其中｜I｜是商品的总数。\n\n\n\n- 新颖度与平均流行度\n\n  我们使用推荐列表中全部物品的平均流行度衡量推荐结果的新颖度。如果推荐的物品都很热门，那么说明推荐的新颖度比较低。反之，说明推荐结果比较新颖。\n\n  流行度的定义如下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31ysonuj7j20go0583yh.jpg\" alt=\"image-20220609141507747\" style=\"zoom: 50%;\" />\n\n其中p是每个物品的流行度，可以通过该物品在测试集中出现的次数来简单计算，N是推荐物品集合的总数。\n\n这里在计算平均流行度的时候对每个物品的流行度取对数，这是因为物品的流行度分布满足长尾分布，取对数后，流行度的平均值更加稳定。\n\n\n\n- 对数损失函数\n\n  对数损失函数（LogLoss）也是经常在离线评估中使用的指数，在一个二分类问题中，LogLoss的定义如下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31ytfdu1bj20q604yaa5.jpg\" alt=\"image-20220609141550454\" style=\"zoom:50%;\" />\n\n其中，yi为输入实例xi的真实类别，pi为预测输入实例xi是正样本的概率，N是样本总数。\n\n> LogLoss就是逻辑回归的损失函数，而大量深度学习模型的输出层正式逻辑回归或者Softmax，因此采用LogLoss作为评估指标能够非常直观地反应模型损失函数的变化。\n\n- 准确率\n\n  对于分类问题，比如CTR问题，准确率（Accuracy）是指分类正确的样本占总样本个数的比例，即：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31yv659pkj20di04ejra.jpg\" alt=\"image-20220609141730897\" style=\"zoom:50%;\" />\n\n其中，n_corret代表被正确分类的个数，n_total代表总样本个数。准确率是分类任务中较为直观的评价指标，虽然具有较强的可解释性，但是也存在明显的缺陷：当不同类别的样本的比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。例如，如果负样本占比99%，那么分类器将所有样本都预测为负样本，也可以取得99%的准确率。\n\n\n\n- 精准率和召回率\n\n  精准率（Precision）是分类正确的正样本个数占分类器判定为正样本的样本个数的比例。召回率（Recall）是分类正确的正样本个数占真正的正样本个数的比例。\n\n  在排序模型中，通常没有一个确定的阈值把预测结果直接判定为正样本或负样本，而是采用TopN排序结果的精准率（Precision@N）和召回率（Recall@N)来衡量排序模型的性能，即认为模型排序的TopN的结果就是模型判定的正样本，然后分别计算Precision@N和Recall@N。\n\n以TopN推荐为例，令R(u)代表模型根据用户在训练集上的行为给用户计算出的推荐列表，而T(u)代表用户在测试集上的真实喜爱列表。那么推荐结果的精准率的定义如下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31ywln0cpj20lc074weo.jpg\" alt=\"image-20220609141853524\" style=\"zoom:50%;\" />\n\n从公式上看，它是把用户真实喜爱列表和推荐列表的交集的大小去除以推荐列表的大小，它的意义是计算在所预测的推荐列表中究竟有多少物品是用户感兴趣的。\n召回率的定义如下:\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31ywv0m2fj20jg06st8v.jpg\" alt=\"image-20220609141907865\" style=\"zoom:50%;\" />\n\n可以看到它与精准率的定义非常相似，唯一不同的是分母变成了用户真实喜爱列表大小。它的意义在于用户真实喜爱列表中的物品中有多少是被推荐算法预测出来的，即真实列表的召回率。\n维基百科上的图片很好地展示了Precision和Recall的计算公式，方便记忆:\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31yxddde4j20m814f764.jpg)\n\n- 上图中圆圈内的代表被选出的样本，用其中的正样本除以被选中的样本总数就是Precision，用其中的正样本除以所有的正样本数量就是Recall。\n\n  \n\n> 注意准确率（Accuracy）和精准率（Precision）的区别。\n\n精准率和召回率是矛盾统一的两个指标：为了提高精准率，分类器需要尽量在“更有把握时”才把样本预测为正样本，即降低了精准率计算公式中的分母部分。但往往会因为过于保守而漏掉很多“没有把握”的正样本，导致召回率过低。\n以挑选西瓜为例，若希望将好瓜尽可能多地挑选出来，则可通过增加选瓜的数量来实现，如果将所有的西瓜都选上，那么所有的好瓜也必然都被选上了，这样就会导致Precision很低，但是Recall就会相对较高。若希望选出的瓜中好瓜比例尽可能高，则可只挑选最有把握的瓜，但这样就难免会漏掉不少好瓜，使得Recall较低。\n为了综合反映Precision和Recall的结果，可以使用F1-score，F1-score是精准率和召回率调和平均值，定义如下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31yxwhwrkj20e604gaa3.jpg\" alt=\"image-20220609142008816\" style=\"zoom:50%;\" />\n\n用一张图总结一下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31zehwvulj20xc0d0dgu.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n上图左边是混淆矩阵，右边分别是精准率、召回率、F1-score、准确率的计算公式。\n\n\n\n> 关于混淆矩阵，在下一节有详细介绍。\n\n\n\n- P-R曲线\n\n  P-R曲线，顾名思义，其中P代表Precision，R代表Recall。P-R曲线就是根据精确率和召回率而绘制的曲线，一般横轴选择召回率，纵轴选择精确率。对于一个排序模型来说，其P-R曲线上的一个点代表“在某一阈值下，模型将大于该阈值的结果判定为正样本，将小于该阈值的结果判定为负样本时，排序结果对应的召回率和精确率”。整条P-R曲线是通过从高到低移动正样本的阈值生成的。如下图所示，其中包含了3个模型的P-R曲线，其中横轴0点附近代表阈值最大时模型的Precision和Recall。\n\n\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31zeovw2gj20xa0pogn9.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\nP-R图直观地显示出模型在样本总体上的Precision、Recall。在进行比较的时候，若一个模型的P-R曲线被另外一个模型的P-R曲线完全“包住”，则可断言后者的性能优于前者。如上图中模型A的性能就优于模型C；如果两个模型的P-R曲线出现了交叉，如上图汇总的A和B，则难以一般性地断言两者孰优孰劣，只能在具体的Precision和Recall条件下进行比较。\n\n\n\n- **ROC曲线**\n\n  ROC曲线的全称是“the Receiver Operating Characteristic”曲线，中文译为“受试者工作特征曲线”。ROC曲线最早诞生于军事领域，而后在医学领域应用甚广，“受试者工作特征曲线”这一名称也正是来自于医学领域。\n\n  在正式介绍ROC曲线之前，我们先来彻底理解一下混淆矩阵的定义。混淆矩阵中有Positive、Negative、True、False等概念，意义如下：\n\n​\t称预测类别为1的为Positive（阳性），预测类别为0的为Negative（阴性）\n\n​\t预测正确的为True（真），预测错误的为False（伪）\n\n对上述概念进行组合，就产生了如下的混淆矩阵：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31zerkkb7j20id0dg3yy.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n然后，由此引出True Positive Rate（真阳率TPR）、False Positive Rate（伪阳率FPR）两个概念，计算方式如下：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31z2eftscj204p02z0sk.jpg)\n\n仔细观察上面的两个式子，发现两个式子的分子其实对应了混淆矩阵的第二行，即预测类别为1的那一行。另外可以发现TPR就是用TP除以TP所在的列，FPR就是用FP除以FP所在的列。二者的含义如下：\n\n- TPR代表在所有真实类别为1的样本中，预测类别为1的比例\n- FPR代表在所有真实类别为0的样本中，预测类别为1的比例\n\n如果我们计算出了TPR和FPR，那么ROC曲线的绘制就很简单了，ROC曲线的横轴是FPR、纵轴是TPR，当二者相等时，绘制出的曲线是一条直线，如下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31zevqc3pj20ky0gidg9.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n表示的意义是：对于不论真实类别是0还是1的样本，模型预测样本为1的概率都是相等的。\n换句话说，模型对正例和负例毫无区分能力，做决策和抛硬币没啥区别。因此，我们认为AUC的最小值为0.5（当然也存在预测相反这种极端的情况，AUC小于0.5，这种情况相当于分类器总是把对的说成错的，错的认为是对的，那么只要把预测类别取反，便得到了一个AUC大于0.5的分类器）。\n\n而我们希望模型达到的效果是：对于真实类别为1的样本，模型预测为1的概率（即TPR），要大于真实类别为0而预测类别为1的概率（即FPR），即y＞x，因此大部分的ROC曲线长成下面这个样子：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31zey6bt4j20kw0gijrw.jpg\" alt=\"img\" style=\"zoom: 50%;\" />\n\n最理想的情况下，既没有真实类别为1而错分为0的样本——TPR一直为1，也没有真实类别为0而错分为1的样本——FPR一直为0，AUC为1，这便是AUC的极大值。\n下面举一个小例子，以分类问题为例，预测类别为离散标签，假设8个样本的预测情况如下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31zf0x57dj20xc04dt8z.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n\n\n得到的混淆矩阵如下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31zf3r9uuj20gw094t8y.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n进而计算得到TPR=3/4，FPR=2/4，得到ROC曲线：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31zf5o9y0j20jg0eumxm.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n可以看到这实际上式两段直线组成的曲线，是因为我们只画出了一个关键点。\n如果对于CTR任务，预测的结果是一个概率值，那应该如何画出ROC曲线呢？比如预测结果如下：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31z4nawqsj20xc04fmxi.jpg)\n\n这时，需要设置阈值来得到混淆矩阵，不同的阈值会影响得到的TPR，FPR。如果阈值取0.5，小于0.5的为0，否则为1，那么我们就得到了与之前一样的混淆矩阵。其他的阈值就不再啰嗦了。依次使用所有预测值作为阈值，得到一系列TPR，FPR，然后画出关键点，再连线即可得到ROC曲线。\n因此，ROC曲线跟P-R曲线一样，也是通过不断地移动模型正样本阈值来生成的。\n\n- **AUC**\n\n  AUC（Area Under Curve）的意思是曲线下的面积。它通常被定义为[ROC曲线](https://links.jianshu.com/go?to=https%3A%2F%2Fbaike.baidu.com%2Fitem%2FROC%E6%9B%B2%E7%BA%BF%2F775606)下与[坐标轴](https://links.jianshu.com/go?to=https%3A%2F%2Fbaike.baidu.com%2Fitem%2F%E5%9D%90%E6%A0%87%E8%BD%B4%2F9763108)围成的[面积](https://links.jianshu.com/go?to=https%3A%2F%2Fbaike.baidu.com%2Fitem%2F%E9%9D%A2%E7%A7%AF%2F100551)，显然这个面积的数值不会大于1（但是这个曲线也不一定是ROC，也可以是前面提及的P-R曲线）。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。AUC越接近1.0，检测方法真实性越高;等于0.5时，则真实性最低，无应用价值。我们往往使用AUC值作为模型的评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。\n\n  综上，**AUC是衡量二分类模型优劣的一种评价指标，表示预测的正例排在负例前面的概率。**\n\n- **mAP**\n\n  平均精度均值（mean Average Precision， mAP）是另一个在推荐系统、信息领域中常用的评估指标。该指标其实是对平均精度（Average Precision，AP）的再次平均，因此在计算mAP之前，我们先了解一下什么是平均精度。\n\n  假设推荐系统对某一用户测试集的排序结果如下：\n\n| 推荐序列 | N=1  | N=2  | N=3  | N=4  | N=5  | N=6  |\n| -------- | ---- | ---- | ---- | ---- | ---- | ---- |\n| 真实标签 | 1    | 0    | 0    | 1    | 1    | 1    |\n\n其中，1代表正样本，0代表负样本。我们来计算下它们的Precision。如下表所示：\n\n| 推荐序列    | N=1  | N=2  | N=3  | N=4  | N=5  | N=6  |\n| ----------- | ---- | ---- | ---- | ---- | ---- | ---- |\n| 真实标签    | 1    | 0    | 0    | 1    | 1    | 1    |\n| Precision@N | 1/1  | 1/2  | 1/3  | 2/4  | 3/5  | 4/6  |\n\nAP的计算只取正样本处的Precision进行平均，即AP = (1/1+2/4+3/5+4/6)/4=0.6917。如果推荐系统对测试集中每个用户都进行样本排序，那么每个用户都会计算出一个AP值，再对所有用户的AP值进行平均，就得到了mAP。也就是说，mAP是对精确度平均的平均。\n值得注意的是，mAP的计算方法和P-R曲线、ROC曲线的计算方式完全不同，因为mAP需要对每个用户的样本进行分用户排序，而P-R曲线和ROC曲线均是对全量测试样本进行排序。\n\n# \n\n# 实例\n\n下面以一个经典的莺尾花分类的例子来展示各种指标的计算。\n导入莺尾花数据，使用Holdout检验，将数据集随机划分成训练集和测试集：\n\n\n\n```python\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Add noisy features\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# Limit to the two first classes, and split into training and test\nX_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n                                                    test_size=.5,\n                                                    random_state=random_state)\n```\n\n创建一个线性SVM分类器，计算测试数据到决策平面的距离以及对测试数据进行预测：\n\n\n\n```python\n# Create a simple classifier\nclassifier = svm.LinearSVC(random_state=random_state)\nclassifier.fit(X_train, y_train)\ny_score = classifier.decision_function(X_test)\ny_predict = classifier.predict(X_test)\n```\n\n计算准确率：\n\n\n\n```python\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_predict)\nprint(\"Accuracy: \", accuracy)\n```\n\n\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31z5xdv3pj208c01o0sj.jpg)\n\n计算精准率：\n\n\n\n```python\nfrom sklearn.metrics import precision_score\nprecision = precision_score(y_test, y_predict)\nprint(\"Precision: \", precision)\n```\n\n\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31z672ruxj208s01udfn.jpg)\n\n计算召回率：\n\n\n\n```python\nfrom sklearn.metrics import recall_score\nrecall = recall_score(y_test, y_predict)\nprint(\"Recall: \", recall)\n```\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31z6uhwbkj20d401ot8l.jpg)\n\n计算F1-Score：\n\n\n\n```python\nfrom sklearn.metrics import f1_score\nF1_score = f1_score(y_test, y_predict)\nprint(\"F1-score: \", F1_score)\n```\n\n\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31z7rbrxhj20em01smx1.jpg)\n\n计算精确率均值AP：\n\n\n\n```python\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_score)\nprint('Average precision: {0:0.2f}'.format(average_precision))\n```\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31z812ynpj20c401ojr8.jpg)\n\n计算混淆矩阵：\n\n\n\n```python\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_predict)\nprint(\"Confusion Matrix: \\n\", confusion_matrix)\n```\n\n\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31z8af21uj208m03ggli.jpg)\n\n绘制P-R曲线，并且计算AUC：\n\n\n\n```python\nfrom sklearn.metrics import precision_recall_curve, auc\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndisp = plot_precision_recall_curve(classifier, X_test, y_test)\ndisp.ax_.set_title('P-R Example')\n\nprecision, recall, _thresholds = precision_recall_curve(y_test, y_predict)\nauc = auc(recall, precision)\nprint(\"AUC: \", auc)\n```\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31zfd59juj20ue0ksq3p.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31z8tjgbcj20ge01yt8n.jpg)\n\n绘制ROC曲线并且计算AUC：\n\n\n\n```python\nfrom sklearn.metrics import roc_auc_score, auc, roc_curve\nimport matplotlib.pyplot as plt\n\nfpr, tpr, thresholds = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)  #auc为Roc曲线下的面积\n\n#开始画ROC曲线\nplt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.1])\nplt.ylim([-0.1,1.1])\nplt.xlabel('FPR') #横坐标是fpr\nplt.ylabel('TPR')  #纵坐标是tpr\nplt.title('ROC Example')\nplt.show()\n```\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31z99o8w6j20rc0jc0tf.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n# A/B测试与线上评估指标\n\n无论离线评估如何仿真线上环境，终究无法完全还原线上的所有变量。对几乎所有的互联网公司来说，线上A/B测试都是验证新模块、新功能、新产品是否有效的主要测试方法。\n\n## A/B测试\n\n\n\n\n\nA/B测试又称为“分流测试”或“分桶测试”，是一个随机实验，通常被分为实验组和对照组。在利用控制变量法保持单一变量的前提下，将A、B两组数据进行对比，得出实验结论。具体到互联网场景下的算法测试中，可以将用户随机分成实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以旧模型，比较实验组和对照组在各线上评估指标上的差异。可以由下图来展示：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h31z9vompuj20gt085glv.jpg)\n\n上图中用户被随机均分成两组，橘色和绿色代表被控制的变量，最右侧是转化率。通过这种方式可以看到，系统中单个变量对系统产生的整体影响。\n相对离线评估而言，线上A/B测试无法被替代的原因主要有以下三点：\n\n- **离线评估无法完全消除数据有偏现象的影响，因此得出的离线评估结果无法完全替代线上评估结果。**\n- **离线评估无法完全还原线上的工程环境。**一般来说，离线评估往往不考虑线上的延迟、数据丢失、标签缺失等情况。因此，离线评估环境只能说是理想状态下的工程环境，得出的评估结果存在一定的失真现象。\n- **线上系统的某些商业指标在离线评估中无法计算**。离线评估一般针对模型本身进行评估，无法直接获得与模型相关的其他指标，特别是商业指标。也新的推荐模型为例，离线评估关注的往往是ROC曲线、PR曲线等的改进，而线上评估可以全面了解该推荐模型带来的用户点击率、留存时长、PV访问量等的变化。这些都需要由A/B测试进行全面评估。\n\n\n\n## 线上A/B测试的评估指标\n\n一般来讲，A/B测试都是模型上线前的最后一道测试，通过A/B测试检验的模型将直接服务于线上用户，完成公司的商业目标。因此，A/B测试的指标与线上业务的核心指标保持一致。\n下表列出了电商类推荐模型、新闻类推荐模型、视频类推荐模型的线上A/B测试的主要评估指标：\n\n| 推荐系统类别   | 线上A/B测试评估指标                                          |\n| -------------- | ------------------------------------------------------------ |\n| 电商类推荐模型 | 点击率、转化率、客单价（用户平均消费金额）                   |\n| 新闻类推荐模型 | 留存率（x日后仍活跃的用户数/x日前的用户数）、平均停留时长、平均点击个数 |\n| 视频类推荐模型 | 播放完成率（播放时长/视频时长）、平均播放时长、播放总时长    |\n\n线上A/B测试的指标与离线评估指标有较大差异。离线评估不具备直接计算业务核心指标的条件，因此退而求其次，选择了偏向于技术评估的模型相关指标。但在公司层面，更关心能够驱动业务发展的核心指标。因此，在具备线上测试环境时，利用A/B测试验证模型对业务核心指标的提升效果是有必要的。从这个意义上讲，线上A/B测试的作用是离线评估无法替代的。\n\n\n\n# 参考\n\n- 《机器学习》 -- 周志华\n- 《推荐系统实战》-- 项亮\n- 《深度学习推荐系统》-- 王喆\n- https://www.jianshu.com/p/5df19746daf9\n- [https://www.zhihu.com/question/39840928](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F39840928)\n- [https://baike.baidu.com/item/AUC/19282953?fr=aladdin](https://links.jianshu.com/go?to=https%3A%2F%2Fbaike.baidu.com%2Fitem%2FAUC%2F19282953%3Ffr%3Daladdin)\n- [https://en.wikipedia.org/wiki/Precision_and_recall](https://links.jianshu.com/go?to=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FPrecision_and_recall)\n- [https://blog.csdn.net/bqw18744018044/article/details/81024520](https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fbqw18744018044%2Farticle%2Fdetails%2F81024520)\n- [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection](https://links.jianshu.com/go?to=https%3A%2F%2Fscikit-learn.org%2Fstable%2Fmodules%2Fclasses.html%23module-sklearn.model_selection)\n- [https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html](https://links.jianshu.com/go?to=https%3A%2F%2Fscikit-learn.org%2Fstable%2Fauto_examples%2Fmodel_selection%2Fplot_precision_recall.html)\n\n\n\n","tags":["评价指标","推荐系统"],"categories":["AI"]},{"title":"Pycharm远程连接服务器、配置SSH、配置py环境","url":"/2022/06/10/Pycharm远程连接服务器、配置SSH、配置py环境/","content":"\n\n\n# Pycharm远程连接服务器、配置SSH、配置py环境\n\n在这里我将配置python、pytorch到指定服务器，前提是需要知道远程服务器提供的ip、port、账号、密码。\n\n## 1 配置[SSH](https://so.csdn.net/so/search?q=SSH&spm=1001.2101.3001.7020)\n\n输入ip、port、账号、密码\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h333xx6lhij215m0u0tby.jpg\" alt=\"image-20220610135817289\" style=\"zoom:50%;\" />\n\n## 2 新建项目\n\n新建你想要的项目名称\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h333yn0cklj21400u0n0m.jpg\" alt=\"image-20220610135923520\" style=\"zoom:50%;\" />\n\n在`previously configured interpreter`右边的‘…’新建SSH Interpreter。\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h333zxjmqfj219r0u0taz.jpg\" alt=\"image-20220610140037859\" style=\"zoom:50%;\" />\n\n先使用默认的python环境\n\n<img src=\"https://image.baidu.com/search/down?url=https://img-blog.csdnimg.cn/087849741a584a21b55aa28bd8460046.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\" />\n\n确认后改一下remote project location\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3341bc5kcj20z00a675a.jpg\" alt=\"image-20220610140158018\" style=\"zoom: 67%;\" />\n\n在这里我设置的是`/usr/local/testSSH`\n\n## 3 设置[远程连接](https://so.csdn.net/so/search?q=远程连接&spm=1001.2101.3001.7020)配置\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3353rfmccj21fo0u0dmt.jpg)\n\n点一下autodetect，设置自己的根目录\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3354i9ew2j20zq0u0408.jpg\" alt=\"image-20220610143937733\" style=\"zoom: 67%;\" />\n\nMappings也改一下：\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h33551nb8qj20zq0u0taf.jpg\" alt=\"image-20220610144009512\" style=\"zoom:67%;\" />\n\nbrowse remote host可以帮助我们查看远程服务器的状态：\n\n<img src=\"https://image.baidu.com/search/down?url=https://img-blog.csdnimg.cn/8e3e5a53690841fab0eabbf5c98de505.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\" />\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h33572qry4j20d80akjrp.jpg)\n\n## 4 配置远程服务器的[虚拟环境](https://so.csdn.net/so/search?q=虚拟环境&spm=1001.2101.3001.7020)\n\n使用start SSH [session](https://so.csdn.net/so/search?q=session&spm=1001.2101.3001.7020)\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h33589t2ioj20m60s2gnj.jpg\" alt=\"image-20220610144305804\" style=\"zoom: 50%;\" />\n\n随后弹出的窗口，我们就很熟悉了\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3358sno1ij20t20ga3zd.jpg)\n\n随后就是正常的conda创建虚拟环境的方法，文末给出一个参考文献。\n\n> //一些常用的方法\n> //创建虚拟环境\n> conda create -n tfEnvi python=3.6\n> //进入虚拟环境\n> conda activate tfEnvi\n> //安装tensorflow\n> pip install tensorflow==2.1.0\n\n虚拟环境会在自己的源目录下，`.conda/envs`\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3359j918cj20do0a2aag.jpg)\n\n## 5 使用虚拟环境\n\nAdd…\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h3359xck8mj21h40j60vf.jpg)\n\n老样子，SSH Interpreter-〉Existing server configuration，这次Interptreter使用自己创建的环境，还有更改Sync folders。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h335a4dpx1j21ba0ecgne.jpg)\n\nApply-》ok。\n\n## 6 同步代码\n\n在你本地写好代码后，如果你要同步代码。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h335ay4rboj21aa0t2aet.jpg)\n\n按下按钮，成功上传至远程服务器。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h335b1iy0fj21rp0u0agu.jpg)\n\n最后，快乐的运行代码吧。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h335bd23k4j21hf0u00x3.jpg)","tags":["工具","pycharm","ssh"],"categories":["AI"]},{"title":"基于用户的协同过滤算法（UserCF）原理以及代码实践","url":"/2022/06/09/基于用户的协同过滤算法（UserCF）原理以及代码实践/","content":"\n\n\n# 推荐系统文档\n\n## 1、基于用户的协同过滤算法（UserCF）原理以及代码实践\n\n\n## 简介\n\n协同过滤（collaborative filtering）是一种在推荐系统中广泛使用的技术。该技术通过分析用户或者事物之间的相似性，来预测用户可能感兴趣的内容并将此内容推荐给用户。这里的相似性可以是人口特征的相似性，也可以是历史浏览内容的相似性，还可以是个人通过一定机制给与某个事物的回应。比如，A和B是无话不谈的好朋友，并且都喜欢看电影，那么协同过滤会认为A和B的相似度很高，会将A喜欢但是B没有关注的电影推荐给B，反之亦然。\n\n协同过滤推荐分为3种类型：\n\n- **基于用户(user-based)的协同过滤(UserCF)**\n- 基于物品(item-based)的协同过滤（ItemCF算法)\n- 基于模型(model-based)的协同过滤 (ModelCF算法)\n\n本文主要讲述基于用户协同过滤算法的原理以及代码实现。\n\n----------\n\n## 算法原理\n\nUserCF算法主要是考虑用户与用户之间的相似度，给用户推荐和他兴趣相似的其他用户喜欢的物品。俗话说\"物以群分，人以类聚\"，人们总是倾向于跟自己志同道合的人交朋友。同理，你朋友喜欢的东西你大概率也可能会喜欢，UserCF算法正是利用了这个原理。举个例子，如果要给一个用户A推荐物品，可以先找到与A最为相似的用户B，接着获取用户B最喜欢的且用户A没有听说过的物品，并预测用户A对这些物品的评分，从中选取评分最高的若干个物品推荐给用户A。\n\n从上述描述可以知道，UserCF算法的主要步骤如下：\n\n1. 找到与目标用户兴趣相似的用户集合\n2. 找到这个集合中的用户最喜欢的，且目标用户还未接触过的物品推荐给目标用户\n\n上述是UserCF算法的基本思路，方便读者形成对UserCF算法的整体印象。也许你看了上述文字依然没有思路，没关系，且继续往下看。\n\n首先，根据算法的步骤1，我们自然而然就会提出一个问题，那就是如何度量两个用户之间的相似度？试想一下，我们在现实生活中如何判断两个人是否兴趣相似呢？比如你喜欢打LOL，恰巧你室友也喜欢，那显然你们的共同话题会比较多，因为你们有共同的兴趣爱好。我们刚好可以利用这一点来计算用户之间的相似度。\n\n对于用户u和用户v，令N(u)代表用户u喜欢的物品合集，令N(v)代表用户v喜欢的物品合集。N(u)∩N(v)代表的是用户u和用户v都喜欢的物品，N(u)∪N(v)代表的是用户u和用户v喜欢的物品的合集，那么可以利用以下公式来简单计算相似度:\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvfinphsj206h02g744.jpg)\n\n上述公式叫做Jaccard公式，直观上理解就是，将用户u与用户v都喜欢的物品的数量除以他们喜欢物品的总和，如果u和v喜欢的物品是一模一样的，则u和v的相似度为1。\n\n还有另外一种余弦相似度计算公式:\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvfvghe1j206902n744.jpg)\n\n上述公式的分母部分代表的是u喜欢的物品的数量与v喜欢的物品的数量的乘积，而不再是他们之间的交集。\n\n举个简单例子来表明如何计算用户u和v之间的相似度，假如用户u和用户v喜欢的游戏如下表：\n\n| 用户 | 喜爱的游戏                     |\n| ---- | ------------------------------ |\n| u    | {英雄联盟, 王者荣耀，绝地求生} |\n| v    | {英雄联盟，和平精英}           |\n\n利用余弦相似度计算公式可以得到如下结果：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvg8rkjfj20h302pq32.jpg)\n\n故，我们可以计算得到用户u和用户v之间的相似度为：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvghyxulj201401l0o6.jpg)\n\n至此，我们已经可以计算任意两个用户之间的相似度了，下一步要做的事情是建立一张用户相似度表，此表中保存了任意两个用户之间的相似度，方便后续挑选出与用户u最相似的若干个用户。\n\n当用户相似度表建立起来了之后，UserCF算法就可以给用户推荐与他兴趣最相似的K个用户喜欢的物品了。那此时又会出现一个问题，假如我们要给用户w进行推荐，并且在用户相似度表中找到了与他最相似的K个用户，这K个用户喜欢的物品有很多，我们怎么知道要推荐哪些物品给用户w呢？此时就涉及到了用户对物品的感兴趣程度，我们当然会把用户最感兴趣的物品推荐给他。故使用以下公式来度量用户u对物品i的感兴趣程度：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvgs99ntj207w02amwz.jpg)\n\n乍一看感觉很复杂，其实不然。\n其中![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvhqfvojj201u00p3y9.jpg) 代表的是与用户![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvin6zanj200f00j0d0.jpg)最相似的K个用户，将与用户![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zviun6lrj200f00j0d0.jpg)相似的用户列表按照相似度进行排序就可以得到。![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvj7d3lzj201700t0oa.jpg)代表的是对喜欢物品i的用户集合，![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvjj17zij201d00s0mt.jpg)代表的是用户![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvin6zanj200f00j0d0.jpg)和用户![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvkb86m2j200i00k0by.jpg)之间的相似度，这个也可以直接从用户相似度表中得到。![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvkkid5jj201000p0k5.jpg)代表用户v对物品i的兴趣，因为使用的是单一行为的隐反馈数据，所以所有的![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvkueka8j201y00o0q4.jpg)。\n\n对于与用户![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvluu9tyj200f00j0d0.jpg)最相似的K个用户，我们分别计算用户![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvin6zanj200f00j0d0.jpg)与这K个用户喜欢的物品集合![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvljkeavj204u00vq2p.jpg)之间的感兴趣程度，得到用户![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvin6zanj200f00j0d0.jpg)对这N个物品的感兴趣程度列表，然后将其逆序排序，取前m个物品推荐给用户![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zvin6zanj200f00j0d0.jpg)，至此UserCF算法结束。\n\n--------------\n\n## 算法工作流程\n\n接下来，我们用一个简单例子演示一下UserCF的具体工作流程。\n下表是我们臆造的原始数据，也称之为User-Item表，即用户-物品列表，记录了每个用户喜爱的物品，数据表格如下：\n\n| 用户 | 喜爱的物品 |\n| ---- | ---------- |\n| A    | {a,b,d}    |\n| B    | {a,c,d}    |\n| C    | {b,e}      |\n| D    | {c,d,e}    |\n\n接下来我们需要计算用户相似度矩阵，最直观的方法就是，对于用户列表{A,B,C,D},我们对其中两两用户都使用余弦相似度算法计算相似度。这种算法的时间复杂度是O(N^2)，当用户量很大的时候，计算非常耗时,在实际运用中不可取。观察一下相似度计算公式会发现，其实如果用户![\\mu](https://math.jianshu.com/math?formula=%5Cmu)和用户![\\nu](https://math.jianshu.com/math?formula=%5Cnu)没有共同喜欢的物品（比如表中的用户B和用户C），那么他们之间的相似度为0，也就没有必要计算了。\n也就是说我们没必要对任意两个用户之间都计算相似度，我们只需要计算那些彼此之间有共同喜爱物品的用户之间的相似度，故首先可以想到建立倒排表，即Item-User表，具体如下：\n\n| 物品 | 喜爱它的用户 |\n| ---- | ------------ |\n| a    | {A,B}        |\n| b    | {A,C}        |\n| c    | {B,D}        |\n| d    | {A,B,D}      |\n| e    | {C,D}        |\n\n有了这张表之后，相当于我们将原先零散的用户按照他们喜爱的物品给划分成若干个小圈子。比如用户A、B由于都喜欢物品a，所以被分到了一起。同理，用户C、D都喜欢物品e，因此也被分到了一起。\n那这么做的目的是什么呢？回顾一下上面说的用户相似度计算方法，可以看到无论是哪一种方法都要先求出|N(![\\mu](https://math.jianshu.com/math?formula=%5Cmu))∩N(![\\nu](https://math.jianshu.com/math?formula=%5Cnu))|，所以我们的目的就是为了快速地求出任意用户之间的交集。\n由此可以建立下面的用户喜爱物品交集矩阵W：\n\n|      | A    | B    | C    | D    |\n| ---- | ---- | ---- | ---- | ---- |\n| A    | 0    | 2    | 1    | 1    |\n| B    | 2    | 0    | 0    | 2    |\n| C    | 1    | 0    | 0    | 1    |\n| D    | 1    | 2    | 1    | 0    |\n\n其中W[u][v]代表的含义是用户u和用户v都喜爱的物品个数，即![image-20220607123616571](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zkpewkkfj202s00pa9t.jpg)。比如用户A、C都喜欢物品b，所以W[A][C]=1。用户A、B除了喜欢物品a以外，还共同喜欢物品d，因此W[A][B]=2。\n由于用户A、B之间的交集和用户B、A之间的交集是一样的，所以此矩阵是一个对称矩阵。\n其实这里的W就是相似度计算公式中的分子部分，而每个用户的喜爱列表从之前的User-Item列表中就已经可以得到了，因此我们就可以计算出用户相似度矩阵了，结果如下：\n\n|      | A    | B    | C    | D    |\n| ---- | ---- | ---- | ---- | ---- |\n| A    | 0    | 0.67 | 0.41 | 0.33 |\n| B    | 0.67 | 0    | 0    | 0.67 |\n| C    | 0.41 | 0    | 0    | 0.41 |\n| D    | 0.33 | 0.67 | 0.41 | 0    |\n\n当用户相似度矩阵建立好了之后，我们就可以对用户进行物品推荐了。\n比如要对用户C进行物品推荐，通过查表可以知道（上表第四行），用户A和用户D是比较相似的两个用户。\n再通过User-Item表查询到用户A喜欢的物品列表{a,b,d}，用户D喜欢的物品列表{c,d,e}， 故用户A、D喜欢物品的交集是{a,b,c,d,e}，其中用户C喜欢的列表是{b,e}，为了避免重复推荐用户已经喜欢的物品，所以要先从物品列表中去掉用户C已经喜欢的物品，故最终待推荐的物品列表为{a,c,d}。\n此时我们来计算用户C对待推荐物品列表中物品的感兴趣程度：\n\np(C,a) = W [C] [A]= 0.41\n\n p(C,c) = W [C] [D] = 0.41\np(C,d) = W [C] [A] + W [C] [D] = 0.82\n\n接着按照用户C对待推荐物品感兴趣程度对待推荐列表进行逆序排序，得到最终的推荐列表{d,a,c}，我们可以将整个推荐列表或者取前K个物品推荐给用户C。\n至此，UserCF算法整体流程结束。\n可以看到，算法的输出的最应该推荐给用户C的物品是d，我们不难发现这是因为与用户C最相似的用户A和用户D都喜欢物品d，因此将d推荐给C是比较合理的选择。\n\n\n\n### 相似度算法改进\n\n上述算法使用的是余弦相似度计算公式，但是这个公式过于粗糙，原因是余弦相似度只是简单地计算了用户u和用户v共同喜欢的物品在他们总共喜欢物品中占据的比例。\n比如，两个用户都购买了时下热门物品，这并不能说明他们兴趣相似，因为热门物品是绝大多数人都会买的东西。换句话说，两个用户对冷门物品采用过相同的行为更能说明他们兴趣的相似度。因此John.S.Breese提出了以下相似度计算公式：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zkq425uzj208002jglh.jpg)\n\n相比余弦相似度公式，这里的分子![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2zkqkrpsaj203p01mwe9.jpg)惩罚了用户u和用户v共同喜欢的热门物品对他们相似度的影响。\n在下一节的代码实践中，分别实现了基于这两种相似度计算的UserCF算法。\n\n------------------\n\n## 代码实践\n\n上一节是基于一个非常简易的测试数据集，接下来我们将UserCF算法运用在推荐系统经典数据集MovieLens上。\nMovieLens数据集介绍以及数据处理参见[链接](https://www.jianshu.com/p/a59ff0dc22a3)。\n\n根据之前的介绍，可知整个算法流程分为两个阶段：\n\n- 训练阶段\n- 推荐阶段\n\n对于训练阶段，可分为以下几步：\n\n1. 数据预处理，建立User-Item表\n2. 建立Item-User倒排表\n3. 建立用户物品交集矩阵\n4. 建立用户相似度矩阵\n\n对于推荐阶段，可分为以下几步：\n\n1. 寻找与被推荐用户最相似的K个用户\n2. 计算用户对物品的感兴趣列表并逆序排列\n\n### 训练阶段\n\n#### 数据预处理，建立User-Item表\n\n我们采用MovieLens数据集中的ratings.dat文件，因为这里面包含了用户对电影的评分数据，注意我们忽略掉评分那一栏，将其简化成用户喜欢或者不喜欢。只要用户有参与过评分的电影，无论分值如何，我们都认为这部电影是用户喜欢的。\n\n**注意，ratings.dat里面包含了100多万条评价数据，为了减少训练时间，可以只读取部分数据，本文读取了前29415条数据，即前200个用户的评价数据。ratings.dat原始数据每行包含了4列，本文中只取了’UserID‘、’MovieID‘这两列，关于此数据集的详细介绍请参见[链接](https://www.jianshu.com/p/a59ff0dc22a3)。**\n\n接下来使用以下代码来读取数据并建立User-Item表：\n\n\n\n```python\nimport random\nimport pandas as pd\n\ndef LoadMovieLensData(filepath, train_rate):\n    ratings = pd.read_table(filepath, sep=\"::\", header=None, names=[\"UserID\", \"MovieID\", \"Rating\", \"TimeStamp\"],\\\n                            engine='python')\n    ratings = ratings[['UserID','MovieID']]\n    train = []\n    test = []\n    random.seed(3)\n    for idx, row in ratings.iterrows():\n        user = int(row['UserID'])\n        item = int(row['MovieID'])\n        if random.random() < train_rate:\n            train.append([user, item])\n        else:\n            test.append([user, item])\n    return PreProcessData(train), PreProcessData(test)\n\ndef PreProcessData(originData):\n    \"\"\"\n    建立User-Item表，结构如下：\n        {\"User1\": {MovieID1, MoveID2, MoveID3,...}\n         \"User2\": {MovieID12, MoveID5, MoveID8,...}\n         ...\n        }\n    \"\"\"\n    trainData = dict()\n    for user, item in originData:\n        trainData.setdefault(user, set())\n        trainData[user].add(item)\n    return trainData\n```\n\n#### 建立Item-User倒排表\n\n这里使用了python的dict里面嵌套set来实现倒排表，伪代码如下：\n\n```python\ndef UserItemTable(userItemTab):\n    \"\"\"\n    建立User-Item倒排表\n    :param userItemTab: user-item表\n    :return:\n    \"\"\"\n    item_user = dict()\n    for user, items in userItemTab.items():\n        for item in items:\n            item_user.setdefault(item, set())\n            item_user[item].add(user)\n```\n\n#### 建立用户物品交集矩阵\n\n同理，保存用户物品交集的矩阵采用了双重dict来实现。\n\n\n\n```python\ndef UserInterSection(item_user):\n    \"\"\"\n    建立用户物品交集矩阵W, 其中C[u][v]代表的含义是用户u和用户v之间共同喜欢的物品数\n    :param item_user: item_user 倒排表\n    \"\"\"\n    userInterSection = dict()\n    for item, users in item_user.items():\n        for u in users:\n            for v in users:\n                if u == v:\n                    continue\n                userInterSection.setdefault(u, defaultdict(int))\n                userInterSection[u][v] += 1  # 将用户u和用户v共同喜欢的物品数量加一\n```\n\n#### 建立用户相似度矩阵\n\n用户相似度矩阵只需要在用户物品交集矩阵的基础上除以用户u和用户v各自喜爱物品列表数量的乘积。\n\n\n\n```python\ndef UserSimMatrix(userItemTab, userInterSection):\n    \"\"\"\n    建立用户相似度矩阵\n    :param userItemTab: User-Item表\n    :param userInterSection: 用户物品交集矩阵\n    :return: \n    \"\"\"\n    userSimMatrix = dict() #用户相似度矩阵\n    for u, related_user in userInterSection.items():\n        for v, cuv in related_user.items():\n            nu = len(userItemTab[u])\n            nv = len(userItemTab[v])\n            userSimMatrix[u][v] = cuv / math.sqrt(nu * nv)\n```\n\n### 推荐阶段\n\n下面代码实现了推荐的功能，函数最后会返回一个逆序排列的dict，里面包含了UserCF算法筛选出来的推荐物品：\n\n\n\n```python\n    def recommend(self, user, N, K):\n        \"\"\"\n        用户u对物品i的感兴趣程度：\n            p(u,i) = ∑WuvRvi\n            其中Wuv代表的是u和v之间的相似度， Rvi代表的是用户v对物品i的感兴趣程度，因为采用单一行为的隐反馈数据，所以Rvi=1。\n            所以这个表达式的含义是，要计算用户u对物品i的感兴趣程度，则要找到与用户u最相似的K个用户，对于这k个用户喜欢的物品且用户u\n            没有反馈的物品，都累加用户u与用户v之间的相似度。\n        :param user: 被推荐的用户user\n        :param N: 推荐的商品个数\n        :param K: 查找的最相似的用户个数\n        :return: 按照user对推荐物品的感兴趣程度排序的N个商品\n        \"\"\"\n        recommends = dict()\n        # 先获取user喜欢的item数组\n        related_items = self._trainData[user]\n        # 将其他用户与user按照相似度逆序排序之后取前K个\n        for v, sim in sorted(self._userSimMatrix[user].items(), key=itemgetter(1), reverse=True)[:K]:\n            # 从与user相似的用户的喜爱列表中寻找可能的物品进行推荐\n            for item in self._trainData[v]:\n                # 如果与user相似的用户喜爱的物品与user喜欢的物品重复了，直接跳过\n                if item in related_items:\n                    continue\n                recommends.setdefault(item, 0.)\n                recommends[item] += sim\n        # 根据被推荐物品的相似度逆序排列，然后推荐前N个物品给到用户\n        return dict(sorted(recommends.items(), key=itemgetter(1), reverse=True)[:N])\n```\n\n\n\n### 完整代码\n\n下面的代码实现了完整的功能，修改“ratings.dat\"文件的路径之后，可以直接运行。\n\n```python\nimport math\nimport random\nimport pandas as pd\nfrom Utils import modelsave\nfrom collections import defaultdict\nfrom operator import itemgetter\n\ndef LoadMovieLensData(filepath, train_rate):\n    ratings = pd.read_table(filepath, sep=\"::\", header=None, names=[\"UserID\", \"MovieID\", \"Rating\", \"TimeStamp\"],\\\n                            engine='python')\n    ratings = ratings[['UserID','MovieID']]\n\n    train = []\n    test = []\n    random.seed(3)\n    for idx, row in ratings.iterrows():\n        user = int(row['UserID'])\n        item = int(row['MovieID'])\n        if random.random() < train_rate:\n            train.append([user, item])\n        else:\n            test.append([user, item])\n    return PreProcessData(train), PreProcessData(test)\n\ndef PreProcessData(originData):\n    \"\"\"\n    建立User-Item表，结构如下：\n        {\"User1\": {MovieID1, MoveID2, MoveID3,...}\n         \"User2\": {MovieID12, MoveID5, MoveID8,...}\n         ...\n        }\n    \"\"\"\n    trainData = dict()\n    for user, item in originData:\n        trainData.setdefault(user, set())\n        trainData[user].add(item)\n    return trainData\n\nclass UserCF(object):\n    \"\"\" User based Collaborative Filtering Algorithm Implementation\"\"\"\n    def __init__(self, trainData, similarity=\"cosine\"):\n        self._trainData = trainData\n        self._similarity = similarity\n        self._userSimMatrix = dict() # 用户相似度矩阵\n\n    def similarity(self):\n        # 建立User-Item倒排表\n        item_user = dict()\n        for user, items in self._trainData.items():\n            for item in items:\n                item_user.setdefault(item, set())\n                item_user[item].add(user)\n\n        # 建立用户物品交集矩阵W, 其中C[u][v]代表的含义是用户u和用户v之间共同喜欢的物品数\n        for item, users in item_user.items():\n            for u in users:\n                for v in users:\n                    if u == v:\n                        continue\n                    self._userSimMatrix.setdefault(u, defaultdict(int))\n                    if self._similarity == \"cosine\":\n                        self._userSimMatrix[u][v] += 1 #将用户u和用户v共同喜欢的物品数量加一\n                    elif self._similarity == \"iif\":\n                        self._userSimMatrix[u][v] += 1. / math.log(1 + len(users))\n\n        # 建立用户相似度矩阵\n        for u, related_user in self._userSimMatrix.items():\n            # 相似度公式为 |N[u]∩N[v]|/sqrt(N[u]||N[v])\n            for v, cuv in related_user.items():\n                nu = len(self._trainData[u])\n                nv = len(self._trainData[v])\n                self._userSimMatrix[u][v] = cuv / math.sqrt(nu * nv)\n\n    def recommend(self, user, N, K):\n        \"\"\"\n        用户u对物品i的感兴趣程度：\n            p(u,i) = ∑WuvRvi\n            其中Wuv代表的是u和v之间的相似度， Rvi代表的是用户v对物品i的感兴趣程度，因为采用单一行为的隐反馈数据，所以Rvi=1。\n            所以这个表达式的含义是，要计算用户u对物品i的感兴趣程度，则要找到与用户u最相似的K个用户，对于这k个用户喜欢的物品且用户u\n            没有反馈的物品，都累加用户u与用户v之间的相似度。\n        :param user: 被推荐的用户user\n        :param N: 推荐的商品个数\n        :param K: 查找的最相似的用户个数\n        :return: 按照user对推荐物品的感兴趣程度排序的N个商品\n        \"\"\"\n        recommends = dict()\n        # 先获取user具有正反馈的item数组\n        related_items = self._trainData[user]\n        # 将其他用户与user按照相似度逆序排序之后取前K个\n        for v, sim in sorted(self._userSimMatrix[user].items(), key=itemgetter(1), reverse=True)[:K]:\n            # 从与user相似的用户的喜爱列表中寻找可能的物品进行推荐\n            for item in self._trainData[v]:\n                # 如果与user相似的用户喜爱的物品与user喜欢的物品重复了，直接跳过\n                if item in related_items:\n                    continue\n                recommends.setdefault(item, 0.)\n                recommends[item] += sim\n        # 根据被推荐物品的相似度逆序排列，然后推荐前N个物品给到用户\n        return dict(sorted(recommends.items(), key=itemgetter(1), reverse=True)[:N])\n\n    def train(self):\n        self.similarity()\n\n\nif __name__ == \"__main__\":\n    train, test = LoadMovieLensData(\"../Data/ml-1m/ratings.dat\", 0.8)\n    print(\"train data size: %d, test data size: %d\" % (len(train), len(test)))\n    UserCF = UserCF(train)\n    UserCF.train()\n\n    # 分别对测试集中的前4个用户进行电影推荐\n    print(UserCF.recommend(list(test.keys())[0], 5, 80))\n    print(UserCF.recommend(list(test.keys())[1], 5, 80))\n    print(UserCF.recommend(list(test.keys())[2], 5, 80))\n    print(UserCF.recommend(list(test.keys())[3], 5, 80))\n```\n\n完整代码见[https://github.com/HeartbreakSurvivor/RsAlgorithms/blob/main/Test/usercf_test.py](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FHeartbreakSurvivor%2FRsAlgorithms%2Fblob%2Fmain%2FTest%2Fusercf_test.py)。\n\n## 参考\n\n- [协同过滤--维基百科](https://links.jianshu.com/go?to=https%3A%2F%2Fzh.wikipedia.org%2Fwiki%2F%E5%8D%94%E5%90%8C%E9%81%8E%E6%BF%BE%23%E5%9F%BA%E4%BA%8E%E9%A1%B9%E7%9B%AE%EF%BC%88Item-based%EF%BC%89%E7%9A%84%E5%8D%94%E5%90%8C%E9%81%8E%E6%BF%BE)\n- 《推荐系统实战》-- 项亮\n- https://www.jianshu.com/p/a59ff0dc22a3\n- 本机代码路径：/Users/zhiqiang/Downloads/RsAlgorithms-main/UserCF/usercf.py","tags":["UserCF"],"categories":["AI"]},{"title":"基于物品的协同过滤算法（ItemCF）原理以及代码实践","url":"/2022/06/08/基于物品的协同过滤算法（ItemCF）原理以及代码实践/","content":"\n\n\n# 推荐系统文档\n\n## 1、基于物品的协同过滤算法（ItemCF）原理以及代码实践\n\n\n## 简介\n\n协同过滤（collaborative filtering）是一种在推荐系统中广泛使用的技术。该技术通过分析用户或者事物之间的相似性，来预测用户可能感兴趣的内容并将此内容推荐给用户。这里的相似性可以是人口特征的相似性，也可以是历史浏览内容的相似性，还可以是个人通过一定机制给与某个事物的回应。比如，A和B是无话不谈的好朋友，并且都喜欢看电影，那么协同过滤会认为A和B的相似度很高，会将A喜欢但是B没有关注的电影推荐给B，反之亦然。\n\n协同过滤推荐分为3种类型：\n\n- 基于用户(user-based)的协同过滤(UserCF)\n- **基于物品(item-based)的协同过滤（ItemCF算法)**\n- 基于模型(model-based)的协同过滤 (ModelCF算法)\n\n本文主要讲述基于用户协同过滤算法的原理以及代码实现。\n\n----------\n\n## 算法原理\n\nItemCF算法是目前业界使用最广泛的算法之一，亚马逊、Netflix、YouTube的推荐算法的基础都是基于ItemCF。\n不知道大家平时在网上购物的时候有没有这样的体验，比如你在网上商城下单了一个手机，在订单完成的界面，网页会给你推荐同款手机的手机壳，你此时很可能就会点进去浏览一下，顺便买一个手机壳。其实这就是ItemCF算法在背后默默工作。ItemCF算法给用户推荐那些和他们之前喜欢的物品相似的物品。因为你之前买了手机，ItemCF算法计算出来手机壳与手机之间的相似度较大，所以给你推荐了一个手机壳，这就是它的工作原理。看起来是不是跟UserCF算法很相似是不是？只不过这次不再是计算用户之间的相似度，而是换成了计算物品之间的相似度。\n\n由上述描述可以知道ItemCF算法的主要步骤如下：\n\n1. 计算物品之间的相似度\n2. 根据物品的相似度和用户的历史行为给用户生成推荐列表\n\n那么摆在我们面前的第一个问题就是如何计算物品之间的相似度，这里尤其要特别注意一下：\n\n**ItemCF算法并不是直接根据物品本身的属性来计算相似度，而是通过分析用户的行为来计算物品之间的相似度。**\n\n什么意思呢？比如手机和手机壳，除了形状相似之外没有什么其它的相似点，直接计算相似度似乎也无从下手。但是换个角度来考虑这个问题，如果有很多个用户在买了手机的同时，又买了手机壳，那是不是可以认为手机和手机壳比较相似呢？\n由此引出物品相似度的计算公式：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30me16ugnj206f02jjr7.jpg)\n\n上式中，分母![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30meh4pfxj201j00w0pq.jpg)表示的是喜欢物品![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mezccogj200i00n0dh.jpg)的用户的数量，分子![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mfedmklj203g010mwx.jpg)是同时喜欢物品![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mezccogj200i00n0dh.jpg)和![](https://tva1.sinaimg.cn/large/e6c9d24ely1h30mga5ra8j200i00k0co.jpg)的用户数量。因此上式可以理解成，喜欢![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mezccogj200i00n0dh.jpg)的用户当中有多少也喜欢![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mga5ra8j200i00k0co.jpg)的，如果喜欢![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mezccogj200i00n0dh.jpg)的都喜欢![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mga5ra8j200i00k0co.jpg)，那么![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mj4bp7sj202a00r0sh.jpg)。\n\n上述公式看起来还是很合理的，但是如果物品![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mga5ra8j200i00k0co.jpg)很热门，很多人都喜欢，那么上式子中分子与分母就会很接近，此时![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mk1vzyuj201f00t0pv.jpg)就会很接近1。即任何商品都和热门商品之间的相似度很高，这会导致ItemCF算法会总是推荐热门商品，这并不是一个好的设计。因此可以采用下面的公式：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30ml4s45aj207b02ka9w.jpg)\n\n上式的改进在于，如果![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mm15if9j201900u0op.jpg)很大的话，分母也会相应变大，相当于惩罚了物品![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mga5ra8j200i00k0co.jpg)的权值，减轻了热门物品会和很多其他物品相似的可能性。在建立起了物品的相似度矩阵之后，与UserCF算法一样，我们也要面临一个问题，就是如何从众多相似的物品中挑选出用户最感兴趣的物品。因此ItemCF算法通过以下公式计算用户u对物品j的感兴趣程度：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mn7rgh6j207z02dmwz.jpg)\n\n上式中的![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mnjndxfj201900s0pg.jpg)是用户喜欢的物品集合，![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mntpik1j201o00q3y9.jpg)是和物品j最相似的K个物品的集合，![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mo4gu25j200x00u0ky.jpg)是物品j和i的相似度，![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mocbi9nj200v00p0ht.jpg)是用户![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mezccogj200i00n0dh.jpg)对物品i的兴趣（对于隐反馈数据集，如果用户![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mezccogj200i00n0dh.jpg)对物品i有过行为，即可令![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mpdfvttj201v00t0qb.jpg)\n\n对于上式的通俗理解就是，对于用户![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mezccogj200i00n0dh.jpg)喜欢的物品列表中的每一个物品i，都根据物品相似度矩阵找到与其最相似的K个物品，令为![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mr4n76vj204j00vt8h.jpg)然后使用物品相似度![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mrl6ebxj200v00n0it.jpg)来表示用户对物品j的感兴趣程度。\n最后将筛选出来的物品按照用户对其感兴趣程度逆序排序，取全体列表或者列表前K个物品推荐给用户，至此ItemCF算法完成。\n\n-------------------\n\n\n\n## 算法工作流程\n\n接下来我们用一个简单例子演示一下ItemCF的具体工作流程。\n下表是一个简易的原始数据集，也称之为User-Item表，即用户-物品列表，记录了每个用户喜爱的物品，数据表格如下：\n\n| 用户 | 喜爱的物品 |\n| ---- | ---------- |\n| A    | {a,b,d}    |\n| B    | {b,c,e}    |\n| C    | {c,d}      |\n| D    | {b,c,d}    |\n| E    | {a,d}      |\n\n首先，我们需要根据物品相似度公式建立物品相似度矩阵。\n\n\n\n先看公式的分子部分\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30oag7wtrj203r00smwx.jpg)\n\n也就是我们需要先求出同时喜欢物品![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mezccogj200i00n0dh.jpg)和![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mga5ra8j200i00k0co.jpg)的用户数量，这一点可以根据User-Item表快速构建出来，下面以用户A为例，展示下构建过程：\n用户A的喜欢物品列表是{a,b,d}，我们对这个列表的成员进行两两组合，可以得到下面的矩阵：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mu0w2mxj208w084mx8.jpg)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t用户A的共现矩阵\n\n上图是用户A的共现矩阵C，C[i][j]代表的含义是同时喜欢物品i和物品j的用户数量。举个例子，C[a][b]=1代表的含义就是同时喜欢物品a和物品b的用户有一个，就是用户A。\n由于同时喜欢物品i和物品j的人与同时喜欢物品j和物品i的人数是一样的，所以这是一个对称矩阵。\n同样的道理，其他用户也都有一张类似的表格。因为不同的用户的喜爱物品列表不一致，会导致每个用户形成的共现矩阵大小不一，这里为了统一起见，将矩阵大小扩充到nxn,其中n是所有用户喜欢物品列表的并集的大小。\n\n下面用一张图来描述如何构建完整的共现矩阵：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30muobn7pj20xc0hjdif.jpg)\n\n通过对不同用户的喜爱物品集合构成的共现矩阵进行累加，最终得到了上图中的矩阵C，这其实就是相似度公式中的分子部分。\n\n> 这里只是为了演示如何计算物品整体共现矩阵，实际在代码中我们并不会采用分别为每个用户建立一个共现矩阵再累加的方式。为了加快效率和节省空间，可以使用python的dict来实现，具体可以参考代码实现章节。\n\n接下来我们计算最终的物品相似度矩阵，以物品a和物品b的相似度计算为例，通过上面计算的计算可知C[a][b]=1，即同时喜欢物品a和物品b的用户有一位。根据User-Item表可以统计出N(a)=2，N(b)=3，那么物品a和物品b的相似度![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mv9w2ynj200x00p0k1.jpg)计算如下：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mvl91bmj20iq04074b.jpg)\n\n了物品相似度矩阵W之后，我们对用户进行物品推荐了，下面以给用户C推荐物品为例，展示一下计算过程：\n可以看到用户C喜欢的物品列表为{c,d}，通过查物品相似度矩阵可知，与物品c相似的有{b,d,e}，同理与物品d相似的物品有{a,b,c}，故最终给用户C推荐的物品列表为{a,b,e}**(注意这里并不是{a,b,c,d,e}，因为要去掉用户C喜欢的物品，防止重复推荐)**。接下来分别计算用户C对这些物品的感兴趣程度。\n\nP(C,a) = W [d] [a] = 0.71\nP(C,b) = W [c] [b] + W [d] [b]= 0.67 + 0.58 = 1.25\nP(C,e) = W [c] [e] = 0.58\n\n故给用户C的推荐列表是{b,a,e}。\n\n\n\n### 相似度算法改进\n\n从前面的讨论可以看到，在协同过滤中两个物品产生相似度是因为它们共同出现在很多用户的兴趣列表中。换句话说，每个用户的兴趣列表都对物品的相似度产生贡献。那么是不是每个用户的贡献都相同呢?\n假设有这么一个用户，他是开书店的，并且买了当当网上80%的书准备用来自己卖。那么他的购物车里包含当当网80%的书。假设当当网有100万本书，也就是说他买了80万本。从前面对ItemCF的讨论可以看到，这意味着因为存在这么一个用户，有80万本书两两之间就产生了相似 度，也就是说，内存里即将诞生一个80万乘80万的稠密矩阵。\n\n\n\nJohn S. Breese在论文1中提出了一个称为IUF(Inverse User Frequence)，即用户活跃度对数的 倒数的参数，他也认为活跃用户对物品相似度的贡献应该小于不活跃的用户，他提出应该增加IUF 参数来修正物品相似度的计算公式:\n\n\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mweretlj20gf04nq30.jpg)\n\n上述公式对活跃用户做了一种软性的惩罚，但是对于很多过于活跃的用户，比如上面那位买了当当网80%图书的用户，为了避免相似度矩阵过于稠密，我们在实际计算中一般直接忽略他的兴趣列表，而不将其纳入到相似度计算的数据集中。\n\n### 相似度矩阵归一化处理\n\nKarypis在研究中发现如果将ItemCF的相似度矩阵按最大值归一化，可以提高推荐的准确率。其研究表明，如果已经得到了物品相似度矩阵w，那么可以用如下公式得到归一化之后的相似度矩阵w'：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30mwqgnx6j205202i742.jpg)\n\n实验表明，归一化的好处不仅仅在于增强推荐的准确度，还可以提高推荐的覆盖率和多样性。\n\n## 代码实践\n\n上一节是基于一个非常简易的测试数据集，接下来我们将UserCF算法运用在推荐系统经典数据集MovieLens上。\nMovieLens数据集介绍以及数据处理参见[链接](https://www.jianshu.com/p/a59ff0dc22a3)。\n\n根据之前的介绍，可知整个算法流程分为两个阶段：\n\n- 训练阶段\n- 推荐阶段\n\n对于训练阶段，可分为以下几步：\n\n1. 数据预处理，建立User-Item表\n2. 建立物品整体共现矩阵\n3. 建立物品相似度矩阵\n\n对于推荐阶段，可分为以下几步：\n\n1. 寻找与被推荐用户喜爱物品集最相似的N个物品\n2. 计算用户对这N个物品的感兴趣程序列表并逆序排列\n\n### 训练阶段\n\n#### 数据预处理，建立User-Item表\n\n我们采用MovieLens数据集中的ratings.dat文件，因为这里面包含了用户对电影的评分数据，注意我们忽略掉评分那一栏，将其简化成用户喜欢或者不喜欢。只要用户有参与过评分的电影，无论分值如何，我们都认为这部电影是用户喜欢的。\n\n**注意，ratings.dat里面包含了100多万条评价数据，为了减少训练时间，可以只读取部分数据，本文读取了前29415条数据，即前200个用户的评价数据。ratings.dat原始数据每行包含了4列，本文中只取了’UserID‘、’MovieID‘这两列，关于此数据集的详细介绍请参见[链接](https://www.jianshu.com/p/a59ff0dc22a3)。**\n\n接下来使用以下代码来读取数据并建立User-Item表：\n\n\n\n```python\nimport random\nimport pandas as pd\n\ndef LoadMovieLensData(filepath, train_rate):\n    ratings = pd.read_table(filepath, sep=\"::\", header=None, names=[\"UserID\", \"MovieID\", \"Rating\", \"TimeStamp\"],\\\n                            engine='python')\n    ratings = ratings[['UserID','MovieID']]\n    train = []\n    test = []\n    random.seed(3)\n    for idx, row in ratings.iterrows():\n        user = int(row['UserID'])\n        item = int(row['MovieID'])\n        if random.random() < train_rate:\n            train.append([user, item])\n        else:\n            test.append([user, item])\n    return PreProcessData(train), PreProcessData(test)\n\ndef PreProcessData(originData):\n    \"\"\"\n    建立User-Item表，结构如下：\n        {\"User1\": {MovieID1, MoveID2, MoveID3,...}\n         \"User2\": {MovieID12, MoveID5, MoveID8,...}\n         ...\n        }\n    \"\"\"\n    trainData = dict()\n    for user, item in originData:\n        trainData.setdefault(user, set())\n        trainData[user].add(item)\n    return trainData\n```\n\n#### 建立物品整体共现矩阵\n\n这里并没有采用对每个用户都建立共现矩阵再累加的方式，而是直接采用了两重dict来实现一个Matrix，然后在此基础上直接建立共现矩阵。\n\n\n\n```python\ndef ItemMatrix(trainData, similarity):\n    \"\"\"\n    建立物品共现矩阵\n    :param trainData: User-Item表 \n    :param similarity: 相似度计算函数选择\n    :return: \n    \"\"\"\n    N = defaultdict(int)  # 记录每个物品的喜爱人数\n    itemSimMatrix = defaultdict(int) # 共现矩阵\n    for user, items in trainData.items():\n        for i in items:\n            itemSimMatrix.setdefault(i, dict())\n            N[i] += 1\n            for j in items:\n                if i == j:\n                    continue\n                itemSimMatrix[i].setdefault(j, 0)\n                if similarity == \"cosine\":\n                    itemSimMatrix[i][j] += 1\n                elif similarity == \"iuf\":\n                    itemSimMatrix[i][j] += 1. / math.log1p(len(items) * 1.)\n    return itemSimMatrix\n```\n\n#### 建立物品相似度矩阵\n\n这一步是是直接在物品共现矩阵的基础上除以两个物品各自喜爱人数的乘积，并且包含了数据归一化的处理。\n\n\n\n```python\ndef ItemSimilarityMatrix(ItemMatrix, N, isNorm):\n    \"\"\"\n    计算物品相似度矩阵\n    :param ItemMatrix: \n    :param N: \n    :param isNorm: \n    :return: \n    \"\"\"\n    itemSimMatrix = dict()\n    for i, related_items in ItemMatrix.items():\n        for j, cij in related_items.items():\n            # 计算相似度\n            itemSimMatrix[i][j] = cij / math.sqrt(N[i] * N[j])\n    # 是否要标准化物品相似度矩阵\n    if isNorm:\n        for i, relations in itemSimMatrix.items():\n            max_num = relations[max(relations, key=relations.get)]\n            # 对字典进行归一化操作之后返回新的字典\n            itemSimMatrix[i] = {k: v / max_num for k, v in relations.items()}\n    return itemSimMatrix\n```\n\n### 推荐阶段\n\n\n\n```python\ndef recommend(trainData, itemSimMatrix, user, N, K):\n    \"\"\"\n    :param trainData: User-Item表\n    :param itemSimMatrix: 物品相似度矩阵\n    :param user: 被推荐的用户user\n    :param N: 推荐的商品个数\n    :param K: 查找的最相似的用户个数\n    :return: 按照user对推荐物品的感兴趣程度排序的N个商品\n    \"\"\"\n    recommends = dict()\n    # 先获取user的喜爱物品列表\n    items = trainData[user]\n    for item in items:\n        # 对每个用户喜爱物品在物品相似矩阵中找到与其最相似的K个\n        for i, sim in sorted(itemSimMatrix[item].items(), key=itemgetter(1), reverse=True)[:K]:\n            if i in items:\n                continue  # 如果与user喜爱的物品重复了，则直接跳过\n            recommends.setdefault(i, 0.)\n            recommends[i] += sim\n    # 根据被推荐物品的相似度逆序排列，然后推荐前N个物品给到用户\n    return dict(sorted(recommends.items(), key=itemgetter(1), reverse=True)[:N])\n```\n\n### 完整代码\n\n下面的代码实现了完整的功能，修改“ratings.dat\"文件的路径之后，可以直接运行。\n\n\n\n```python\nimport math\nimport random\nimport pandas as pd\nfrom collections import defaultdict\nfrom operator import itemgetter\n\ndef LoadMovieLensData(filepath, train_rate):\n    ratings = pd.read_table(filepath, sep=\"::\", header=None, names=[\"UserID\", \"MovieID\", \"Rating\", \"TimeStamp\"],\\\n                            engine='python')\n    ratings = ratings[['UserID','MovieID']]\n\n    train = []\n    test = []\n    random.seed(3)\n    for idx, row in ratings.iterrows():\n        user = int(row['UserID'])\n        item = int(row['MovieID'])\n        if random.random() < train_rate:\n            train.append([user, item])\n        else:\n            test.append([user, item])\n    return PreProcessData(train), PreProcessData(test)\n\ndef PreProcessData(originData):\n    \"\"\"\n    建立User-Item表，结构如下：\n        {\"User1\": {MovieID1, MoveID2, MoveID3,...}\n         \"User2\": {MovieID12, MoveID5, MoveID8,...}\n         ...\n        }\n    \"\"\"\n    trainData = dict()\n    for user, item in originData:\n        trainData.setdefault(user, set())\n        trainData[user].add(item)\n    return trainData\n\n\nclass ItemCF(object):\n    \"\"\" Item based Collaborative Filtering Algorithm Implementation\"\"\"\n    def __init__(self, trainData, similarity=\"cosine\", norm=True):\n        self._trainData = trainData\n        self._similarity = similarity\n        self._isNorm = norm\n        self._itemSimMatrix = dict() # 物品相似度矩阵\n\n    def similarity(self):\n        N = defaultdict(int) #记录每个物品的喜爱人数\n        for user, items in self._trainData.items():\n            for i in items:\n                self._itemSimMatrix.setdefault(i, dict())\n                N[i] += 1\n                for j in items:\n                    if i == j:\n                        continue\n                    self._itemSimMatrix[i].setdefault(j, 0)\n                    if self._similarity == \"cosine\":\n                        self._itemSimMatrix[i][j] += 1\n                    elif self._similarity == \"iuf\":\n                        self._itemSimMatrix[i][j] += 1. / math.log1p(len(items) * 1.)\n        for i, related_items in self._itemSimMatrix.items():\n            for j, cij in related_items.items():\n                self._itemSimMatrix[i][j] = cij / math.sqrt(N[i]*N[j])\n        # 是否要标准化物品相似度矩阵\n        if self._isNorm:\n            for i, relations in self._itemSimMatrix.items():\n                max_num = relations[max(relations, key=relations.get)]\n                # 对字典进行归一化操作之后返回新的字典\n                self._itemSimMatrix[i] = {k : v/max_num for k, v in relations.items()}\n\n    def recommend(self, user, N, K):\n        \"\"\"\n        :param user: 被推荐的用户user\n        :param N: 推荐的商品个数\n        :param K: 查找的最相似的用户个数\n        :return: 按照user对推荐物品的感兴趣程度排序的N个商品\n        \"\"\"\n        recommends = dict()\n        # 先获取user的喜爱物品列表\n        items = self._trainData[user]\n        for item in items:\n            # 对每个用户喜爱物品在物品相似矩阵中找到与其最相似的K个\n            for i, sim in sorted(self._itemSimMatrix[item].items(), key=itemgetter(1), reverse=True)[:K]:\n                if i in items:\n                    continue  # 如果与user喜爱的物品重复了，则直接跳过\n                recommends.setdefault(i, 0.)\n                recommends[i] += sim\n        # 根据被推荐物品的相似度逆序排列，然后推荐前N个物品给到用户\n        return dict(sorted(recommends.items(), key=itemgetter(1), reverse=True)[:N])\n\n    def train(self):\n        self.similarity()\n\nif __name__ == \"__main__\":\n    train, test = LoadMovieLensData(\"../Data/ml-1m/ratings.dat\", 0.8)\n    print(\"train data size: %d, test data size: %d\" % (len(train), len(test)))\n    ItemCF = ItemCF(train, similarity='iuf', norm=True)\n    ItemCF.train()\n\n    # 分别对以下4个用户进行物品推荐\n    print(ItemCF.recommend(1, 5, 80))\n    print(ItemCF.recommend(2, 5, 80))\n    print(ItemCF.recommend(3, 5, 80))\n    print(ItemCF.recommend(4, 5, 80))\n```\n\n上述代码对测试集中前4个用户进行了电影推荐，对每个用户而言，从与他们喜爱电影相似的80部电影中挑选出5部推荐。\n输出结果是一个dict，里面包含了给用户推荐的电影以及用户对每部电影的感兴趣程度，按照逆序排列。\n\n\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h30nuf0a3hj223e0gaq7q.jpg)","tags":["ItemCF","推荐系统"],"categories":["AI"]},{"title":"关于ES9200漏洞添加授权验证方法实践","url":"/2022/06/03/关于ES9200漏洞添加授权验证方法实践/","content":"\n# 关于ES9200漏洞添加授权验证方法实践\n\nElasticsearch往往存有公司大量的数据，如果安全不过关，那么就会有严重的数据安全隐患。\nElasticsearch 的安全认证方式有不少，如http-basic，search guard，shield等，本文讲的是使用Xpack进行安全认证。\n\n1、关于http-basic也实验过，使用的是elasticSearch-http-basic提供了针对ES HTTP连接的IP白名单、密码权限和信任代理功能。可以安装[elasticsearch-http-basic](https://github.com/Asquera/elasticsearch-http-basic)包，但是在Github上目前发布的是已经是7年前的版本了。而且对应es的版本也只是支持到es6.x，所以排除了http-basic这个插件包。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h860le4dexj30d80a8q38.jpg)\n\n2、在本项目中涉及到的ES版本是7.8以及8.2。部署上线后会经由扫描系统Elasticsearch 安全漏洞(CVE-2020-7019)\n\n<img src=\"https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h860vfsub9j30ub0u0jxs.jpg\" alt=\"image-20221115191050553\" style=\"zoom:67%;\" />\n\n所以想使用X-Pack进行安全认证。\n\n## XPack\n\nX-Pack是Elastic Stack扩展功能，提供安全性，警报，监视，报告，机器学习和许多其他功能。 ES7.0+之后，默认情况下，当安装Elasticsearch时，会安装X-Pack，无需单独再安装。\n\n自6.8以及7.1+版本之后，基础级安全永久免费。\n\n### 2、相关安全配置介绍\n\n#### 2.1、xpack.security.enabled\n\n默认为true，启用节点上ES的XPACK安全功能，相当于总开关\n\n#### 2.2、xpack.security.http.ssl\n\n这个是用来开启https的，以及对应的设置，整体配置项如下：\n\n```shell\nxpack.security.http.ssl:\n  enabled: false 【开启还是关闭】\n  verification_mode: certificate【如下】\n   【full：它验证所提供的证书是否由受信任的权威机构(CA)签名，并验证服务器的主机名(或IP地址)是否与证书中识别的名称匹配。】\n   【certificate：它验证所提供的证书是否由受信任的机构(CA)签名，但不执行任何主机名验证。】\n   【none：它不执行服务器证书的验证。】\n  truststore.path: certs/elastic-certificates.p12 【信任存储库文件的存放位置】\n  keystore.path: certs/elastic-certificates.p12【密钥存储库文件的存放位置】\n```\n\n#### 2.3、xpack.security.transport.ssl\n\n这个是传输层的认证设置，整体配置项如下：\n\n```shell\nxpack.security.transport.ssl:\n  enabled: true【开启还是关闭】\n  verification_mode: certificate【如下】\n   【full：它验证所提供的证书是否由受信任的权威机构(CA)签名，并验证服务器的主机名(或IP地址)是否与证书中识别的名称匹配。】\n   【certificate：它验证所提供的证书是否由受信任的机构(CA)签名，但不执行任何主机名验证。】\n   【none：它不执行服务器证书的验证。】\n  keystore.path: certs/elastic-certificates.p12【信任存储库文件的存放位置】\n  truststore.path: certs/elastic-certificates.p12【密钥存储库文件的存放位置】\n```\n\n### 3、ES集群认证配置\n\n命令操作都是在ES安装根目录下执行的（这个好像不用管）\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h862qr4zncj30ip09aacc.jpg)\n\n#### 3.1、创建证书\n\n##### 3.1.1、创建一个证书颁发机构\n\n提示命名文件：直接回车，默认文件名elastic-stack-ca.p12文件\n提示输入密码：可以直接回车，也可以输入密码进行设置（如果这里设置密码的话，下面在给keystore和truststore设置的时候要输入密码，较为繁琐）\n\n```\n./bin/elasticsearch-certutil ca\n```\n\n生成如下文件：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h862qwf26hj309p05awer.jpg)\n\n##### 3.1.2、为节点生成证书和私钥\n\n提示命名文件，直接回车，默认文件名elastic-certificates.p12文件\n提示输入密码：可以直接回车，也可以输入密码进行设置\n\n```\n./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12\n```\n\n这里使用3.1.1创建的elastic-stack-ca.p12文件\n\n##### 3.1.3、config目录下创建certs目录\n\n```\nmkdir config/certs \n```\n\n##### 3.1.4、将文件可拷贝到certs目录下\n\n```\nmv elastic-certificates.p12 config/certs/\n```\n\n#### 3.2、给keystore和truststore设置密码\n\n> keystore可以看成一个放key的库，key就是公钥，私钥，数字签名等组成的一个信息\n>\n> truststore是放信任证书的一个store\n>\n> truststore和keystore的性质是一样的，都是存在key的一个仓库，区别在于，truststore存放的是只包含公钥的数字证书，代表了可以信任的证书，而keystore是包含私钥的。\n\n如果在创建证书的过程中加了密码，需要输入这个密码。每个节点都需要。执行如下：\n\n```\n./bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password\n```\n\n```\n./bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password\n```\n\n```\n./bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password\n```\n\n```\n./bin/elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password\n```\n\n这样就会在config目录下生成keystore文件了\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h862r57shrj30k009240y.jpg)\n\n#### 3.3、修改配置文件并重启\n\n在config/elasticsearch.yml文件中设置并重启：\n\n```\nxpack.security.enabled: true\n\n\nxpack.security.http.ssl:\n  enabled: false\n  verification_mode: certificate\n  truststore.path: certs/elastic-certificates.p12\n  keystore.path: certs/elastic-certificates.p12\n\nxpack.security.transport.ssl:\n  enabled: true\n  verification_mode: certificate\n  keystore.path: certs/elastic-certificates.p12\n  truststore.path: certs/elastic-certificates.p12\n```\n\n#### 3.4 创建用户密码\n\n集群中的节点都按照上面的方式完成配置并启动后，就可以设置账号密码了，有4种方式：\n\na、自动创建密码\n\n```\n./bin/elasticsearch-setup-passwords auto\n```\n\nb、手动输入密码\n\n```\n./bin/elasticsearch-setup-passwords interactive\n```\n\nc、重置用户密码（随机密码）\n\n```\n./bin/elasticsearch-reset-password -u elastic\n```\n\nd、重置用户密码（指定密码）\n\n```\n./bin/elasticsearch-reset-password -u elastic -i <password>\n```\n\n这里我们设置的密码是scDEfDfTT=9k7aaA=H_m，也就是user：elastic的密码。\n\n### 4、认证验证场景\n\n> 这里说明一下：\n> xpack.security.http.ssl的enable为true 就会是https，为false就是http，我这里是关掉了\n\n#### 4.1 浏览器访问验证、\n\n在web页面浏览的话就是：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h862rg46eij30gz0amt9e.jpg)\n\n#### 4.2 curl认证\n\n当你执行curl去访问es api的时候也会提示需要进行认证。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h862rkp5r9j30vd02rwfq.jpg)\n\n但是加上账户密码认证授权就可以访问了：\n\n```\ncurl --user elastic:scDEfDfTT=9k7aaA=H_m localhost:9200\n```\n\n#### 4.3 kibana认证\n\nkibana中配置ES中配置的kibana账号密码即可连接ES认证。\n\n修改kibana的配置文件`config/kibana.yml`在配置文件中添加下面内容\n\n```\nelasticsearch.username: \"kibana\"\nelasticsearch.password: \"XXX\"\nelasticsearch.hosts: [\"http://1.1.1.1:9200\",\"http://2.2.2.2:9200\",\"http://3.3.3.3:9200\"]\nserver.port: 5601\n```\n\n重启Kibana\n\n这里补充一个之前在介绍kibana没有记录的内容。\n\nkibana 使用ps -ef|grep kibana是查不到进程的，因为其实运行在node里面。但是我们也不能关闭所有node里面的软件，所以我们需要查询kibana监听端口5601的进程。\n\n使用下面命令关闭kibana\n\n```\n[esadmin@****** elasticsearch-7.2.0-a]$ netstat -tunlp|grep 5601\n(Not all processes could be identified, non-owned process info\n will not be shown, you would have to be root to see it all.)\ntcp        0      0 0.0.0.0:5601            0.0.0.0:*               LISTEN      16177/bin/../node/b \n\n[root@****** elasticsearch-7.2.0-a]# kill -9 16177\n```\n\n\n然后重启Kibana\n\n```\nnohup ./kibana &\n```\n\n\n\n---------------\n\n### 5、另外在启动es的时候提示这样一个错误：\n\n> max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]。\n\n解决办法如下：\n\n```\nsudo vi /etc/security/limits.conf\n```\n\n下面这行代码就添加到这个配置文件的末尾就好\n\n```\n*                soft    nofile          65536\n*                hard    nofile          65536\n```\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8627i09odj30m50d90v2.jpg)\n\n然后我们再看看有没有设置好\n\n**就把当前账号退出，可以切换root就可以**\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8627v67juj30bb02oglr.jpg)\n\n\n\n在初次进入用zwyuser账户登陆的时候并不能加载该配置，但是su root后，再su zwyuser后就会加载该配置，原因并未具体深究，查阅到一篇相关的文章https://blog.csdn.net/zzddada/article/details/121701005。\n\n### 6、bulk操作elasticsearch8报错解决：The bulk request must be terminated by a newline [\\n]\n\nbulk批量操作的时候，bulk请求的时候，必须以换行符（\\n）结束，请求体是一种叫NDJSON的结构。\n\n可以解决的办法是：\n\n将批量操作的每一条数据先转成字符串，并在后面加上 **\\n** 换行符，将最后所有的拼接成一个大字符串\n\n\n\n如果想使用json文件的话，要在文件名字前面添加@符号。即 @xxx.json\n\n\n\n\n\n\n\ncurl --user elastic:scDEfDfTT=9k7aaA=H_m -XGET -H \"Content-Type: application/json\" '192.168.200.8:9200/policy_laoshan/_search' -d '{\"query\":{\"bool\":{\"should\":[{\"term\":{\"content\":{\"value\":\"城乡\"}}}]}},\"from\":0,\"size\":20},\"highlight\":{\"pre_tags\": \"\\<span style='color:red'>\",\"post_tags\": \"</span>\"}'\n\n建筑\n\n\n\n\n\n```json\n curl --user elastic:scDEfDfTT=9k7aaA=H_m -XPOST -H \"Content-Type: application/json;charset=utf-8\" 'loc \nalhost:9200/policy_laoshan/_search' -d '{\"query\": {\"match\": {\"content\": \"企业服务业\"}},\"highlight\":{\"fields\":{\"content\":{}},\"pr \ne_tags\":\"<span style='color:red'>\",\"post_tags\":\"</span>\"},\"from\":0,\"size\":30}'\n```\n\n之前在query的时候传递的是term值，term是不具备分词功能的，这里改成了match，具备分词功能。\n","tags":["Pack","ES"],"categories":["AI"]},{"title":"五月底最后一周总结","url":"/2022/05/31/5月底最后一周总结/","content":"\n\n\n- 构建了百度地图 12345工单\n\n  ![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2tyhisp52j21hc0tbajb.jpg)\n\n- 对hexo进行了评价插件的设定\n\n搭建hexo 博客并添加gitalk\n\nhttps://www.jianshu.com/p/656e6101bf0f\n\nhttps://aurora.tridiamond.tech/zh/guide/plugins.html#%E8%AF%84%E8%AE%BA\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2tyjaguy3j21gv0qd469.jpg)\n\n","tags":["工作总结"],"categories":["AI"]},{"title":"百度地图Web服务api文档","url":"/2022/05/31/百度地图Web服务API/","content":"\n\n\n# Web服务API\n\n> 百度地图Web服务API为开发者提供http/https接口，即开发者通过http/https形式发起检索请求，获取返回json或xml格式的检索数据。用户可以基于此开发JavaScript、C#、C++、Java等语言的地图应用。\n\n\n\n## 核心服务简介\n\n- 地点检索服务\n- 地点输入提示服务\n- 正/逆地理编码服务\n- 路线规划服务\n- 批量算路服务\n- IP定位服务\n- 鹰眼轨迹服务\n- 轻量级轨迹服务\n- 时区服务\n- 推荐上车点服务\n- 坐标转换服务\n- 地图调起服务\n- 静态图服务\n- 全景静态图服务\n\n\n\n---------\n\n## 获取密钥\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2klwpnkr1j21h10grmyf.jpg)\n\n访问应用（AK）： NwrFdXX7d9ulyeUvuvrRL1pRwHEHl7qU    浏览器端\n\n​\t\t\t\t\t\t\t\tvVo0GpUwrXGIyq7Xpv6U7Bui5dNGDjaL     服务端\n\n​\t\t\t\t\t\t\t\tC3mLyfNmVlzGD4lZEpsMK0GhuxDPmQKB   js API\n\n## 地点检索\n\n> 地点检索服务\n>\n> 地点检索服务（又名Place API）是一类Web API接口服务；\n> 服务提供多种场景的地点（POI）检索功能，包括城市检索、圆形区域检索、矩形区域检索。开发者可通过接口获取地点（POI）基础或详细地理信息。\n> **注意**：地点检索服务适用于【XX大厦】、【XX小区】等POI地点名称的检索，若需要检索结构化地址，如【北京市海淀区上地十街十号】，则推荐使用[地理编码](http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-geocoding)服务。\n\n\n\n### 1、功能介绍\n\n#### 行政区划区域检索\n\n开发者可通过该功能，检索某一行政区划内（目前最细到城市级别）的地点信息。\n\n#### 圆形区域检索\n\n开发者可设置圆心和半径，检索圆形区域内的地点信息（常用于周边检索场景）。\n\n#### 地点详情检索\n\n不同于其他三种检索功能。地点详情检索针对指定POI，检索其相关的详情信息。开发者可以通过三种区域检索（或其他服务）功能，**获取POI id**。使用“地点详情检索”功能，**传入id**，即可检索POI详情信息，如评分、营业时间等（不同类型POI对应不同类别详情数据）。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kmrxy473j20bk06yt94.jpg)\n\n多边形区域检索\n\n多边形区域检索为高级权限，如有需求请提交[【工单】](http://lbsyun.baidu.com/apiconsole/fankui)咨询。\n\n\n\n### 2、服务文档\n\n#### \t行政区划区域检索\n\n```json\nhttps://api.map.baidu.com/place/v2/search?query=ATM机&tag=银行&region=北京&output=json&ak=您的ak //GET请求\n```\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kpea39sej21bj0u0jv2.jpg)\n\n返回结果如下：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kpqvle6bj21at0u0gpr.jpg)\n\n\n\n\n\n#### 圆形区域检索\n\n```json\nhttps://api.map.baidu.com/place/v2/search?query=银行&location=39.915,116.404&radius=2000&output=xml&ak=您的密钥 //GET请求\t\t\n```\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kpqtoq0zj21c10u0ada.jpg)\n\n其中**query**是检索关键字。圆形区域检索和矩形区域内检索支持多个关键字并集检索，不同关键字间以$符号分隔，最多支持10个关键字检索。如:”银行$酒店”\n如果需要按POI分类进行检索，请将分类通过query参数进行设置，如query=美食\n\n**page_size**单次召回POI数量，默认为10条记录，最大返回20条\n\n\n\n返回结果如下：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kpwnzk3zj21eb0u0tci.jpg)\n\n\n\n#### **地点详情检索服务**\n\n```json\nhttps://api.map.baidu.com/place/v2/detail?uid=435d7aea036e54355abbbcc8&output=json&scope=2&ak=您的密钥 //GET请求\n```\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kq7iswh5j21cg0u0goy.jpg)\n\n返回结果如下：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kq7x6z69j21on0u078k.jpg)\n\n## \n\n\n\n## 地址输入提示服务\n\n> 地点输入提示服务（又名Place Suggestion API）是一类Web API接口服务；\t\n>\n> 匹配用户输入内容，提供输入提示功能。常与[地点检索](http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi)服务搭配使用。也可作为轻量级地点检索服务单独使用（不支持复杂检索场景）。\n\n### 1、功能介绍\n\n地点输入提示\n\n用户可通过该服务，匹配用户输入关键词的地点推荐列表。\n在应用方面，可将地点推荐列表展示给用户，用户可通过点击等交互方式，结合[地点检索](http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi)服务，检索用户点击的POI信息，实现地点详情检索功能。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kqha6lpsj20bk06w74q.jpg)\n\n轻量级地点检索\n\n通过关键词和检索城市限制，可满足轻量级地点检索需求，获取地点基础数据\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kqhhitnvj20bk06ydgb.jpg)\n\n### 2、服务文档\n\n```json\nhttps://api.map.baidu.com/place/v2/suggestion?query=天安门&region=北京&city_limit=true&output=json&ak=你的ak //GET请求\n```\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kr29th27j21ij0u041o.jpg)\n\n\n\n结果如下：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kr2lkhg0j21e00u0n0l.jpg)\n\n\n\n\n\n\n\n## 正/逆地理编码服务\n\n> 地理编码服务（又名Geocoder）是一类Web API接口服务；\n> **地理编码服务**提供将结构化地址数据（如：北京市海淀区上地十街十号）转换为对应坐标点（经纬度）功能；\n> 地理编码服务当前未推出国际化服务，解析地址仅限国内；\n\n### 1、功能介绍\n\n地理编码服务\n\n用户可通过该功能，将结构化地址（省/市/区/街道/门牌号）解析为对应的位置坐标。地址结构越完整，地址内容越准确，解析的坐标精度越高。\n\n### 2、服务文档\n\n```json\nhttps://api.map.baidu.com/geocoding/v3/?address=北京市海淀区上地十街10号&output=json&ak=您的ak&callback=showLocation //GET请求\n注意：当前为V3.0版本接口文档，V2.0及以前版本自2019.6.18起新用户无法使用。老用户仍可继续使用V2.0及以前版本请求实现逆地理编码服务，为保障用户体验，建议您尽快迁移到V3.0版本。\n```\n\n\n\n请求如下：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2krujqqxjj21eo0u041r.jpg)\n\n结果如下：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2krv0kvzsj21hf0u00vn.jpg)\n\n## 天气查询\n\n### 1、功能介绍\n\n国内天气查询服务分为基础服务和高级权限。\n在基础服务中，用户可通过行政区划代码查询实时天气信息及未来5天天气预报。\n在高级权限中，用户可通过经纬度查询实时天气信息、未来7天天气预报及未来24小时逐小时预报。同时，用户还可以通过高级权限获取空气质量指数、生活指数、气象预警等丰富信息。高级权限需付费开通，您可以[联系我们](http://lbsyun.baidu.com/apiconsole/fankui#?typeOne=产品需求&typeTwo=高级服务)开通15天试用并了解更多信息。\n\n### 2、服务文档\n\n```json\nhttps://api.map.baidu.com/weather/v1/?district_id=222405&data_type=all&ak=你的ak  //GET请求\t\n```\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2ks9urql4j21ex0u0q6d.jpg)\n\n结果如下：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2ksa9ae70j21cf0u0tc7.jpg)\n\n\n\n\n\n## 地址解析聚合\n\n### 1、功能介绍\n\n该服务用于解析地址结构。结合自然语言理解能力，可以对地址信息按照文本信息、空间位置信息等因子进行区域化聚合。同时也可解析并提取地址中核心结构，如行政区划（省、市、区、乡镇）、街道、POI，以及地址中的联系人，联系方式等信息，并对地址进行一定的补全和纠错。\n\n\n\n聚合模式\n\n将地址按照基础地物进行分类聚合，结合百度地图领先的地图地理信息解析能力以及百度NLP自然语言能力，对地址信息进行结构化理解，并结合基础地图单位进行位置聚合。\n目前可聚合的单位为：省、市、区县、乡镇街道、道路、路段、末端地点（POI或AOI）\n\n标准化模式\n\n结构化解析并提取复杂地址中的核心内容，包含人名、联系方式、行政区划结构（省、市、区县、乡镇街道）、POI信息。\n\n异常地址识别服务\n\n针对系统传入地址进行判断是否存在不合规的情况，通过找到不合规的case问题，系统给用户提示问题的原因，来协助降低地址输入时的错误率。\n\n\n\n### 2、服务文档\n\n```json\nhttps://api.map.baidu.com/address_analyzer/v1?address=北京市海淀区信息路甲九号&ak=你的ak\n```\n\n**付费服务**\n\n\n\n\n\n## JavaScript API v3.0\n\n\n\n### 使用方法\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2mpk27rybj20p4039jrf.jpg)\n\nak申请需要选择浏览器端-Javascript API\n\n\n\n### 百度地图的Hello World\n\n```html\n<!DOCTYPE html>  \n<html>\n<head>  \n<meta name=\"viewport\" content=\"initial-scale=1.0, user-scalable=no\" />  \n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />  \n<title>Hello, World</title>  \n<style type=\"text/css\">  \nhtml{height:100%}  \nbody{height:100%;margin:0px;padding:0px}  \n#container{height:100%}  \n</style>  \n<script type=\"text/javascript\" src=\"https://api.map.baidu.com/api?v=3.0&ak=您的密钥\">\n//v3.0版本的引用方式：src=\"https://api.map.baidu.com/api?v=3.0&ak=您的密钥\"\n</script>\n</head>  \n \n<body>  \n<div id=\"container\"></div> \n<script type=\"text/javascript\"> \nvar map = new BMap.Map(\"container\");\n// 创建地图实例  \nvar point = new BMap.Point(116.404, 39.915);\n// 创建点坐标  \nmap.centerAndZoom(point, 15);\n// 初始化地图，设置中心点坐标和地图级别  \n</script>  \n</body>  \n</html>\n```\n\n### 分步步骤：\n\n#### 1申请百度账号和ak\n\n[点我申请](https://lbs.baidu.com/apiconsole/key)\n\n#### 2准备页面\n\n根据HTML标准，每一份HTML文档都应该声明正确的文档类型，我们建议您使用最新的符合HTML5规范的文档声明：\n\n```html\n<!DOCTYPE html>\n```\n\n您也可以根据需要选择其他类型的文档声明，这样浏览器会以标准的方式对页面进行渲染，保证页面最大的兼容性。我们不建议您使用quirks模式进行开发。\n\n#### 3适应移动端页面展示\n\n下面我们添加一个meta标签，以便使您的页面更好的在移动平台上展示。\n\n```html\n<meta name=\"viewport\" content=\"initial-scale=1.0, user-scalable=no\" />  \n```\n\n#### 4设置容器样式\n\n设置容器样式大小，使地图充满整个浏览器窗口：\n\n```html\n<style type=\"text/css\">  \n    html{height:100%}    \n    body{height:100%;margin:0px;padding:0px}    \n    #container{height:100%}    \n</style> \n```\n\n#### 5引用百度地图API文件\n\n```html\n<script type=\"text/javascript\" src=\"https://api.map.baidu.com/api?v=3.0&ak=您的密钥\"></script>\n```\n\n#### 6创建地图容器元素\n\n地图需要一个HTML元素作为容器，这样才能展现到页面上。这里我们创建了一个div元素。\n\n```html\n<div id=\"container\"></div> \n```\n\n#### 7创建地图实例\n\n位于BMap命名空间下的Map类表示地图，通过new操作符可以创建一个地图实例。其参数可以是元素id也可以是元素对象。\n\n```javascript\nvar map = new BMap.Map(\"container\"); \n```\n\n注意：\n\n1.在调用此构造函数时应确保容器元素已经添加到地图上。\n\n2.命名空间 API使用BMap作为命名空间，所有类均在该命名空间之下，比如：BMap.Map、BMap.Control、BMap.Overlay。\n\n#### 8设置中心点坐标\n\n这里我们使用BMap命名空间下的Point类来创建一个坐标点。Point类描述了一个地理坐标点，其中116.404表示经度，39.915表示纬度。（为天安门坐标）\n\n```javascript\nvar point = new BMap.Point(116.404, 39.915); \n```\n\n注意：在使用百度地图JavaScript API服务时，需使用百度BD09坐标，如使用其他坐标（ WGS84、GCJ02）进行展示，需先将其他坐标转换为BD09，详细说明请参考坐标转换说明，请勿使用非官方的转换方法！！！\n\n#### 9地图初始化，同时设置地图展示级别\n\n在创建地图实例后，我们需要对其进行初始化，BMap.Map.centerAndZoom()方法要求设置中心点坐标和地图级别。 地图必须经过初始化才可以执行其他操作。\n\n```javascript\nmap.centerAndZoom(point, 15);  \n```\n\n至此，我们就快速创建了一张以天安门为中心的地图~\n\n\n\n### 1、地图配置与操作\n\n**默认情况下地图不支持鼠标滚轮缩放操作，如果希望在地图中使用鼠标滚轮控制缩放，可以调用map.enableScrollWheelZoom方法来开启。配置选项可以在Map类参考中找到。**\n\nmap.enableScrollWheelZoom方法来开启。配置选项可以在Map类参考中找到。\n\n此外，您还可以通过编程的方式与地图交互。Map类提供了若干修改地图状态的方法。例如：setCenter()、panTo()、zoomTo()等等。\n\n示例： 等待两秒钟后，地图它会移动到新中心点。panTo()方法将让地图平滑移动至新中心点，如果移动距离超过了当前地图区域大小，则地图会直跳到该点。\n\n\n\n\n\n### 2、坐标转换说明\n\n目前国内主要有以下三种坐标系：\n\nWGS84：为一种大地坐标系，也是目前广泛使用的GPS全球卫星定位系统使用的坐标系。\n\nGCJ02：又称火星坐标系，是由中国国家测绘局制订的地理信息系统的坐标系统。由WGS84坐标系经加密后的坐标系。\n\nBD09：为百度坐标系，在GCJ02坐标系基础上再次加密。其中bd09ll表示百度经纬度坐标，bd09mc表示百度墨卡托米制坐标。\n\n非中国地区地图，服务坐标统一使用WGS84坐标。\n\n其他坐标转百度坐标\n\n百度对外接口的坐标系为BD09坐标系，并不是GPS采集的真实经纬度，在使用百度地图JavaScript API服务前，需先将非百度坐标通过坐标转换接口转换成百度坐标。 坐标转换、批量坐标转换示例详见[**JavaScript API示例**](http://lbsyun.baidu.com/jsdemo.htm#a5_2)。\n\n\n\n### 3、添加控件\n\n> 百度地图上负责与地图交互的UI元素称为控件。百度地图API中提供了丰富的控件，您还可以通过Control类来实现自定义控件。\n\n#### 提供的控件\n\n提供的控件\n\n详情可见[类参考](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a2b0)。\n\n| **控件**     | **类名**           | **简介**                                                     |\n| ------------ | ------------------ | ------------------------------------------------------------ |\n| 抽象基类     | Control            | 所有控件均继承此类的方法、属性。通过此类您可实现自定义控件   |\n| 平移缩放控件 | NavigationControl  | PC端默认位于地图左上方，它包含控制地图的平移和缩放的功能。移动端提供缩放控件，默认位于地图右下方 |\n| 缩略地图     | OverviewMapControl | 默认位于地图右下方，是一个可折叠的缩略地图                   |\n| 比例尺       | ScaleControl       | 默认位于地图左下方，显示地图的比例关系                       |\n| 地图类型     | MapTypeControl     | 默认位于地图右上方                                           |\n| 版权         | CopyrightControl   | 默认位于地图左下方                                           |\n| 定位         | GeolocationControl | 针对移动端开发，默认位于地图左下方                           |\n\n#### 向地图添加控件\n\n> 可以使用Map.addControl()方法向地图添加控件。\n\n##### 1地图初始化\n\n添加控件前，地图需要进行初始化。例如，要将标准地图控件添加到地图中，可在代码中添加如下内容：\n\n```javascript\nvar map = new BMap.Map(\"container\");    \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);    \nmap.addControl(new BMap.NavigationControl());\n```\n\n添加平移缩放控件效果如下：\n\n![image-20220527102930582](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24egy1h2mr7zc8brj203b06caa2.jpg)\n\n##### 2添加多个控件\n\n在本例中我们向地图添加一个平移缩放控件、一个比例尺控件和一个缩略图控件。在地图中添加控件后，它们即刻生效。\n\n```javascript\nmap.addControl(new BMap.NavigationControl());    \nmap.addControl(new BMap.ScaleControl());    \nmap.addControl(new BMap.OverviewMapControl());    \nmap.addControl(new BMap.MapTypeControl());    \nmap.setCurrentCity(\"北京\"); // 仅当设置城市信息时，MapTypeControl的切换功能才能可用\n```\n\n\n\n#### 控制控件位置\n\n初始化控件时，可提供一个可选参数，其中的anchor和offset属性共同控制控件在地图上的位置。 anchor表示控件的停靠位置，即控件停靠在地图的哪个角。当地图尺寸发生变化时，控件会根据停靠位置的不同来调整自己的位置。\n\n| **anchor值**             | **位置说明**               |\n| ------------------------ | -------------------------- |\n| BMAP_ANCHOR_TOP_LEFT     | 表示控件定位于地图的左上角 |\n| BMAP_ANCHOR_TOP_RIGHT    | 表示控件定位于地图的右上角 |\n| BMAP_ANCHOR_BOTTOM_LEFT  | 表示控件定位于地图的左下角 |\n| BMAP_ANCHOR_BOTTOM_RIGHT | 表示控件定位于地图的右下角 |\n\n控件位置偏移\n\n除了指定停靠位置外，还可以通过偏移量来指示控件距离地图边界有多少像素。如果两个控件的停靠位置相同，那么控件可能会重叠在一起，这时就可以通过偏移值使二者分开显示。\n\n如下示例为：将比例尺放置在地图的左下角，由于API默认会有版权信息，因此需要添加一些偏移值以防止控件重叠。\n\n```javascript\nvar opts = {offset: new BMap.Size(150, 5)}\nmap.addControl(new BMap.ScaleControl(opts));\n```\n\n#### 修改控件配置\n\n地图API的控件提供了丰富的配置参数，您可参考API文档来修改它们以便得到符合要求的控件外观。例如，NavigationControl控件就提供了如下类型：\n\n| **平移缩放控件的类型**        | **说明**                     |\n| ----------------------------- | ---------------------------- |\n| BMAP_NAVIGATION_CONTROL_LARGE | 表示显示完整的平移缩放控件   |\n| BMAP_NAVIGATION_CONTROL_SMALL | 表示显示小型的平移缩放控件   |\n| BMAP_NAVIGATION_CONTROL_PAN   | 表示只显示控件的平移部分功能 |\n| BMAP_NAVIGATION_CONTROL_ZOOM  | 表示只显示控件的缩放部分功能 |\n\n下图从左向右依次展示了上述不同类型的控件外观，前四个为PC端平移缩放控件样式，最后一个为移动端缩放控件样式： ![control.png](https://mapopen-website-wiki.cdn.bcebos.com/static/img/control.png)\n\n如下示例为：将调整平移缩放地图控件的外观。\n\n```javascript\nvar opts = {type: BMAP_NAVIGATION_CONTROL_SMALL}    \nmap.addControl(new BMap.NavigationControl(opts));\n```\n\n\n\n\n\n#### 自定义控件\n\n百度地图API允许您通过继承Control来创建自定义地图控件。\n\n1定义一个自定义控件的构造函数并继承Control\n\n您需要定义自定义控件的构造函数，并在构造函数中提供defaultAnchor和defaultOffset两个属性，以便API正确定位控件位置，接着让其继承于Control。在下面的示例中我们定义一个名为ZoomControl的控件，每一次点击将地图放大两个级别。它具有文本标识，而不是平移缩放控件中使用的图形图标。\n\n```javascript\n// 定义一个控件类，即function    \nfunction ZoomControl(){    \n    // 设置默认停靠位置和偏移量  \n    this.defaultAnchor = BMAP_ANCHOR_TOP_LEFT;    \n    this.defaultOffset = new BMap.Size(10, 10);    \n}    \n// 通过JavaScript的prototype属性继承于BMap.Control   \nZoomControl.prototype = new BMap.Control();\n```\n\n2初始化自定义控件\n\n设置自定义控件构造函数的prototype属性为Control的实例，以便继承控件基类。\n\n当调用map.addControl()方法添加自定义控件时，API会调用该对象的initialize()方法用来初始化控件，您需要实现此方法并在其中创建控件所需的DOM元素，并添加DOM事件。所有自定义控件中的DOM元素最终都应该添加到地图容器（即地图所在的DOM元素）中去，地图容器可以通过map.getContainer()方法获得。最后initialize()方法需要返回控件容器的DOM元素。\n\n```javascript\n// 自定义控件必须实现initialize方法，并且将控件的DOM元素返回   \n// 在本方法中创建个div元素作为控件的容器，并将其添加到地图容器中   \nZoomControl.prototype.initialize = function(map){    \n    // 创建一个DOM元素   \n    var div = document.createElement(\"div\");    \n    // 添加文字说明    \n    div.appendChild(document.createTextNode(\"放大2级\"));    \n    // 设置样式    \n    div.style.cursor = \"pointer\";    \n    div.style.border = \"1px solid gray\";    \n    div.style.backgroundColor = \"white\";    \n    // 绑定事件，点击一次放大两级    \n    div.onclick = function(e){  \n        map.zoomTo(map.getZoom() + 2);    \n    }    \n    // 添加DOM元素到地图中   \n    map.getContainer().appendChild(div);    \n    // 将DOM元素返回  \n    return div;    \n }\n```\n\n3添加自定义控件\n\n添加自定义控件与添加其他控件方法一致，调用map.addControl()方法即可。\n\n```javascript\n// 创建控件实例    \nvar myZoomCtrl = new ZoomControl();    \n// 添加到地图当中    \nmap.addControl(myZoomCtrl);\n```\n\n### 4、标注\n\n> 所有叠加或覆盖到地图的内容，我们统称为地图覆盖物。覆盖物拥有自己的地理坐标，当您拖动或缩放地图时，它们会相应的移动。\n>\n> 覆盖物主要分为：标注（点标注、矢量图形（包括折线、多边形、圆））、信息窗口、图层。\n>\n> 本节重点介绍一下如何向地图添加标注，以及与地图相关的一些交互。\n\n\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n\t<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n\t<meta name=\"viewport\" content=\"initial-scale=1.0, user-scalable=no\" />\n\t<style type=\"text/css\">\n\t\tbody, html{width: 100%;height: 100%;margin:0;font-family:\"微软雅黑\";}\n\t\t#allmap{height:500px;width:100%;}\n\t\t#r-result{width:100%;}\n\t</style>\n\t<script type=\"text/javascript\" src=\"//api.map.baidu.com/api?v=2.0&ak=您的密钥\"></script>\n\t<title>添加/删除覆盖物</title>\n</head>\n<body>\n\t<div id=\"allmap\"></div>\n\t<div id=\"r-result\">\n\t\t<input type=\"button\" onclick=\"add_overlay();\" value=\"添加覆盖物\" />\n\t\t<input type=\"button\" onclick=\"remove_overlay();\" value=\"删除覆盖物\" />\n\t</div>\n</body>\n</html>\n<script type=\"text/javascript\">\n\t// 百度地图API功能\n\tvar map = new BMap.Map(\"allmap\");\n\tvar point = new BMap.Point(116.404, 39.915);\n\tmap.centerAndZoom(point, 15);\n\t\n\tvar marker = new BMap.Marker(new BMap.Point(116.404, 39.915)); // 创建点\n\tvar polyline = new BMap.Polyline([\n\t\tnew BMap.Point(116.399, 39.910),\n\t\tnew BMap.Point(116.405, 39.920),\n\t\tnew BMap.Point(116.425, 39.900)\n\t], {strokeColor:\"blue\", strokeWeight:2, strokeOpacity:0.5});   //创建折线\n\t\n\tvar circle = new BMap.Circle(point,500,{strokeColor:\"blue\", strokeWeight:2, strokeOpacity:0.5}); //创建圆\n\t\n\tvar polygon = new BMap.Polygon([\n\t\tnew BMap.Point(116.387112,39.920977),\n\t\tnew BMap.Point(116.385243,39.913063),\n\t\tnew BMap.Point(116.394226,39.917988),\n\t\tnew BMap.Point(116.401772,39.921364),\n\t\tnew BMap.Point(116.41248,39.927893)\n\t], {strokeColor:\"blue\", strokeWeight:2, strokeOpacity:0.5});  //创建多边形\n\t\n\tvar pStart = new BMap.Point(116.392214,39.918985);\n\tvar pEnd = new BMap.Point(116.41478,39.911901);\n\tvar rectangle = new BMap.Polygon([\n\t\tnew BMap.Point(pStart.lng,pStart.lat),\n\t\tnew BMap.Point(pEnd.lng,pStart.lat),\n\t\tnew BMap.Point(pEnd.lng,pEnd.lat),\n\t\tnew BMap.Point(pStart.lng,pEnd.lat)\n\t], {strokeColor:\"blue\", strokeWeight:2, strokeOpacity:0.5});  //创建矩形\n\t\n\t//添加覆盖物\n\tfunction add_overlay(){\n\t\tmap.addOverlay(marker);            //增加点\n\t\tmap.addOverlay(polyline);          //增加折线\n\t\tmap.addOverlay(circle);            //增加圆\n\t\tmap.addOverlay(polygon);           //增加多边形\n\t\tmap.addOverlay(rectangle);         //增加矩形\n\t}\n\t//清除覆盖物\n\tfunction remove_overlay(){\n\t\tmap.clearOverlays();         \n\t}\n</script>\n```\n\n#### 1、提供的覆盖物\n\n可以使用map.addOverlay方法向地图添加覆盖物，使用map.removeOverlay方法移除覆盖物，注意此方法不适用于InfoWindow。\n\n详情请见[类参考](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a3b0)。\n\n\n\n| **覆盖物**   | **类名**        | **说明**                                                     |\n| ------------ | --------------- | ------------------------------------------------------------ |\n| 抽象基类     | Overlay         | 所有的覆盖物均继承此类的方法                                 |\n| 点           | Marker          | 表示地图上的点，可自定义标注的图标                           |\n| 文本         | Label           | 表示地图上的文本标注，您可以自定义标注的文本内容             |\n| 折线         | Polyline        | 表示地图上的折线                                             |\n| 多边形       | Polygon         | 表示地图上的多边形。多边形类似于闭合的折线，另外您也可以为其添加填充颜色 |\n| 圆           | Circle          | 表示地图上的圆                                               |\n| 信息窗口     | InfoWindow      | 信息窗口也是一种特殊的覆盖物，它可以展示更为丰富的文字和多媒体信息。注意：同一时刻只能有一个信息窗口在地图上打开 |\n| 地面叠加层   | GoundOverlay    | 表示叠加在地图上的图片，图片的链接、大小、位置等属性可以自定义 |\n| 海量点       | PointCollection | 针对点的数量很大的情况，可以使用海量点进行展示               |\n| 自定义覆盖物 | 自定义          | 支持通过继承覆盖物基类Overlay，自定义覆盖物                  |\n\n#### 2、标注点\n\nMarker是一个用来往地图上添加点标记的类。使用它将任何你希望用户看到的兴趣点标注在地图上。\n\nAPI提供了默认图标样式，您也可以通过Icon类来指定自定义图标。Marker的构造函数的参数为Point和MarkerOptions（可选）。\n\n注意：当您使用自定义图标时，标注的地理坐标点将位于标注所用图标的中心位置，您可通过Icon的offset属性修改标定位置。\n\nMarker使用详情请见[类参考](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a3b2)。\n\n\n\n1向地图添加标注\n\n如下示例，向地图中心点添加了一个标注，并使用默认的标注样式：\n\n```javascript\nvar map = new BMap.Map(\"container\");    \nvar point = new BMap.Point(116.404, 39.915);    \nmap.centerAndZoom(point, 15);    \nvar marker = new BMap.Marker(point);        // 创建标注    \nmap.addOverlay(marker);                     // 将标注添加到地图中 \n```\n\n2定义标注图标\n\n通过Icon类可实现自定义标注的图标，下面示例通过参数MarkerOptions的icon属性进行设置，您也可以使用marker.setIcon()方法。\n\n```javascript\nvar map = new BMap.Map(\"container\");    \nvar point = new BMap.Point(116.404, 39.915);    \nmap.centerAndZoom(point, 15);  // 编写自定义函数，创建标注   \nfunction addMarker(point, index){  // 创建图标对象   \n    var myIcon = new BMap.Icon(\"markers.png\", new BMap.Size(23, 25), {    \n        // 指定定位位置。   \n        // 当标注显示在地图上时，其所指向的地理位置距离图标左上    \n        // 角各偏移10像素和25像素。您可以看到在本例中该位置即是   \n        // 图标中央下端的尖角位置。    \n        anchor: new BMap.Size(10, 25),    \n        // 设置图片偏移。   \n        // 当您需要从一幅较大的图片中截取某部分作为标注图标时，您   \n        // 需要指定大图的偏移位置，此做法与css sprites技术类似。    \n        imageOffset: new BMap.Size(0, 0 - index * 25)   // 设置图片偏移    \n    });      \n    // 创建标注对象并添加到地图   \n    var marker = new BMap.Marker(point, {icon: myIcon});    \n    map.addOverlay(marker);    \n}    \n// 随机向地图添加10个标注    \nvar bounds = map.getBounds();    \nvar lngSpan = bounds.maxX - bounds.minX;    \nvar latSpan = bounds.maxY - bounds.minY;    \nfor (var i = 0; i < 10; i ++) {    \n    var point = new BMap.Point(bounds.minX + lngSpan * (Math.random() * 0.7 + 0.15),    \n                                  bounds.minY + latSpan * (Math.random() * 0.7 + 0.15));    \n    addMarker(point, i);    \n}\n```\n\n3监听标注事件\n\n事件方法与Map事件机制相同。可参考[事件部分](http://lbsyun.baidu.com/index.php?title=jspopular/guide/event)。\n\n```javascript\nmarker.addEventListener(\"click\", function(){    \n    alert(\"您点击了标注\");    \n});  \n```\n\n4可拖拽的标注\n\n[marker](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference.html#a3b2)的enableDragging和disableDragging方法可用来开启和关闭标注的拖拽功能。默认情况下标注不支持拖拽，您需要调用marker.enableDragging()方法来开启拖拽功能。在标注开启拖拽功能后，您可以监听标注的dragend事件来捕获拖拽后标注的最新位置。\n\n```javascript\nmarker.enableDragging();    \nmarker.addEventListener(\"dragend\", function(e){    \n    alert(\"当前位置：\" + e.point.lng + \", \" + e.point.lat);    \n})   \n```\n\n### 5、自定义标注\n\n> 本节重点介绍如何向地图添加自定义标注覆盖物。 覆盖物相关使用详情请见[类参考](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a3b0)。\n\n![image-20220530111252651](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2q9c0fcygj20pq0acac3.jpg)\n\n##### 1、定义构造函数并继承Overlay\n\n首先您需要定义自定义覆盖物的构造函数，通过构造函数参数可以传递一些自由的变量。设置自定义覆盖物对象的prototype属性为[Overlay](https://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference.html#a3b0)的实例，以便继承覆盖物基类。\n\n如下示例，我们定义一个名为SquareOverlay的构造函数，它包含中心点和边长两个参数，用来在地图上创建一个方形覆盖物。\n\n```javascript\n// 定义自定义覆盖物的构造函数  \nfunction SquareOverlay(center, length, color){\n    this._center = center;\n    this._length = length;\n    this._color = color;\n}\n// 继承API的BMap.Overlay\nSquareOverlay.prototype = new BMap.Overlay();\n```\n\n##### 2、初始化自定义覆盖物\n\n实现[initialize](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference.html#a3b0)方法，当调用map.addOverlay方法时，API会调用此方法。\n\n当调用map.addOverlay方法添加自定义覆盖物时，API会调用该对象的initialize方法用来初始化覆盖物，在初始化过程中需要创建覆盖物所需要的DOM元素，并添加到地图相应的容器中。这里我们选择添加在容器markerPane上。\n\n```javascript\n// 实现初始化方法  \nSquareOverlay.prototype.initialize = function(map){\n    // 保存map对象实例\n    this._map = map;\n    // 创建div元素，作为自定义覆盖物的容器\n    var div = document.createElement(\"div\");\n    div.style.position = \"absolute\";\n    // 可以根据参数设置元素外观\n    div.style.width = this._length + \"px\";\n    div.style.height = this._length + \"px\";\n    div.style.background = this._color;\n    // 将div添加到覆盖物容器中\n    map.getPanes().markerPane.appendChild(div);\n    // 保存div实例\n    this._div = div;\n    // 需要将div元素作为方法的返回值，当调用该覆盖物的show、\n    // hide方法，或者对覆盖物进行移除时，API都将操作此元素。\n    return div;\n}\n```\n\n地图提供了若干容器供覆盖物展示，通过[map.getPanes](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference.html#a3b0)方法可以得到这些容器元素，它们包括：\n\nfloatPane\n\nmarkerMouseTarget\n\nfloatShadow\n\nlabelPane\n\nmarkerPane\n\nmapPane\n\n这些对象代表了不同的覆盖物容器元素，它们之间存在着覆盖关系，最上一层为floatPane，用于显示信息窗口内容，下面依次为标注点击区域层、信息窗口阴影层、文本标注层、标注层和矢量图形层。\n\n我们自定义的方形覆盖物可以添加到任意图层上，如上示例，我们选择添加到markerPane上，作为其一个子结点。\n\n##### 3、绘制覆盖物\n\n实现[draw](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference.html#a3b0)方法。\n\n到目前为止，我们仅仅把覆盖物添加到了地图上，但是并没有将它放置在正确的位置上。\n\n您需要在draw方法中设置覆盖物的位置，每当地图状态发生变化（比如：位置移动、级别变化）时，API都会调用覆盖物的draw方法，用于重新计算覆盖物的位置。\n\n通过map.pointToOverlayPixel方法可以将地理坐标转换到覆盖物的所需要的像素坐标。\n\n```javascript\n// 实现绘制方法   \nSquareOverlay.prototype.draw = function(){    \n// 根据地理坐标转换为像素坐标，并设置给容器    \n    var position = this._map.pointToOverlayPixel(this._center);    \n    this._div.style.left = position.x - this._length / 2 + \"px\";    \n    this._div.style.top = position.y - this._length / 2 + \"px\";    \n}\n```\n\n##### 4、移除覆盖物\n\n当调用map.removeOverlay或者map.clearOverlays方法时，API会自动将initialize方法返回的DOM元素进行移除。\n\n##### 5、显示和隐藏覆盖物\n\n自定义覆盖物会自动继承Overlay的show和hide方法，方法会修改由initialize方法返回的DOM元素的style.display属性。\n\n如果自定义覆盖物元素较为复杂，您也可以自己实现show和hide方法。\n\n```javascript\n// 实现显示方法    \nSquareOverlay.prototype.show = function(){    \n    if (this._div){    \n        this._div.style.display = \"\";    \n    }    \n}      \n// 实现隐藏方法  \nSquareOverlay.prototype.hide = function(){    \n    if (this._div){    \n        this._div.style.display = \"none\";    \n    }    \n}\n```\n\n自定义其他方法 通过构造函数的prototype属性，您可以添加任何自定义的方法，比如下面这个方法每调用一次就能改变覆盖物的显示状态：\n\n```javascript\n// 添加自定义方法   \nSquareOverlay.prototype.toggle = function(){    \n    if (this._div){    \n        if (this._div.style.display == \"\"){    \n            this.hide();    \n        }    \n        else {    \n            this.show();    \n        }    \n    }    \n}\n```\n\n##### 6、添加覆盖物\n\n您现在已经完成了一个完整的自定义覆盖物的编写，可以添加到地图上了。\n\n```javascript\n// 初始化地图  \nvar map = new BMap.Map(\"container\");    \nvar point = new BMap.Point(116.404, 39.915);    \nmap.centerAndZoom(point, 15);    \n// 添加自定义覆盖物   \nvar mySquare = new SquareOverlay(map.getCenter(), 100, \"red\");    \nmap.addOverlay(mySquare);\n```\n\n\n\n#### 6、信息窗口\n\n![image-20220530112019539](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2q9jsko6aj20pt0acq4w.jpg)\n\n##### 1、提供的信息窗口\n\nInfoWindow：信息窗口。也是一种特殊的覆盖物，它可以展示更为丰富的文字和多媒体信息。注意：同一时刻只能有一个信息窗口在地图上打开。\n\n详情可见[类参考](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a3b7)。\n\n##### 2、添加信息窗口\n\n信息窗口是地图上方浮动显示的HTML内容，可直接在地图上的任意位置打开，也可以在标注对象上打开（此时信息窗口的坐标与标注的坐标一致）。\n\n注意：同一时刻地图上只能有一个信息窗口处于打开状态。\n\n\n\n```javascript\nvar opts = {    \n    width : 250,     // 信息窗口宽度    \n    height: 100,     // 信息窗口高度    \n    title : \"Hello\"  // 信息窗口标题   \n}    \nvar infoWindow = new BMap.InfoWindow(\"World\", opts);  // 创建信息窗口对象    \nmap.openInfoWindow(infoWindow, map.getCenter());      // 打开信息窗口\n```\n\n##### 3、提供的自定义信息窗口工具\n\n百度地图还向开发者提供自定义的信息窗口工具。\n\ninfoBox：自定义信息窗口工具。类似于infoWindow，比infoWindow更有灵活性，比如可以定制border，关闭按钮样式等。\n\n详情可见[类参考](http://api.map.baidu.com/library/InfoBox/1.2/docs/symbols/BMapLib.InfoBox.html)。[点击查看示例](http://api.map.baidu.com/library/InfoBox/1.2/examples/InfoBox_House.html)\n\n\n\n#### 7、叠加图层\n\n\n\n> 地图可以包含一个或多个图层，每个图层在每个级别都是由若干张图块组成的，它们覆盖了地球的整个表面。例如您所看到包括街道、兴趣点、学校、公园等内容的地图展现就是一个图层，另外交通流量的展现也是通过图层来实现的。\n>\n> [DEMO详情](http://lbsyun.baidu.com/jsdemo.htm#g0_3)\n\n![image-20220530112232097](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2q9m2yo0ej20ps0a9gns.jpg)\n\n\n\n提供的图层\n\nTrafficLayer：交通流量图层。详情可见[类参考](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a6b2)。\n\n添加和移除图层\n\n通过map.addTileLayer方法可向地图添加图层，例如下面代码将显示北京市的交通流量。\n\n```javascript\nvar map = new BMap.Map(\"l-map\");         // 创建地图实例      \nvar point = new BMap.Point(116.404, 39.915);  // 创建点坐标     \nmap.centerAndZoom(point, 15);                 // 初始化地图，设置中心点坐标和地图级别     \nvar traffic = new BMap.TrafficLayer();        // 创建交通流量图层实例      \nmap.addTileLayer(traffic);                    // 将图层添加到地图上\n```\n\n若要从地图上移除图层，需要调用map.removeTileLayer方法。\n\n```javascript\nmap.removeTileLayer(traffic);                // 将图层移除\n```\n\n\n\n#### 8、自定义叠加图层\n\n> 百度地图API支持丰富的图层展示，除了支持在“叠加图层页面”介绍的交通流量图层，还支持图片、栅格图，可以根据自己的实际情况实现自定义栅格图层和自定义图层。\n\n##### 1、基础知识——百度地图坐标系\n\n在使用自定义方式叠加图层之前，您需要先了解百度地图的地图坐标系。 百度地图坐标系涉及：\n\n- 经纬度球面坐标系统\n\n- 墨卡托平面坐标系统\n\n- 图块编号系统\n\n###### XY经纬度球面坐标系统\n\n经纬度是一种利用三维空间的球面来定义地球上的空间的球面坐标系，它能够标示地球上任何一个位置。\n\n通过伦敦格林尼治天文台原址的经线为0度经线，从0度经线向东、向西各分180度。赤道为0度纬线，赤道以北的纬线称为北纬、以南的称为南纬。\n\n在百度地图中，东经和北纬用正数表示，西经和南纬用负数表示。例如北京的位置大约是北纬39.9度，东经116.4度，那么用数值标示就是经度116.6，纬度39.9。\n\n在百度地图中，习惯经度在前，纬度在后，例如：\n\n```javascript\nvar point = new BMap.Point(116.404, 39.915);  // 创建点坐标，经度在前，纬度在后\n```\n\n###### XY墨卡托平面坐标系统\n\n由于百度地图是显示在平面上的，因此在地图内部系统中需要将球面坐标转换为平面坐标，这个转换过程称为投影。\n\n百度地图使用的是墨卡托投影。墨卡托平面坐标如下图所示，平面坐标与经纬度坐标系的原点是重合的。\n\n![jsguide05.jpg](https://mapopen-website-wiki.cdn.bcebos.com/static/img/jsguide05.jpg)\n\n###### XY图块编号系统\n\n百度地图在每一个级别将整个地图划分成若干个图块，通过编号系统将整个图块整合在一起以便显示完整的地图。当地图被拖动或者级别发生变化时，地图API将会根据平面坐标计算出当前视野内所需显示的图块的编号。\n\n百度地图图块编号规则如下图所示：\n\n![jsguide06.jpg](https://mapopen-website-wiki.cdn.bcebos.com/static/img/jsguide06.jpg)\n\n从平面坐标原点开始的右上方向的图块编号为0,0，以此类推。在最低的缩放级别（级别 1）中，整个地球由4张图块组成。随着级别的增长，地图所使用的图块个数也随之增多。\n\n\n\n\n\n#### 9、自定义绘制层\n\n> JavaScript API v3.0 的Overlay中，新增了CanvasLayer类。提供了在地图上绘制自定义的Canvas2D或WebGL覆盖物的功能。绘制的覆盖物和其他的Overlay一样，会根据自己的经纬度贴合在地图上。\n\n![image-20220530140900099](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2qef8k5iaj20pr0a4dim.jpg)\n\n创建Canvas2D覆盖物\n\nCanvasLayer类提供了在地图上进行Web前端canvas绘制的功能。具体的canvas绘制方法和逻辑可以参考canvas的相关资料。\n使用CanvasLayer的简单示例如下：\n\n```javascript\nvar mp = new BMap.Map(\"allmap\");\nmp.centerAndZoom(new BMap.Point(116.3964,39.9093), 10);\n\nvar canvasLayer = new BMap.CanvasLayer({\n    update: update\n});\n\nfunction update() {\n    var ctx = this.canvas.getContext(\"2d\");\n\n    if (!ctx) {\n        return;\n    }\n\n    ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);\n\n    var temp = {};\n    ctx.fillStyle = \"rgba(50, 50, 255, 0.7)\";\n    ctx.beginPath();\n    var data = [\n    new BMap.Point(116.297047,39.979542),\n    new BMap.Point(116.321768,39.88748),\n    new BMap.Point(116.494243,39.956539)\n    ];\n\n    for (var i = 0, len = data.length; i < len; i++) {\n\n        // 绘制时需要对经纬度进行转换\n        var pixel = mp.pointToPixel(data[i]);\n\n        ctx.fillRect(pixel.x, pixel.y, 30, 30);\n    }\n}\n\nmp.addOverlay(canvasLayer);\n```\n\n\n\n注意这里的api应该使用3.0版本的 而不是2.0版本的 不然会导致**Uncaught TypeError: BMap.CanvasLayer is not a constructor**\n\n\n\n#### 10、 事件处理\n\n\n\n> 浏览器中的JavaScript是“事件驱动的”，这表示JavaScript通过生成事件来响应交互，并期望程序能够“监听”感兴趣的活动。例如，在浏览器中，用户的鼠标和键盘交互可以创建在DOM内传播的事件。对某些事件感兴趣的程序会为这些事件注册JavaScript事件监听器，并在接收这些事件时执行代码。\n>\n> 百度地图API拥有一个自己的事件模型，程序员可监听地图API对象的自定义事件，使用方法和DOM事件类似。但请注意，地图API事件是独立的，与标准DOM事件不同。\n\n##### 1、事件监听\n\n百度地图API中的大部分对象都含有addEventListener方法，您可以通过该方法来监听对象事件。例如，BMap.Map包含click、dblclick等事件。在特定环境下这些事件会被触发，同时监听函数会得到相应的事件参数e，比如当用户点击地图时，e参数会包含鼠标所对应的地理位置point。\n\n有关地图API对象的事件，请参考完整的[API类参考文档](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html)。\n\n###### 1弹窗事件\n\naddEventListener方法有两个参数：监听的事件名称和事件触发时调用的函数。\n\n如下示例中，每当用户点击地图时，会弹出一个警告框：\n\n```javascript\nvar map = new BMap.Map(\"container\");    \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);    \nmap.addEventListener(\"click\", function(){    \n    alert(\"您点击了地图。\");    \n}\n);\n```\n\n###### 2捕获状态\n\n通过监听事件还可以捕获事件触发后的状态。\n\n如下示例，显示用户拖动地图后地图中心的经纬度信息：\n\n```javascript\nvar map = new BMap.Map(\"container\");    \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);    \nmap.addEventListener(\"dragend\", function(){    \n    var center = map.getCenter();    \n    alert(\"地图中心点变更为：\" + center.lng + \", \" + center.lat);    \n}\n);\n```\n\n##### 2、事件参数和this\n\n在标准的DOM事件模型中（DOM Level 2 Events），监听函数会得到一个事件对象e，在e中可以获取有关该事件的信息。同时在监听函数中this会指向触发该事件的DOM元素。\n\n百度地图API的事件模型与此类似，在事件监听函数中传递事件对象e，每个e参数至少包含事件类型（type）和触发该事件的对象（target）。\n\nAPI还保证函数内的this指向触发（同时也是绑定）事件的API对象。\n\n###### 1事件参数e获取有关信息\n\n例如，通过参数e得到点击的经纬度坐标：\n\n```javascript\nvar map = new BMap.Map(\"container\");    \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);    \nmap.addEventListener(\"click\", function(e){    \n    alert(e.point.lng + \", \" + e.point.lat);    \n});\n```\n\n###### 2this指向DOM元素\n\n监听函数中this会指向触发该事件的DOM元素。\n\n如下示例，通过this得到地图缩放后的级别：\n\n```javascript\nvar map = new BMap.Map(\"container\");    \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);    \nmap.addEventListener(\"zoomend\", function(){    \n    alert(\"地图缩放至：\" + this.getZoom() + \"级\");    \n});\n```\n\n##### 3、移除监听事件\n\n当您不再希望监听事件时，可以将事件监听进行移除。每个API对象提供了removeEventListener用来移除事件监听函数。\n\n###### 1移除监听事件\n\n如下示例中，用户第一次点击地图会触发事件监听函数，在函数内部对事件监听进行了移除，因此后续的点击操作则不会触发监听函数。\n\n```javascript\nvar map = new BMap.Map(\"container\");    \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);    \nfunction showInfo(e){    \n    alert(e.point.lng + \", \" + e.point.lat);    \n    map.removeEventListener(\"click\", showInfo);    \n}    \nmap.addEventListener(\"click\", showInfo);\n```\n\n\n\n#### 11、检索POI\n\n> 简介\n>\n> 检索服务提供某一特定地区的兴趣点位置查询服务（POI：Point of Interest，感兴趣点）。允许设置检索城市、检索圆形区域内POI、检索矩形区域内POI、检索结果详情。\n\n##### 1、提供的检索服务\n\nLocalSearch：本地搜索，提供某一特定地区的位置搜索服务，比如在北京市搜索“公园”。目前支持城市检索、圆形检索、矩形检索。详情可见[类参考](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a7b33)。\n\n说明：BMap.LocalSearch提供本地搜索服务，在使用本地搜索时需要为其设置一个检索区域，检索区域可以是BMap.Map对象、BMap.Point对象或者是省市名称（比如：\"北京市\"）的字符串。BMap.LocalSearch构造函数的第二个参数是可选的，您可以在其中指定结果的呈现。BMap.RenderOptions类提供了若干控制呈现的属性，其中map指定了结果所展现的地图实例，panel指定了结果列表的容器元素。\n\n##### 2、检索POI方法\n\n检索POI服务提供三种检索方法：城市检索、圆形区域检索、矩形区域检索。\n\n###### 1城市检索\n\n[search方法](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a7b0)提供根据关键字检索特定POI信息服务。 如下示例，为根据关键字“天安门”检索POI：\n\n```javascript\nvar map = new BMap.Map(\"container\");      \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);      \nvar local = new BMap.LocalSearch(map, {      \n    renderOptions:{map: map}      \n});      \nlocal.search(\"天安门\");\n```\n\n###### 2圆形区域检索\n\n[searchNearby方法](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a7b0)提供圆形区域检索服务。您可以在某个地点附近进行搜索，也可以在某一个特定结果点周围进行搜索。 下面示例展示如何在前门附近搜索小吃：\n\n```javascript\nvar map = new BMap.Map(\"container\");         \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);      \nvar local = new BMap.LocalSearch(map,   \n              { renderOptions:{map: map, autoViewport: true}});      \nlocal.searchNearby(\"小吃\", \"前门\");   \n```\n\n###### 3矩形区域检索\n\n[searchInBounds方法](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a7b0)提供矩形区域检索服务。矩形范围搜索将根据您提供的视野范围提供搜索结果。\n\n注意：当搜索范围过大时可能会出现无结果的情况。\n\n如下示例，展示在当前地图视野范围内搜索银行：\n\n```javascript\nvar map = new BMap.Map(\"container\");        \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 14);      \nvar local = new BMap.LocalSearch(map,   \n              { renderOptions:{map: map}});      \nlocal.searchInBounds(\"银行\", map.getBounds());   \n```\n\n```html\n<script type=\"text/javascript\" src=\"//api.map.baidu.com/api?type=webgl?v=2.0&ak=C3mLyfNmVlzGD4lZEpsMK0GhuxDPmQKB\">\n```\n\n##### 3、配置搜索\n\nBMap.LocalSearch提供了[若干配置方法](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a7b0)，通过它们可以自定义搜索服务的行为以满足您的需求。\n\n如下示例中，我们调整每页显示8个结果，并且根据结果点位置自动调整地图视野，不显示第一条结果的信息窗口：\n\n```javascript\nvar map = new BMap.Map(\"container\");    \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 14);  \nvar local = new BMap.LocalSearch(\"北京市\",   \n            {renderOptions: {map: map,autoViewport: true},pageCapacity: 8});      \nlocal.search(\"中关村\");\n```\n\n##### 4、结果面板\n\n通过设置BMap.LocalSearchOptions.renderOptions.[panel](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a7b0)属性，可以为本地搜索对象提供一个结果列表容器，搜索结果会自动添加到容器元素中。\n\n如下示例：\n\n```\nvar map = new BMap.Map(\"container\");     \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);  \nvar local = new BMap.LocalSearch(map,   \n            {renderOptions: {map: map,panel: \"results\"});      \nlocal.search(\"中关村\");\n```\n\n##### 5、数据接口\n\n除了搜索结果会自动添加到地图和列表外，您还可以通过数据接口获得详细的数据信息，结合地图API您可以自行向地图添加标注和信息窗口。\n\nBMap.[LocalSearch](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a7b0)和BMap.[LocalSearchOptions类](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a7b1)提供了若干设置回调函数的接口，通过它们可得到搜索结果的数据信息。 例如，通过onSearchComplete回调函数参数可以获得BMap.LocalResult对象实例，它包含了每一次搜索结果的数据信息。 当回调函数被执行时，您可以使用BMap.LocalSearch.getStatus()方法来确认搜索是否成功或者得到错误的详细信息。\n\n在下面这个示例中，通过[onSearchComplete](http://lbsyun.baidu.com/cms/jsapi/reference/jsapi_reference_3_0.html#a7b1)回调函数得到第一页每条结果的标题和地址信息，并输出到页面上：\n\n```javascript\nvar map = new BMap.Map(\"container\");          \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);      \nvar options = {      \n    onSearchComplete: function(results){      \n        if (local.getStatus() == BMAP_STATUS_SUCCESS){      \n            // 判断状态是否正确      \n            var s = [];      \n            for (var i = 0; i < results.getCurrentNumPois(); i ++){      \n                s.push(results.getPoi(i).title + \", \" + results.getPoi(i).address);      \n            }      \n            document.getElementById(\"log\").innerHTML = s.join(\"<br>\");      \n        }      \n    }      \n };      \nvar local = new BMap.LocalSearch(map, options);      \nlocal.search(\"公园\");\n```\n\n需要单独定义一个id为log的div标签\n\n\n\n#### 12、逆/地址解析\n\n地址解析服务提供从地址转换到经纬度的服务，反之，逆地址解析则提供从经纬度坐标转换到地址的转换功能。\n\n![geocoding.png](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2qiz6y3gcj20ak01nweh.jpg)\n\n##### 1、提供的转换类\n\nGeocoder：逆/地址解析，用于坐标与地址间的相互转换。详情见[类参考](http://mapopen-pub-jsapi.bj.bcebos.com/jsapi/reference/jsapi_reference.html#a7b27)\n\n\n\n##### 2、地址解析服务\n\n根据地址描述获得坐标信息。\n\n百度地图API提供Geocoder类进行地址解析，您可以通过Geocoder.getPoint()方法来将一段地址描述转换为一个坐标。\n\n如下示例，我们将地址“北京市海淀区上地10街10号”转换获取该位置的地理经纬度坐标，并在这个位置上添加一个标注。注意：在调用Geocoder.getPoint()方法时您需要提供地址解析所在的城市（本例为“北京市”）。\n\n```javascript\nvar map = new BMap.Map(\"l-map\");      \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);      \n// 创建地址解析器实例     \nvar myGeo = new BMap.Geocoder();      \n// 将地址解析结果显示在地图上，并调整地图视野    \nmyGeo.getPoint(\"北京市海淀区上地10街10号\", function(point){      \n    if (point) {      \n        map.centerAndZoom(point, 16);      \n        map.addOverlay(new BMap.Marker(point));      \n    }      \n }, \n\"北京市\");\n```\n\n##### 3、逆地址解析服务\n\n根据坐标点获得该地点的地址描述，是地址解析的逆向转换。\n\n您可以通过Geocoder.getLocation()方法获得地址描述。当解析工作完成后，您提供的回调函数将会被触发。如果解析成功，则回调函数的参数为GeocoderResult对象，否则为null。\n在构造Geocoder对象时，可以增加参数{extensions_town: true}来获得乡镇级数据，仅限国内。\n\n###### 1指定经纬度获取地址\n\n```\nvar map = new BMap.Map(\"l-map\");      \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 11);      \n// 创建地理编码实例, 并配置参数获取乡镇级数据\nvar myGeo = new BMap.Geocoder({extensions_town: true});      \n// 根据坐标得到地址描述    \nmyGeo.getLocation(new BMap.Point(116.364, 39.993), function(result){      \n    if (result){      \n    alert(result.address);      \n    }      \n});\n```\n\n###### 2鼠标点击地图获取地址\n\n```javascript\nvar map = new BMap.Map(\"allmap\");\nvar point = new BMap.Point(116.331398,39.897445);\nmap.centerAndZoom(point,12);\nvar geoc = new BMap.Geocoder();    \nmap.addEventListener(\"click\", function(e){        \n    var pt = e.point;\n    geoc.getLocation(pt, function(rs){\n        var addComp = rs.addressComponents;\n        alert(addComp.province + \", \" + addComp.city + \", \" + addComp.district + \", \" + addComp.street + \", \" + addComp.streetNumber);\n    });        \n});\n```\n\n\n\n#### 13、出行路线规划\n\n![image-20220530171950351](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2qjxv3qzxj20fc0d5mz2.jpg)\n\n百度地图JavaScript API v3.0提供了驾车、公交、步行和骑行四种出行方式的路线规划功能。开发者在使用线路规划的接口时，可以使用我们提供的默认展示效果。也可以通过监听事件回调，使用检索数据完成自己的需求。v3.0的线路规划服务和v2.0的相比，在功能上有所差异。详细请参考本文档和[JavaScript API v3.0类参考](https://mapopen-pub-jsapi.bj.bcebos.com/jsapi/reference/jsapi_reference_3_0.html)。\n\n提供的路线规划方式\n\n| 路线规划方式 |     类名     |                             简介                             |\n| :----------: | :----------: | :----------------------------------------------------------: |\n|     驾车     | DrivingRoute |                     提供驾车路线规划服务                     |\n|     公交     | TransitRoute | 提供市内公交和跨城交通方式（飞机、火车、大巴）的路线规划服务 |\n|     步行     | WalkingRoute |                     提供步行路线规划服务                     |\n|     骑行     | RidingRoute  |                     提供骑行线路规划服务                     |\n\n##### 1、驾车路线规划\n\n###### 1基础驾车路线规划服务示例：\n\n代码如下：\n\n```javascript\nvar map = new BMap.Map(\"container\"); \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 14); \nvar driving = new BMap.DrivingRoute(map, { \n    renderOptions: { \n        map: map, \n        autoViewport: true \n} \n});\nvar start = new BMap.Point(116.310791, 40.003419);\nvar end = new BMap.Point(116.486419, 39.877282);\ndriving.search(start, end);\n```\n\n###### 2数据接口\n\n驾车导航服务也提供了丰富的数据接口，通过onSearchComplete回调函数可以得到BMap.DrivingRouteResult对象，它包含了驾车导航结果数据信息。 结果会包含若干驾车方案，每条方案中包含了若干驾车线路。 每条驾车线路又会包含一系列的关键步骤（BMap.Step），关键步骤描述了具体驾车行驶方案。\n\n```javascript\nvar map = new BMap.Map(\"container\"); \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 14); \nvar options = { \n    onSearchComplete: function(results){ \n        if (driving.getStatus() == BMAP_STATUS_SUCCESS){ \n            // 获取第一条方案 \n            var plan = results.getPlan(0); \n            // 获取方案的驾车线路 \n            var route = plan.getRoute(0); \n            // 获取每个关键步骤，并输出到页面 \n            var s = []; \n            for (var i = 0; i < route.getNumSteps(); i ++) { \n                var step = route.getStep(i); \n                console.log(step); \n            } \n        } \n    } \n}; \nvar driving = new BMap.DrivingRoute(map, options);\nvar start = new BMap.Point(116.310791, 40.003419);\nvar end = new BMap.Point(116.486419, 39.877282);\ndriving.search(start, end);\n```\n\n##### 2、公交路线规划\n\nBMap.TransitRoute类提供公交线路规划服务。\n\n注意：v3.0中，新增了TransitRoutePlan.getTotal 和 TransitRoutePlan.getTotalType方法，可以获取一条公交换乘方案中总路段数（步行+公交），和指定路段的交通方式类型（步行或公交）。\n\n###### 1使用服务示例\n\n代码如下：\n\n```javascript\nvar map = new BMap.Map(\"container\"); \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 14); \nvar transit = new BMap.TransitRoute(map, { \n    renderOptions: { \n        map: map, \n        autoViewport: true \n    } \n});\nvar start = new BMap.Point(116.310791, 40.003419);\nvar end = new BMap.Point(116.486419, 39.877282);\ntransit.search(start, end);\n```\n\n###### 2进行跨城路线规划\n\n代码如下：\n\n```javascript\nvar map = new BMap.Map(\"container\"); \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 14); \nvar transit = new BMap.TransitRoute(map, { \n    renderOptions: { \n        map: map, \n        autoViewport: true\n\n    },\n\n    // 配置跨城公交的换成策略为优先出发早\n\n    intercityPolicy: BMAP_INTERCITY_POLICY_EARLY_START,\n\n    // 配置跨城公交的交通方式策略为飞机优先\n\n    transitTypePolicy: BMAP_TRANSIT_TYPE_POLICY_AIRPLANE\n\n});\n\nvar start = new BMap.Point(116.310791, 40.003419);\nvar end = new BMap.Point(121.490546, 31.233585);\ntransit.search(start, end);\n```\n\n###### 3结果面板\n\n您可以提供用于展示文字结果的容器元素，方案结果会自动在页面中展现。\n\n```javascript\nvar map = new BMap.Map(\"container\"); \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 14); \nvar transit = new BMap.TransitRoute(map, { \n    renderOptions: {map: map, panel: \"results\"} \n}); \nvar start = new BMap.Point(116.310791, 40.003419);\nvar end = new BMap.Point(121.490546, 31.233585);\ntransit.search(start, end);\n```\n\n###### 4数据接口\n\n您可通过数据接口获取详细的公交方案信息。公交导航搜索结果用BMap.TransitRouteResult来表示，其中包含了若干公交出行方案（BMap.TransitRoutePlan）。每条出行方案由步行线路和公交线路组成。 在起点到上车点之间、下车点到终点之间以及每个换乘站之间都会存在步行线路，如果上述的某两点位置重合，那么其间的步行路线长度为0。 如下示例，通过数据接口将第一条方案的路线添加到地图上。\n\n```javascript\nvar map = new BMap.Map(\"container\"); \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 12); \nvar transit = new BMap.TransitRoute(); \ntransit.setSearchCompleteCallback(function(results) { \n    if (transit.getStatus() == BMAP_STATUS_SUCCESS) { \n        var firstPlan = results.getPlan(0); \n        // 绘制步行线路 \n        for (var i = 0; i < firstPlan.getNumRoutes(); i++) { \n            var walk = firstPlan.getRoute(i); \n            if (walk.getDistance(false) > 0){ \n                // 步行线路有可能为0 \n                map.addOverlay(new BMap.Polyline(walk.getPoints(), {lineColor: \"green\"})); \n            } \n        } \n        // 绘制公交线路 \n        for (i = 0; i < firstPlan.getNumLines(); i++) { \n                var line = firstPlan.getLine(i); \n                map.addOverlay(new BMap.Polyline(line.getPoints())); \n        }\n    } \n});\nvar start = new BMap.Point(116.310791, 40.003419);\nvar end = new BMap.Point(121.490546, 31.233585);\ntransit.search(start, end);\n```\n\n##### 3、步行路线规划\n\nBMap.WalkingRoute提供步行线路规划服务。基本用法和驾车线路规划类似。\n\n###### 1使用服务示例\n\n代码如下：\n\n```javascript\nvar map = new BMap.Map(\"container\"); \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 14); \n    var walk = new BMap.WalkingRoute(map, { \n    renderOptions: {map: map} \n}); \nvar start = new BMap.Point(116.310791, 40.003419);\nvar end = new BMap.Point(116.486419, 39.877282);\nwalk.search(start, end);\n```\n\n##### 4、骑行线路规划\n\nBMap.RidingRoute提供骑行线路规划服务，基本用法和步行线路规划基本相同。\n\n###### 1使用服务示例\n\n代码如下：\n\n```javascript\nvar map = new BMap.Map(\"container\"); \nmap.centerAndZoom(new BMap.Point(116.404, 39.915), 14); \nvar riding = new BMap.RidingRoute(map, { \n    renderOptions: {map: map} \n}); \nvar start = new BMap.Point(116.310791, 40.003419);\nvar end = new BMap.Point(116.486419, 39.877282);\nriding.search(start, end);\n```\n\n![image-20220530173113074](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2qk9n6xz2j21400q20wv.jpg)\n\n\n\n#### 14、定位\n\nJavaScript API提供在Web端获取当前位置信息的方法，融合了浏览器定位、IP定位、安卓定位SDK辅助定位等多种手段，提供了获取当前准确位置、获取当前城市信息等功能。\n\n浏览器定位精度依赖浏览器自身特性，IP定位的精度值为城市级别。\n\n对于安卓WebView页面的开发者，可以结合定位SDK进行辅助定位，使用方法参见[定位SDK相关章节](http://lbsyun.baidu.com/index.php?title=android-locsdk/guide/addition-func/assistant-h5)。\n\n注意：\n\n1. 请求JavaScript API v3.0的定位功能时，必须获取用户授权。\n2. 由于Chrome、iOS10等已不再支持非安全域的浏览器定位请求，为保证定位成功率和精度，请尽快升级您的站点到HTTPS。\n3. iOS15系统下浏览器默认关闭位置请求，需要用户设置为允许/询问后方可获取精确的定位。\n\n提供的定位服务\n\n| **接口**        | **类名**    | **简介**                                                     |\n| --------------- | ----------- | ------------------------------------------------------------ |\n| 浏览器定位      | Geolocation | 优先调用浏览器H5定位接口，如果失败会调用IP定位               |\n| IP定位          | LocalCity   | 根据用户IP 返回城市级别的定位结果                            |\n| 定位SDK辅助定位 | Geolocation | 当您的APP中有内置的Web页面，同时在Web页面需要提供您的当前位置信息时，可调用集成在App中的百度地图定位SDK来获取更精准的位置信息 |\n\n定位方法代码示例\n\n##### 1、浏览器定位\n\n```javascript\nvar map = new BMap.Map(\"allmap\");\nvar point = new BMap.Point(116.331398,39.897445);\nmap.centerAndZoom(point,12);\n\nvar geolocation = new BMap.Geolocation();\ngeolocation.getCurrentPosition(function(r){\n\tif(this.getStatus() == BMAP_STATUS_SUCCESS){\n\t\tvar mk = new BMap.Marker(r.point);\n\t\tmap.addOverlay(mk);\n\t\tmap.panTo(r.point);\n\t\talert('您的位置：'+r.point.lng+','+r.point.lat);\n\t}\n\telse {\n\t\talert('failed'+this.getStatus());\n\t}        \n});\n```\n\n##### 2、IP定位\n\n```javascript\nvar map = new BMap.Map(\"allmap\");\nvar point = new BMap.Point(116.331398,39.897445);\nmap.centerAndZoom(point,12);\n\nfunction myFun(result){\n\tvar cityName = result.name;\n\tmap.setCenter(cityName);\n\talert(\"当前定位城市:\"+cityName);\n}\nvar myCity = new BMap.LocalCity();\nmyCity.get(myFun); \n```\n\n##### 3、定位SDK辅助定位\n\n```javascript\nvar map = new BMap.Map(\"allmap\");\nvar point = new BMap.Point(116.331398,39.897445);\nmap.centerAndZoom(point,12);\n\nvar geolocation = new BMap.Geolocation();\n// 开启SDK辅助定位\ngeolocation.enableSDKLocation();\ngeolocation.getCurrentPosition(function(r){\n\tif(this.getStatus() == BMAP_STATUS_SUCCESS){\n\t\tvar mk = new BMap.Marker(r.point);\n\t\tmap.addOverlay(mk);\n\t\tmap.panTo(r.point);\n\t\talert('您的位置：'+r.point.lng+','+r.point.lat);\n\t}\n\telse {\n\t\talert('failed'+this.getStatus());\n\t}        \n});\n```\n\n\n\n#### 15、鼠标绘制\n\n![image-20220531150242144](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2rllg7cgbj21bj0drdl8.jpg)\n\n##### 1、提供的鼠标绘制工具\n\nDrawingManager：鼠标绘制工具。通过此工具用户可以在地图任意位置上画点、画线、画面并显示线的距离及面的面积。详情可见[开源库](https://lbs.baidu.com/index.php?title=jspopular3.0/openlibrary)。\n\n\n\n\n\n##### 2、鼠标绘制过程\n\n###### 1在页面的头部应用鼠标绘制工具开源库的文件\n\n```html\n<script type=\"text/javascript\" src=\"https://api.map.baidu.com/library/DrawingManager/1.4/src/DrawingManager_min.js\"></script>\n<link rel=\"stylesheet\" href=\"https://api.map.baidu.com/library/DrawingManager/1.4/src/DrawingManager_min.css\" />\n```\n\n###### 2在代码中实例化鼠标绘制工具\n\n```javascript\nvar styleOptions = {\n    strokeColor:\"red\",    //边线颜色。\n    fillColor:\"red\",      //填充颜色。当参数为空时，圆形将没有填充效果。\n    strokeWeight: 3,       //边线的宽度，以像素为单位。\n    strokeOpacity: 0.8,    //边线透明度，取值范围0 - 1。\n    fillOpacity: 0.6,      //填充的透明度，取值范围0 - 1。\n    strokeStyle: 'solid' //边线的样式，solid或dashed。\n}\n    //实例化鼠标绘制工具\nvar drawingManager = new BMapLib.DrawingManager(map, {\n    isOpen: false, //是否开启绘制模式\n    enableDrawingTool: true, //是否显示工具栏\n    drawingToolOptions: {\n        anchor: BMAP_ANCHOR_TOP_RIGHT, //位置\n        offset: new BMap.Size(5, 5), //偏离值\n    },\n    circleOptions: styleOptions, //圆的样式\n    polylineOptions: styleOptions, //线的样式\n    polygonOptions: styleOptions, //多边形的样式\n    rectangleOptions: styleOptions //矩形的样式\n});  \n```\n\n\n\n#### 16、点聚合\n\n> 您常常需要在地图上展示非常多的点。当地图可视范围内的点越多，点与点的图标堆叠、覆盖的效果就会越严重。为了保证展示效果，本节介绍如何使用点聚合功能，聚合区域内的点，使得可以自适应地图的比例尺进行数据展示。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2rlofokrmj20pw0a6q48.jpg)\n\n##### 1、提供的点聚合工具\n\nMarkerClusterer：多标注聚合器。此工具解决加载大量点要素到地图上造成缓慢，且产生覆盖现象的问题。详情可见[开源库](https://lbs.baidu.com/index.php?title=jspopular3.0/openlibrary)。\n\n##### 2、点聚合过程\n\n##### 1在页面的头部应用点聚合工具开源库的文件\n\n```html\n<script type=\"text/javascript\" src=\"https://api.map.baidu.com/library/TextIconOverlay/1.2/src/TextIconOverlay_min.js\"></script>\n<script type=\"text/javascript\" src=\"https://api.map.baidu.com/library/MarkerClusterer/1.2/src/MarkerClusterer_min.js\"></script>\n```\n\n##### 2在代码中添加Marker，实例化点聚合\n\n```javascript\nvar MAX = 10;\nvar markers = [];\nvar pt = null;\nvar i = 0;\nfor (; i < MAX; i++) {\n    pt = new BMap.Point(Math.random() * 40 + 85, Math.random() * 30 + 21);\n    markers.push(new BMap.Marker(pt));\n}\n//最简单的用法，生成一个marker数组，然后调用markerClusterer类即可。\nvar markerClusterer = new BMapLib.MarkerClusterer(map, {markers:markers});\n```\n\n\n\n\n\n#### 百度地图开放平台-示例中心：https://lbsyun.baidu.com/jsdemo.htm#aCreateMap","tags":["百度","Web服务API"],"categories":["AI"]},{"title":"抬头收获缘于低头耕耘","url":"/2022/05/30/缘苑园第61期/","content":"\n\n\n# 缘苑园\n\n> 2022年春季刊 总第61期\n\n​\t一次和一个很怀念的朋友在餐厅吃饭偶然看到柜台上有几本书刊，心想最近事情繁杂索性借一本来坐在店里看看以此让心静下来，告之这是结缘的佛刊。心里顿时泛起一阵涟漪，记得上次阅读佛学经典还是3年前在高阳金陵寺求来的《地藏菩萨本愿经》，这次遇到不免是其中的缘由缘分，故向餐厅工作人员借两本阅读，看到背面有郑州观音寺微信联系方式，添加后求缘最新期刊，今日已收到法宝，谢谢师傅邮寄，祝福法喜。\n\n​\t很早就想在一个饭后的午间，坐在工位前阅读，听耳机里颂唱的心经，听着耳机里颂唱的心经，一上午的忙碌都泡在了茶水里，安安静静的，洗去烦恼和忧虑，我承认我的烦恼很重，当然也不完美，把静静地阅读当作自己的修行吧。那些无关紧要的言语，也与我无关。人活着就是修行，会遇到各种各样的人，各种各样的事，这些都是修行的资粮，困难跨过去了就是一次提升，感恩所有，也感恩遇见，感恩你我的相逢然后天涯。\n\n![image-20220530130138510](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2qch7inymj21hc0o0ted.jpg)\n\n![image-20220530130144096](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2qch8g3i4j21hc0o0dls.jpg)\n\n\n\n","tags":["缘苑园","佛缘"],"categories":["live"]},{"title":"【预训练模型-地图】百度GC ERNIE-GeoL版","url":"/2022/05/25/百度地图GC ERNIE-GeoL版/","content":"\n\n\n# GC ERNIE-GeoL版\n\n> GC ERNIE-GeoL版是融入了百度“地理-语言”预训练大模型文心ERNIE-GeoL能力的地理编码（GC）升级版，大幅显著提升了解析准确率，能够更好满足智慧物流、智慧城市等行业地址解析高准确率的需求。\n\n\n\n## 特点1:全方位的产品能力提升，地理编码服务500米内准确率绝对提升超5%\n\n- 应用能力的持续升级：  \n\n​\t\t应用ERNIE-GeoL预训练大模型，大幅提升GC地理编码对**地址解析和坐标打点**的能力，有效解决了因数据、召回、地址消歧而影响坐标精准度的问题。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kl420rt0j20vo0hsn0m.jpg)\n\n- 显著卓越的效果提升：\n\n​\tGC ERNIE-GeoL版学习『语言-地理位置』的关联知识，增强了对不同形式文本表达的**泛化能力**，500米内准确率绝对提升超5%。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kl4gv177j20w00i4gny.jpg)\n\n- 革新颠覆的开发范式：\n\n​\t相较于传统算法模型，通过使用大规模预训练数据构建的ERNIE-GeoL，只利用**少量任务数据**就可以在下游任务中通过精调的方式取得更好效果。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kl4s02l7j20vo0hsq6t.jpg)\n\n## 特点2:多场景的业务需求拓展，推动地理解析类问题优化\n\n> GC ERNIE-GeoL版功能强化提升，解决了因地址类信息缺少导致的一系列位置解析类问题，强化了产品广泛的应用落地潜力\n\n- 补全道路信息：\n\n通过坐标预测，根据POI名称预测坐标，解决了POI库内目标POI缺乏道路信息的问题。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kl7eeay7j20vo0hsn02.jpg)\n\n- 规避定位失败：\n\n通过坐标预测，根据道路号预测坐标，解决POI库内地址未更新导致的定位失败问题。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kl7unt29j20vo0hs760.jpg)\n\n- 提升poi获取时效：\n\n通过POI和坐标的相关性判断锁定指定区域内的POI，解决因地址缺失导致召回大范围POI发生的超时或失败等问题。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kl8d8nv5j20vo0hs0ue.jpg)\n\n- 多名称地址归一：\n\n通过地理实体语义分析，将同一个地址所对应不同名称建立关联，解决了召回区域的POI无法与目标POI建立关联的问题。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2kl8u0xfmj20vo0hs0vt.jpg)\n\n## 特点3:深层次的模型能力创新，聚焦『地理位置-语言』类任务建模\n\n- 构建预训练数据：\n\n随机生成多源异构数据并进行有效整合，再以统一的形式作为预训练模型的数据输入。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2klaxwwu6j20vo0hsjv0.jpg)\n\n- 创建网络学习结构：\n\n有效的设计网络结构和针对性的预训练目标，学习地理实体的文本属性与其对应地理坐标之间的关联。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2klbi30gpj20vo0hsju2.jpg)\n\n- 设计预训练任务：\n\n设计行之有效的预训练目标，加强模型处理不完整或不规范地理描述文本时的稳定性。\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2klc4mh1qj20vo0hsdhu.jpg)","tags":["百度","ERNIE","文心预训练模型"],"categories":["AI"]},{"title":"工作五年反思-李沐","url":"/2022/05/22/工作五年反思-李沐/","content":"\n\n​\t五年前的今天我飞往西雅图参加亚马逊的面试。面试完后连夜做红眼航班飞往波士顿赶去参加老婆在MIT的博士答辩。答辩一半的时候电话响了，对方说恭喜你面试通过，想聊下薪水。我说其实就面了你们一家，直接给就是，先挂了。\n\n​\t答辩完第二天跟老婆去市政局登记结婚。在宣誓厅门口排队的时候，老板打电话过来，很兴奋的说你来了后可以做这个做那个。我说是挺好的，但先要结婚去了。老板一愣，道了一声恭喜，继续往下说。我不得不打断：得先走了，轮到我们进去宣誓了。\n\n​\t五年一眨眼就过去了。外面来看最大的变化是多了两个娃。但最大的变化来自认知，是人生观、世界观、价值观的改变。博士毕业的时候曾写过我的体会：[博士这五年](https://zhuanlan.zhihu.com/p/25099638)\n\n​\t很多同学留言说深受鼓舞。现在我想同样给大家分享这五年工作中的经验和感悟。更确切说是失败的教训，因为每一点就是付出了学费后获得的教训。希望这些同样能对大家有所帮助和启发。\n\n\n\n## **事情的价值是对社会的价值**\n\n读书的时候，你会有明确的目标，例如考试的分数、深造的学校、或找到好工作。工作后的最大不同是你有太多可以最求的目标。这个带来的改变是你需要决定哪些事情现在做，哪些以后做，哪些可以不去做。\n\n决定优先级应该是根据事情的价值。我现在评估一件事的价值是它对社会的价值，用公式来写就是\n\n> 受益人数 x 人均时间 x 单位时间价值差\n\n这里能从一件事情受益的人数，和受益的人均时间是这件事本身属性。第三项取决于你对这件事完成的好坏，就是你做得比别人做的类似的事情要好，从而受益人从你这里受益比从别人那里多。\n\n这个公式可以用在各种不同的事情上，接下来我们会不断使用它。这里先举几个例子。例如伟大的产品一般具有极高的价值。拿微信来说，它是手机通讯软件，面向几十亿手机用户，每人每天会使用数小时，所以它价值的前两项非常大。因为微信用户体验很好，它比其它替代品的用户体验好给用户带来的价值就是价值差。所以微信是一个非常有价值的产品。\n\n举个小点的例子，例如你带人写一篇论文。论文影响的人数就是这个研究领域的大小；作用时间是别人做一个跟你工作相关的研究所花的时间，可能一辈子就几个月；价值差则是你的研究相在前人工作之上的贡献。这样看来，你需要做热门领域和跨时代论文才能取到高价值。但我们知道一篇论文一般贡献不大、也就几个人会读，所以算下来基本没什么价值，为什么大家还是会积极“灌水”呢？\n\n这里我们还要细看两个价值：一是你通过这个研究熟悉了一个新领域或者新方法，对你个人有学习价值。二是你带人做研究能提升他们在想方法、做实验、和写论文上能力，对他们价值很大。所以即使是知名研究者，名字也会出现在很多新手习作一样的论文上面。\n\n再举一个更小的例子。过去四年里我花在带娃上的时间比工作多。在相当一段时间内都觉得事业被娃耽搁了。直到后来我用这个公式来算：虽然人数只是两个人，但受益时间相当高，一周五十小时以上。而且父亲就一个，有我陪和没我陪对小孩来说区别巨大（自我感觉），所以价值公式的后两项很高。此外，带娃对我个人也有价值，包括如何去理解思维方式完全不一样的他人，以及时刻跟自己想暴怒的冲动做斗争，最终达到佛性的状态。这样算下来心里就顺了。\n\n## **服务社会最后也是服务自己**\n\n上面这个价值公式强调的是事情对他人的贡献。在用它之前，我的价值公式更关注自己。例如我常用一件事情的好玩程度，或者里面的技术含量来划分优先度。问题是虽然享受做事情这个过程，但之后的成就感不高。有点类似打完游戏后的空虚。因为做完后经常发现，这个虽然酷炫但没什么用，没多少人理睬。原因是对个人的有直接高价值的事情，对他人价值不一定大。很有可能这件事情本身只对很小群人有意义，可能每个人受益时间短，或者其实是重复造轮子，市面上已经有了差不多的替代品。\n\n如果优化对社会的价值，你会得到对自己的延后回报。这个回报包括了你知道做这个事情对他人有用时带来的更高层次的内心满足，以及他人从你这里受益时给与的馈赠（给你点赞、或者老板给你升职加薪）。当然，这两者不一定同时出现。很多时候你创造的价值不一定被别人关注（数十年维护那些大家用起来习以为常的开源工具包），也有时候大家会夸大吹捧你的贡献。你应该积极寻求别人的肯定，这会给你更多的资源做更大事情。但你应该更关心内心的满足，因为更可控、不容易别他人误导。更多是它会给你内在动力去把事情做得更好，这是你能不断成长的根基。\n\n## **技术最终是为产品服务**\n\n技术专业的同学刚进入公司通常会继续做技术。刚毕业那会儿我觉得进入大公司就是做技术，成为世界上最好的技术专家之一。而且不要做产品，因为如果做产品的话我为什么不自己创业呢，赚的钱还是自己的。后半句没什么问题，但前半句忽略了技术最终是为产品服务这一事实。虽然因为公司的不同，对技术进入产品的预期时间会不同，但通常在半年到五年之间。预期是超过五年的公司屈指可数，而且大多已经作古。所以就算你在公司的研究部门，也应该知道公司对于技术落地时间的预期。否则时间一到就会面临公司削减不达预期的技术的投资。最坏情况是你们上新闻了：某某公司研究院院长离职，部门成员各奔东西。\n\n那么什么样的技术能进产品？通常你会根据公司现有的产品有个大致的想法。接下来你要知道这个产品的主要价值是什么（套用之前的公式）。然后你需要去琢磨你做的技术对这个产品的价值。如果你的技术能提升产品的核心功能，哪怕是一点点，也会得到资源来落地技术。例如提升微信的视频压缩技术、今日头条的推荐算法、苹果的外壳材料。反过来，如果没有抓住主干，例如微信装皮肤、今日头条网页版加速，苹果操作系统兼容其他硬件。就算你可以做到比现有技术提升很多，产品团队也可能没动力帮你，甚至一开始就告诉你别做这个。\n\n所以不管你是在产品团队做技术，还是在公司研究院，都应该对产品的价值有所了解。例如深入理解产品经理的口头禅：市场、刚需、痛点、高频。同时也应该知道你手上的技术对产品的价值，用它来指导你对技术路线的规划。\n\n## **不想当将军的士兵不是好士兵**\n\n人的满足感来自于对比，不管是对比别人还是对比自己过去。这个欲望驱动你去追求有更大价值的事情。这意味着你需要更多资源去做大做强。最起码的是你需要一个团队。你可能是这个团队的管理者、领导者、或者兼任两者。\n\n也许你更喜欢一个人做技术，至少我一开始是这么想的。但随着你的能力的增长，别人对你的责任的期望也越大，你不可避免得去带一个团队。否则你得去其他地方找满足感。与其别动的被推到了这个位置，不如一开始就做好准备。\n\n这里有大量的职场书籍可以参考。我自己的经验很简单：领导者是带路者，需要有好眼光。管理者是后勤官，让团队执行高效。下面分别解释这两点。\n\n## **放眼在三年以后**\n\n领导者最重要的是在带着团队探索未知领域时找出正确的方向。也就是说保证你们做的产品或技术是有价值的。因为做一件事需要时间，所以你得预判事情在未来的价值。如果判断不准，大家辛苦做了很久，做完后发现效果一般，那么团队士气就会低下。各种问题就会接踵而来。\n\n你去想一件事情未来的价值时，时间不要太短也不要太长，三年比较合适。假设你想继续沿着现在的方向走，那么需要考虑三年后你关注的用户群和使用时间是不是会发生变化。变多是好事，不变表示你做不了太大，但如果会变少，你得考虑要转向了。你还需要警惕新技术的出现，很可能新的技术会短短几年就完全推翻旧技术（深度学习、智能手机、电动车）。分析那些失败的例子，当事人其实很早就察觉到了新技术，但低估了它的能量。他们只看到了新技术比现有成熟技术的不足，然后套用成熟技术的发展速度在新技术上，低估了三年后新技术能到达的高度，和用户喜新厌旧的程度。\n\n如果你要做一个新的方向，那你可能不再有技术积累优势，就是跟别人比你给用户带来的价值差可能不明显，甚至更低。那么你需要找到好赛道（投资人口头禅），通常是颠覆性的新技术，以及随之带来产品和用户的变化。只有在快速变化的赛道上，新入局者才更容易通过更准确的预测未来的价值来弯道超车。也就是乱世出英雄。\n\n好的眼光需要一个长期的训练。你需要不断的去做深入思考，获得自己独特的观点，而不是靠朋友圈里大家的高见。所以你需要时不时放下手头的事情，给自己空出时间做深入思考。例如我会时不时去家附近的Bay Trail走上几个小时，边走边想。\n\n## **管理的核心是诚心待人**\n\n如果你有一个明确的团队目标，和一个高质量的团队，高效执行是水到渠成的事情。所以管理者有三个核心事情：招人、留住厉害的成员、和帮助落后的。招人最理想是招比自己厉害的人。另外是每次招的人都比同级别的一半人厉害，这样能保证团队扩张时能不断提升团队质量。\n\n能力突出的成员在哪里都会受欢迎。你的一个任务是让他们能尽可能长的留在团队里（虽然最终是要走的）。一个办法是把自己放在他们的位置，想象你想你的领导如何待你。例如我自己最希望的是不断做有更大价值的事情（成就感），并从中学到新东西（个人提升）。在我困难时候老板能给与支持（经常发生）。其他的都可以换算成当前待遇，例如可以多少时间做不喜欢的事情（不赞同一件事的价值，但又没能说服别人不做）、上下班路上很堵、食堂没中餐。所以大方向上是创造轻松的环境、每年能新立项有价值的项目、和尽量给大家争取待遇。\n\n对于绩效不理想的队员，你需要经常性的指出问题并给予建议，如果一段时间没改进则需要讨论是不是当前项目不合适。如果仍然无进展的话，那只能帮助他们换组，或者要求他们离开。同样，你需要把自己代入对方的位置，明白想得到什么样的帮助和尊重。绝大部分时候，不是他们人不行，只是你们不合适。愉快的分手能让前成员更快的找到更合适的职位（从而避免他们给你寄刀片）。\n\n## **专注！专注！**\n\n有人说创业公司一个常见死因是在有了一定成绩后盲目扩张。这个在哪里都成立。不管你是一个人，带一个团队，还是领导一家公司，资源总是有限。集中资源在最有价值的事情才能保证成功。例如苹果好几十万人，但对于产品线的扩张上非常谨慎。从而能保证每一款产品都砸上足够多资源来颠覆市场。在初期你也许可以广撒网多捕鱼，一旦事情的价值慢慢清晰，我们需要逐步集中资源。因为同时把做几件类似的事情最好，不如只把一件事情做到极致、做到市面上最好。这样你总是可以得到正的价值差。一个第一比十个第二好，第三通常都活不长。同样的道理也可以用在生活、社交、和学习上。\n\n## **只要投入力气，短板可以变成优势**\n\n以前每次发布MXNet的新特性时，知乎同学都是吐槽：回去写好文档先。大家都知道程序员不喜欢写文档。我从小语文和英语都是在及格线徘徊，更是心有抵触。17年的时候痛下决心来写文档，我把我所有留下做技术的时间都花在上面，最后跟大家一起写出了《动手学深度学习》这本教科书，现在被全世界近200所大学采用做教材。所以，你的不足能成为你的机会。只要你直面它，狠下心来花力气，不断去改进，你的短板会变成优势。\n\n## **扬长避短**\n\n所有命运的馈赠，都在暗中标好了价格。当我把精力都花在文档上时，便忽略了MXNet本身。没能组织投入大量资源去持续提升它的性能和易用性，导致它没能做到前二。从价值上来说，《动手学深度学习》和MXNet在用户数和用户价值上差不多，但用户使用深度学习框架时间多于读教材，所以MXNet价值更大。在不擅长的领域花费了大力气打赢一仗，但在优势领域失去了价值更大一个，很难说是划得来。所以，在扬长和补短上面，一定是要根据价值来判断，而不是面子。\n\n## **分布式系统里通讯开销才是大头**\n\n当一台机器算力不够时，我们用多台机器协同工作来共同完成任务。虽然分布式系统是很多家互联网公司的基础架构，但提升性能仍然是很难。因为每台机器实际算力会有不同，时不时还会罢工。而且一台完成自己的小任务时，经常需要等其他机器任务的结果，导致频繁数据通讯和等待。所以大家都知道提升性能的关键是减少通讯开销。\n\n当你需要一个大团队来协同做一件事时，同样通讯开销是大头，优化起来比分布式系统能难，因为人的差异性和不稳定性比机器大多了。人的能力不同，做事效率不同。每个人分工的不同，导致做事方式也不样，甚至优化的目标都不一样。大部分人只关心自己的事情，不想也不愿操心别人的事情。如果不能有效把所有人拧在一起，就是一盘散沙，做不成大事。如果你刚进职场，最关键的一点是你需要意识到：你需要预留足够多的时间和精力来沟通。不要抱怨这是你们公司的制度问题，这是大团队作战时的固有现象。\n\n## **升职**\n\n我因为运气不错升到了一个比较高的职位，从而有机会经常参加公司的从高级工程师、科学家到总监的升职评定。虽然公司、职位、级别不同带来差异性，但总体来说，一个人能否升职成功取决于她做的最大项目对公司的价值是不是达到这个职位的要求。这里有三个要点：一是项目对公司的价值。意味着针对的人群和价值差都是公司关心的，而不是你个人或者社会关心的。这里价值通常就是给公司赚了多少钱，或者3-5年后可能会赚多少钱。二是看的是你最大的项目要够“档次”。累积很多项目，想通过不看功劳看苦劳升职可能是行不通的。三是你在项目中的贡献，例如你负责多大一块，是贡献了代码、团队协调、宣传、制定计划、还是申请到了资源。一个常见误解是跟人合作会降低我的贡献。如果你和合作者配合不好，导致1+1远小于2，那么你的贡献确实降低了。但如果通过合作把项目价值做大了，那么你分到的贡献是不会少的。特别是如果项目价值上了一个档次，那就更好了。\n\n升职有一个经常被忽略的“潜规则”是影响力。随着职位的升高，公司对你的影响力的期望也越高。从能影响一个小团队，包括制定技术路线、帮助队员上手、解答疑惑、甚至是帮助别人来完成工作，到影响隔壁组（经理），影响隔壁部门（高级经理），影响隔壁集团（总监），最后到影响整个公司战略（副总裁）。\n\n除非你是天生的领导者，不然你得花力气去培养自己的影响力。简单来说是在管好自己的事情外，积极的去帮助别人。当别人信你、咨询你意见、愿意找你合作时，那你就有了对他们的影响力。你可能会觉得帮别人会耽误自己的活。但从公司角度来看，是需要鼓励这种奉献精神，而且要予以奖励。此外，你从中赢下的信任给你带来名声和人脉，长远来看是很有用的。\n\n## **加薪**\n\n大公司里薪酬相对透明，每个级别对应的薪酬通常可以在网上找到。一个级别内薪酬有浮动，一般有个最小值和最大值。80%在中值附近，两头各10%，分别是刚升到这个级别的人和快要到下一个级别的人。可以简单的认为，随着能力的提升，你的薪酬会从最小值逐步跳到最大值，然后升职到下一个级别对应的区间。\n\n不同级别的薪酬中值通常是个等比序列，而不是等差。例如比你高一级的人可能工资比你多一半，但高三级的人不是比你多150%，而是多238%。在这个模型里，你需要优化你的五年后，或者十年后能到达的高度。所以在比较offer时，你不要太关心它们之间的数字差价，而是关心去你去了之后的发展（你说我现在多拿点去买币，说不定马上就财富自由了，那也是一个思路 ）。\n\n## 总结：专注于最有价值的事情\n\n如果把这五年的感悟精炼成一句话的话，会是很平淡的一句：专注于最有价值的事情。首先，你需要对价值有清晰的认识。接着，对一件事情，不仅是要认识当下的价值，更多的是对未来价值的预测。其次，当你通过不断的快速试错对未来有了把握的时候，你需要逐步的把你能调用的资源专注到最有价值的那一件事情上，尽你可能的做好。\n\n如果一生中能做好几件有着极大价值的事，那也就值了。\n\n写此文的时候惊闻袁隆平老师逝世。谨以此文纪念他伟大的一生：专注于杂交水稻，创造了人类历史上最伟大的价值之一。我辈楷模。\n","tags":["李沐","反思"],"categories":["AI"]},{"title":"基于知识图谱的推荐系统综述","url":"/2022/05/20/基于知识图谱的推荐系统概述/","content":"\n\n\n# 基于知识图谱的推荐系统综述\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ezxmg6f5j20u006ndgo.jpg)\n\n> ​\t本文是2020年针对知识图谱作为辅助信息用于推荐系统的一篇综述。**知识图谱对于推荐系统不仅能够进行更精确的个性化推荐，而且对推荐也是具有可解释性的，有迹可循**。\n>\n> ​\t本文汇总了近些年来知识图谱辅助推荐系统的一些研究工作，并按不同的方法进行划分类别（**下图**是我根据论文画出的大纲方法类别图)；除此之外，汇总了不同场景下的知识图谱数据集，涵盖7个场景；最后阐述了未来的一些可研究方向及趋势。\n\n\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2evsov3u6j20u00l8abv.jpg)\n\n## 一、背景知识\n\n推荐系统已经广泛应用在实际生活中的很多场景，特别是个性化推荐系统已经有越来越多的研究工作和落地实践，但是仍然面临着一些问题，例如数据稀疏、冷启动等问题。\n\n近年来，利用知识图谱作为辅助信息生成推荐已经引起了人们相当大的兴趣，这种方法不仅可以缓解上述问题，更准确的进行个性化推荐，而且可以对推荐的结果也是可解释的，是有迹可循的。\n\n下面我们将分别来看一下这两方面。\n\n### 1.1 推荐系统\n\n推荐系统在实际生活中已经有很多的应用场景，比如我们所熟知的电影、音乐、POI、新闻、教育、书籍，购物等。\n\n**推荐系统的目的旨在为 user (用户) 推荐一个（或一系列）未观测的 item (物品，电影，新闻等)**。基本步骤如下：\n\n1. 学习 user 和 item 的向量表示  和  .\n2. 根据 1 中的 user 和 item 向量表示，计算表示 user 对 item 的偏好得分，得分函数可以采用內积、DNN等。\n3. 基于 2 中计算的得分，进行排序推荐。\n\n推荐系统主要包含以下三种方法：\n\n- **Collaborative Filtering (CF): 基于协同过滤的推荐系统**，协同过滤算法是从相似度度量出发，考虑 user 或者 item 之间的相似度进行相关推荐，它比较常用的两种方法是基于内存（memory-based）和基于模型（model-based）的两种方法。\n- **Content-based Filtering (CB)：基于内容的推荐系统**，与基于协同过滤从全局 user 和 item 的交互数据中学习他们的向量表示相比，基于内容的推荐方法从 item 的内容中学习 user 和 item 的表示。它认为 user 可能对与他们过去交互过的 item 中相似的 item 感兴趣。\n- **Hybrid Method：混合推荐系统**，CF 方法容易遇到冷启动或者交互矩阵数据稀疏的问题，而混合推荐系统可以利用基于内容的推荐系统中的 user 和 item 信息来缓解这一问题。混合推荐系统通过将 user 和 item 的内容信息，即用户辅助信息和物品辅助信息整合到 CF 的框架中，可以获得更好的推荐性能。\n\n### 1.2 基于知识图谱的推荐系统\n\n近段时间，基于知识图谱的推荐系统(KG-based recommendation system, KGRS)引起研究者的广泛兴趣，主要是把知识图谱作为辅助信息整合到推荐系统中，这样的做法带来两个方面的优势，**其一是能够提高推荐系统的准确性，其二是能够为推荐系统提供可解释性。**\n\n**准确性**：知识图谱可以用来表示实体之间的关系，可以将 item 及其属性信息映射到知识图谱中，以理解 item 之间的相互关系，此外，还可以将 user 和 user 的辅助信息整合到知识图谱中，更准确地捕捉 user 和 item 之间的关系以及 user 的偏好。\n\n​\t**下图所示是一个基于知识图谱的推荐系统**，我们来简单看一下，KG 中包含了电影(圆形代表)、用户，演员和导演(人头像代表)以及电影风格(摄影机代表)这几种实体节点，实体之间又包含了几种不同的关系，通过这个知识图谱，给 Bob 推荐了两部电影 “Avatar”《阿凡达》和 “Blood Diamond”《血钻》。**看图能够看出，电影和用户之间有着不同的潜在关系，有助于提高推荐的准确性。**\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2evsoizzxj20l10aqjsf.jpg)\n\n**可解释性：** 基于 KG 的推荐系统的另一个优点是推荐结果具有可解释性。在上面的图中，通过遵循图谱中的关系序列，我们可以知道向 Bob 推荐这两部电影的原因。例如，**推荐《阿凡达》的一个原因是，《阿凡达》与 Bob 之前看过的“Interstellar”《星际穿越》属于同一类型风格的电影**。\n\n​\t**下图中**列出了一些流行的知识图谱，根据所涵盖知识的范围，这些知识图谱可分为两类，一类是 cross-domain 的知识图谱，另一类是 domain-specific 的知识图谱，也就是说一类是包含知识广的通用型知识图谱，一类是包含特定领域知识的垂直领域知识图谱。\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ezxh13zxj20u008njt9.jpg)\n\n下面我们将看一下知识图谱如何作为推荐系统的辅助信息？是怎么应用的？以及有哪些代表性的Model。\n\n\n\n## 二、Methods\n\n通过对最近研究的相关调研，发现基于 KG 的推荐系统对 KG 的应用有三种方式:\n\n- **基于 Embeddig 的方法(The embedding-based method)**\n- **基于路径的方法(The path-based method)**\n- **联合的方法(The unified method)**\n\n下来将在各小节了解一下对应的方法，在这之前，先给出下面的一章图片，列出相关的符号和概念。\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2evzfz69fj20p00e2764.jpg)\n\n### 2.1 基于Embeddig的方法\n\n基于 Embedding 的方法通常直接使用来自知识图谱的信息来丰富 item 或 user 的表示。为了充分利用 KG 的信息，需要应用KGE/KRL算法将 KG 中的实体和关系映射到低维向量空间。KGE 算法可分为两类：基于翻译的模型，如 TransE、TransH、TransR、TransD等；语义匹配模型，如 DistMult等。\n\n根据 KG 是否包含 user，又将这类方法分为两部分，即 **item graph** 和 **user-item graph**。\n\n#### 2.1.1 使用 item graph\n\n在该方法中，KG 由 item 及其相关属性组成，这些属性是从数据集或外部知识库中提取的。我们将这样的图命名为 **item graph**。注意，user 不包括在 item graph 中。\n\n**这类方法利用 KGE 等模型对 item graph 编码获取更加丰富的 item embedding，然后结合 item 的多种信息构成完整的 item 表示**，例如 user-item 交互矩阵信息、KG 信息、item 属性信息、item 内容信息等。然后再单独计算 user 的表示(可以从交互矩阵中获取)和得分函数。\n\n得分函数的公式如下，其中 user 和 item 的向量表示分别为  和  .目的是计算 user 选择 item 的可能性大小，然后排序之后返回相应的 item。 这里的 f 可以是內积、DNN等。\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ewxgaba9j20lh01vt8j.jpg)\n\n> 在这类方法中，有一些**典型的模型**代表，如 CKE、DKN、KSR。\n\n**我们拿 CKE 模型来简单说一下**：CKE 模型在 CF 框架中整合了各种辅助信息，它们将 item 的结构化知识(item graph)以及内容知识(文本、图像等)采用 KGE 方法编码整合到一起。\n\n具体来说，采用 TransR 算法对 item 的结构化知识  进行编码，对文本特征  和图片特征  采用自编码结构进行提取，这些表示与从 user-item 交互矩阵中抽取到的偏移向量  一起整合，具体公式如下(2)。再获取到 user 的向量表示后，采用两者内积的方式计算 user 选择 item 的可能性大小，然后排序。\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ewyl0i2qj20lk01uq2s.jpg)\n\n#### 2.1.2 使用 user-item graph\n\n**该方法直接构建 user-item graph**，其中 user、item 及其相关属性作为节点(实体)。在 user-item graph 中，属性级关系(品牌、类别等)和 user 相关关系(co-buy、co-view等)作为边(关系)。 在利用 KGE 编码得到相关的实体表示后，既可以利用 item graph 中的公式(1)计算 user 的偏好，也可以将关系向量考虑进去，采用新的计算方法，如下：\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ewyma6phj20lc01x744.jpg)\n\n> 在这类方法中，有一些**代表性模型**如 CFKG、SHINE、DKFM。\n\n**我们简单的说下 CFKG**：CFKG 是构建了一个 user-item 知识图谱，即 user-item graph，在这个 KG 中，user、item 及其相关属性作为实体，用户的一些历史行为(如购买、提及)被视为实体之间的一种关系类型，并且包含了多种关系类型的 item 辅助信息(如评论、品牌、品类、购买组合等)。为了更好的学习实体以及关系的 Embedding，重新定义了一个距离计算函数，如下：\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ex0vwp6yj20j401st8j.jpg)\n\n#### 2.1.3 其他的方法\n\n以往的一些研究工作一般直接利用 KGE 技术学习到的 user 或者 item 表示进行推荐。最近，有些研究工作尝试通过**改进 KGE 方法学习到的实体/关系表示来提高推荐性能**，例如结合 GAN 的 **KTGAN** 方法,以及结合 TransE、GNN 和贝叶斯框架的 **BEM** 方法。\n\n除此之外，另一种趋势是采用**多任务学习**(Multi-task Learning)的方法，在一些基本 KG 相关任务的共同学习下做推荐任务。大概来说，有一个推荐系统的任务 f 用于从 user-item 交互矩阵中学习，推荐 user 感兴趣的 item，对应的在 KG 的三元组分类任务 g 中，判断这个三元组是否有效，这两个任务在损失函数部分结合，共同学习。这样学习的一个大概动机在于推荐系统中的 item embedding 共享来自 KG 中的实体 embedding。这类方法的一些典型代表如：**KTUP、MKR、RCF**等。\n\n### 2.2 基于 Path 的方法\n\n基于 Path 的方法构建 user-item graph，并利用 KG 中实体的连通性模式进行推荐，**基本思想是考虑到 user 和/或 item 之间连通相似性（实体语义相似性），进而提升推荐效果**。\n\n根据 path 的不同使用方式又做了细分，主要是基于 path 的连通相似度和把 path 嵌入到低维空间，获取 path embedding，我们下面分别看一下。\n\n#### 2.2.1 path 的连通相似性\n\n**这种方式是利用计算不同路径下实体之间的语义相似性，并作为一种正则方法优化 user 和 item 的表示**，进一步就可以采用公式(1)中內积的方式计算 user 选择 item 的偏好可能性。\n\n有三种类型的实体相似性方法如下：\n\n**User-User Similarity:** 如果 user 之间具有较高的元路径相似度，那么将迫使 user 在向量空间中接近。\n**Item-Item Similarity:** 与上面的类似，如果 item 基于元路径的相似度高，则 item 的向量表示应该接近。\n**User-Item Similarity:** 如果 user 和 item 的元路径相似度很高，那么 user 和 item 的向量就会非常接近。\n\n> 这个方法的典型模型有: FMG、Hete-MF、HeteRec、HeteRec_p、Hete-CF、SemRec、HERec、RuleRec。\n\n#### 2.2.2 Path embedding\n\n**这种方法直接学习连接 user 和 item 之间的显式 path(部分/所有) embedding，以便直接对 user-item 的关系建模。**\n\n具体来说一下，假设存在 user 和 item 之间存在 K 个路径，针对其中的路径 p ，学习到其向量表示为  ，最终的路径信息如下，其中 g 可能是 max-pooling 或者是加权的 sum-pooling。\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ey9casalj20l301v0sl.jpg)\n\n接下来可以采用下面的方式计算 user 对 item 的偏好。\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ey9n131sj20l701oa9w.jpg)\n\n> 这一方法的代表模型如：MCRec、RKGE、KPRN、PGPR、EIUM、Ekar。\n\n### 2.3 联合的方法\n\n**基于 embedding 的方法利用 KG 中 user/item 的语义表示进行推荐，而基于路径的方法使用语义连通信息，而且这两种方法只利用 KG 中一个方面的信息**。\n\n为了更好地利用KG中的信息，提出了将实体和关系的语义表示与路径连通信息相结合的统一方法，**统一的方法是基于 Embedding 传播的思想**。**这些方法以 KG 中的路径连通性为指导精炼了实体表示(user/item)**。\n\n这里面也是分为了两类方法，具体的下面简单看一下。\n\n#### 2.3.1 基于 user 的历史行为\n\n这个基本思想是利用 user 在历史交互行为中交互过的 item 以及 item 的多跳邻居这些行为丰富 user 的表示信息。\n\n丰富的 user 表示可以表示如下，其中 ![image-20220520163811552](https://tva1.sinaimg.cn/large/e6c9d24ely1h2eylpxlh8j200w00t0hu.jpg) 代表 multi-hop ripple sets，g 代表 concatenate embeddings 的操作。\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2eyko47mij20lh02qa9y.jpg)\n\n因为传播是从 user 参与的 item 开始的，所以这个过程可以看作是在 KG 中传播 user 的偏好。\n\n> 代表的模型如：RippleNet、AKUPM、RCoLM\n\n#### 2.3.2 基于 item 的多跳邻居\n\n这种方式是利用 item 的多跳邻居(multi-hop neighbors)![image-20220520164017465](https://tva1.sinaimg.cn/large/e6c9d24ely1h2eylsql9xj200y00q0iz.jpg) 来丰富 item 表示。一个通常的表示如下：\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2eym3w3mcj20lj02v746.jpg)\n\n![image-20220520164152198](https://tva1.sinaimg.cn/large/e6c9d24ely1h2eynako4wj200x00v0io.jpg)是候选 item ![image-20220520164221322](https://tva1.sinaimg.cn/large/e6c9d24ely1h2eynvp02sj200s00q0d3.jpg) 的 ripple set，g 代表 concatenate embeddings 的操作，concatenate 要做两步的操作。\n\n首先要学习候选 item 的 k 阶邻居的表示：\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ez9151uij20lk0320sn.jpg)\n\n然后更新![image-20220520170339215](https://tva1.sinaimg.cn/large/e6c9d24ely1h2eza1qa6gj200l00r0ec.jpg)![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ez99e8dtj20ks026dfp.jpg)\n\n其中 agg 可以是 Sum Aggregator、Concat Aggregator、Neighbor Aggregator、Bi-Interaction Aggregator等。\n\n> 典型的代表模型如：KGCN、KGCN-LS、KGAT、KNI、IntentGC、AKGE.\n\n### 2.4 Methods 小结\n\n基于 Embedding 的方法使用 KGE 方法对 item graph 或 user-item graph 的 KG 进行处理，获取实体和关系的 embedding，并进一步整合到推荐框架中。但是，**该方法忽略了 KG 的信息连通性，缺少可解释性**。\n\n基于路径(Path)的方法通过预先定义元路径或自动挖掘连接模式，利用 user-item graph 来发现 user 或 item 的路径相似度。**基于路径的方法还可以为推荐的结果的提供可解释性**。\n\n联合方法是将基于 Embedding 的方法与基于路径的方法相结合，充分挖掘两方面的信息，是当前的研究趋势。此外，联合方法还具有解释推荐过程的能力，具备可解释性。\n\n根据上述的方法分类，将其用下面的一张图来表示：\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ezdcpyy6j20u00l8abv.jpg)\n\n基于知识图谱的推荐方法\n\n上述代表性的模型在下图中全部列出：\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2eze0h51uj20jn0i940k.jpg)\n\n## 三、DataSets\n\n基于 KG 的推荐系统除了具有准确性和可解释性之外，另一个优点是这种类型的辅助信息可以很自然地结合到不同应用的推荐系统中。为了证明 KG 作为辅助信息的有效性，基于 KG 的推荐系统在不同场景下的数据集上进行了评估。在本节中，我们将根据数据集对这些工作进行分类，如下：\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h2ezka2x1dj20d90hcdgw.jpg)\n\n## 四、未来研究方向\n\n虽然已经提出了许多新的模型来利用 KG 作为推荐的辅助信息，但仍存在一些机会。在此概述和讨论一些未来的研究方向：\n\n**1 动态推荐(Dynamic Recommendation)**：现有的大多数方法都是采用用户的静态偏好推荐。然而，在某些情况下，如在线购物、新闻推荐、Twitter 和论坛，用户的兴趣可能会很快受到社会事件或朋友的影响。在这种情况下，使用静态偏好建模的推荐可能不足以理解实时兴趣爱好。为了捕获动态偏好，利用动态图网络可以作为一种解决方案。\n\n**2 多任务学习(Multi-task Learning)**：知识图谱中可能存在丢失的事实，从而导致丢失关系或实体，用户的偏好也可能因此而被忽略，从而导致推荐结果的恶化。将知识图谱补全和推荐系统联合训练可以有效提高推荐效果。\n\n**3 跨领域推荐(Cross-Domain Recommendation)**：由于不同领域的数据信息不一致，交互数据也不等同，例如，在亚马逊平台上，图书评论比其他领域更多，然而不同领域的交互数据可以互为补充，因此通过迁移学习等技术，可以共享源领域域中数据相对丰富的交互数据，以便更好地推荐目标领域。\n\n**4 知识增强的语言表示(Knowledge Enhanced Language Representation)**：增强知识的文本表示策略应用于推荐任务中，可以更好地学习 user/item 表示，获得更准确的推荐结果。\n\n**5 知识图谱 Embedding 方法(Knowledge Graph Embedding Method， KGE)**：虽然现在已经将 KGE 方法应用到上述基于知识图谱的推荐系统中，然而，没有一些研究工作表明在数据源、推荐场景和模型架构等不同情况下，应该采用何种特定的 KGE 方法。因此，另一个研究方向是比较不同 KGE 方法在不同条件下的优势。\n\n**6 用户辅助信息(User Side Information)**：目前，基于 KG 的推荐系统大多是通过加入 item 辅助信息来构建 KG，很少有模型考虑 user 辅助信息。然而，如用户社交网络等信息也可以自然地整合到当前基于 KG 的推荐系统框架中。因此，在知识图谱中考虑 user 辅助信息可以是另一个研究方向。\n\n## 总结\n\n本文对基于知识图谱的推荐系统进行了研究，总结了近年来该领域的研究成果。知识图谱不仅能够作为辅助信息来改善推荐效果，并且能够为推荐提供可解释性。此外，还介绍了不同场景中使用的数据集以及指出了未来的研究方向，希望能促进该领域的发展。\n\n[1] Guo Q, Zhuang F, Qin C, et al. A Survey on Knowledge Graph-Based Recommender Systems[J]. arXiv preprint arXiv:2003.00911, 2020.\n[2] Zhang F, Yuan N J, Lian D, et al. Collaborative knowledge base embedding for recommender systems[C]//Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 2016: 353-362.\n[3] Zhang Y, Ai Q, Chen X, et al. Learning over knowledge-base embeddings for recommendation[J]. arXiv preprint arXiv:1803.06540, 2018.\n[4] Wang H, Zhang F, Wang J, et al. Ripplenet: Propagating user preferences on the knowledge graph for recommender systems[C]//Proceedings of the 27th ACM International Conference on Information and Knowledge Management. 2018: 417-426.","tags":["综述","联合学习"],"categories":["知识图谱","推荐系统"]},{"title":"知识图谱在小米的应用与探索","url":"/2022/05/19/知识图谱在小米的应用与探索/","content":"\n\n\n### 知识图谱在小米的应用与探索\n\n## 彭力 小米\n\n> 小米知识图谱于 2017 年创立，已支持公司了每天亿级的访问，已赋能小 \n>\n> 爱同学，小米有品、智能问答、用户画像、虚拟助手、智能客服等互联网产品。 \n>\n> 通过引入知识图谱，这些产品在内容理解、用户理解、实体推荐等方面都有了显 \n>\n> 著的效果提升。\n>\n> 本文的主要内容包括：\n>\n> ​\t小米知识图谱介绍：包括小米的商业模式、小米人工智能部、知识图谱在人工智 \n>\n> 能部的定位、小米知识图谱的发展历程、以及小米知识图谱的落地场景。 \n>\n> ​\t小米知识图谱关键技术：小米知识图谱在成长过程中的技术积累。\n>\n> ​\t小米行业知识图谱探索：结合业务，跟大家分享下小米在行业图谱上的探索。\n\n\n\n## 01、小米知识图谱介绍\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pcy84k16j20qg0e4q4l.jpg)\n\n在了解小米知识图谱之前，先介绍下小米的商业模式。小米在商业模式上提出硬件+新零售+互联网铁人三项的商业模式。这种商业模式下有像手机、小米音箱类的智能硬件；有米商城，有品电商这样的新零售；还有像人工智能这样的互联网服务。三者相扶相持，相互促进，是一种闭环的生态模式，在这种生态模式下，有很多潜在的应用场景，对人工智能，对内容和知识有很多诉求。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pd17c2c6j20oz0f3wga.jpg)\n\n小米人工智能部已经构建了完整的中台体系，囊括了视觉、NLP、知识图谱、语音、深度学习等底层的基础能力，其中知识图谱就处于这一层。 \n\n中间层是问答服务、智能客服等应用能力层，上层是小爱同学、商城等互联网业务和传统业务层，这些都是知识图谱的落地场景，其中小爱同学是小米公司推出的虚拟人工的智能助理，小爱同学适用于手机、音响、电视、手表以及手环等穿戴设备，通过搭载小爱同学的智能硬件，可以满足用户获取知识和信息的需求。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pd2r80iaj20pf0c83zm.jpg)\n\n小米知识图谱在中台体系下不断的成长，2017 年小米知识图谱有了一些开放知识的积累， 2018 年知识图谱团队成立，2018 年底，通用知识图谱的构建，百科类图谱构建完成，2019 年中，业务拓展，线上调用达到近亿次，2019 年底，知识扩增，知识积累了超三百亿，2020 年行业探索，行业图谱落地。虽然发展的比较晚，但是在自己的业务场景下，发展还算迅速。 \n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pd4kst96j20qj0fwgny.jpg)\n\n小米知识图谱在公司的职责，主要是研究开放领域和行业领域的构建和应用技术，并把图谱推广到相关业务场景上，来提高用户的满意度的和业务变现转化能力。团队已构建超三百亿开放知识和涉及 13 个领域。除此之外，小米还参与了一些开放知识图谱的构建，是 OpenKG 之 OpenBase 子项目组主要成员单位，是 IEEE知识图谱国家标准编制组主要成员。\n\n\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pd66f7uoj20q50foq3z.jpg)\n\n小米知识图谱已经已经赋能公司 10+个业务场景，这些落地场景包括智能问答、 智能客服、小爱同学、虚拟助手、全局搜索、NLP 等这样通用的知识领域。还有像游戏中心、广告，小米有品，小米网等这样的行业知识，下面我会重点介绍一下具体场景的细节。\n\n### 1、应用场景：智能问答\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pd7jtqmkj20q30eqwfn.jpg)\n\n第一个是小米知识图谱在智能问答场景的应用，这个比较广泛，落地的设备较多， 已服务于手机、音响、智能穿戴、智能车载、电视、儿童设备。应用于小爱音响、小爱同学、小寻手表、车载设备等，满足用户近亿次/天的请求，后面我们介绍落地场景的示例。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pd8fkxkzj20rc0h340t.jpg)\n\n目前，智能问答包括两种模式：一种是一般问答模式，还有一种是规则推理的。 一般问答场景下，在返回具体答案的同时，还会把关联实体的附加信息满足给用 户，比如用户询问巩俐的籍贯的时候，返回答案不只是会返回山东济南，还会把 问答实体巩俐的视频，人物关系，资讯新闻，代表作品等都呈现给用户，这样在 用户兴趣激发上起到了很大作用。另外一个古诗词 CASE，也能很好的体现这一 点，比如用户问静夜思的作者是谁，用户除了想得到这首诗的作者外，可能还想 温故这首诗，也可能想要了解这首诗的释义。所以我们会把有声资源、释义一并满足给用户。 \n\n最后，问答在歧义场景下还支持列表形式展示。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pfs7i7tkj20qm0fignt.jpg)\n\n除了一般的问答方式以外，小米还支持推理的问答。比如：多条件推理，多跳关系推理，还支持像求最大值，最小值这种基础推理算子。多条件推理的例子如： \n\n山东籍的双子座是谁，首先会对数据库中人物实体的生日推理出星座是双子座，然后推出省份，最后筛选聚合产出实体结果 ，第二种是多跳关系推理，比较典型的就是人物与人物的六度关系推理，如：徐志摩与梁思成的儿子梁从诫是什么关 系？我们会试图计算起始实体到目标实体的关系的最短可达路径呈现给用户。现有的推理逻方法，比如说基于规则的推理、基于模型的推理，规则推理主要包含规则引擎和一阶的逻辑规则。模型推理是用机器学习去表示学习关系推理。所以这里根据自己的需求、应用场景和应用情况去选择。 \n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pdctlosgj20rc0fowgh.jpg)\n\n后面介绍一些基于智能问答的一些方法。基于图谱的智能问答，通用流程如下： 语音识别环节，意图识别，实体匹配，实体查询返回结果。 \n\n举个例子，如武汉大学周边什么好吃的，首先做分词或者词法分析，分出武汉大学和好吃的这些关键 mention，然后意图识别计算得到是美食需求的，第三步是实体识别，把 mention 武汉大学映射到知识图谱中的实体上，把属性好吃映射成推荐食物，最后实体查询计算，返回热干面，武昌鱼，豆皮，油焖大虾。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pdermyj3j20qe0f4jtn.jpg)\n\n小米基于知识问答有很多方法，**第一种是基于模版的方法，**它的大体流程是这样的：\n\n第一步对 query 做实体链接（实体链接技术在第二部分会详细介绍），第二步把实体名用实体链接后的主实体对应的实体类型替换后去离线的模板库匹配，返回模板库中映射后的归一的模版，最后查询实体库返回答案。 \n\n举个例子： \n\n姚明的老婆是谁，第一步先做实体链接，后面把姚明的实体类型人物替换姚明，去人物垂域模板规则库查询模板，发现命中了 lambda_x.配偶这个模板。最后在图谱数据国查询姚明的配偶，返回答案叶莉。这种方法有一个好处就是准确率比较好，是离线挖掘的模板，所以性能也比较好，但是缺点也比较明显泛化能力差。 其中模板的挖掘方法的话，主要是离线从知识图谱中实体中找目标实体对，然后去问答论坛去匹配问题与答案分别出现的 pair，生成模板的 pair，这么做会有很多噪声需要做进一步过滤，比如：需要过滤掉出现多属性的问答对的情况和频次 出现比较低的情况。 \n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pdicbbv1j20re0fu409.jpg)\n\n**为了解决第一种方法泛化性能比较差的问题，用第二种方式基于槽填充的方式来互补。**第一种方法在李白有哪些诗的时候，可以命中模版库满足用户的需求。但是变成李白有名的诗有哪些时，就无法找到答案了。为了解决这种问题，我们用了槽填充和意图识别联合学习的方法方法，借鉴了 2016 年 liu 的基于 attention的意图检测和插槽填充联合学习的的方法。该方法把槽填充与意图识别联合的学习，方法包含两部分槽填充和意图识别，两部分组成，第一部分是槽填充问题转化为序列标注的 NER 问题，第二步是意图识别，把意图识别转换为文本分类问题。 \n\n最后把两个问题整合做一个联合学习。PPT 右下角已给出论文和代码。该方法在部分垂哉上的召回的提升比较明显。在菜谱，古诗垂域上欠召回的 badcase 解决率为 30%\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pdnqyycpj20qy0fg760.jpg)\n\n​\t**第三种方法是基于子图检索**，该方法依赖于实体的关系路径。具体第一步 query 、做实体链接，把实体转化为实体 ID，第二步根据实体周围的属性筛选出候选路径。 第三步对输入文本与候选路径进行实体语义相似和排序，取 top 结果。 以姚明老婆的国籍是啥为例子，第一步用实体链接找到用到接接到知识图谱姚明这个实体；第二步就是找到姚明这个实体周边的候选的属性路径，如姚明的配偶的国籍，姚明配偶的身高，姚明配偶的类型，姚明教练的出生日期，姚明队友的出生地等；第三步用 bert 计算候选路径和目标路径的相似关系，除了相似度外， 引入了像类型过滤这样的条件约束，过滤给出排序分值然后取一个最大值。以上都是基于图谱的结构化的问答场景，对于非结构的, 比如：天空为什么是蓝色的，怎么控制猫的饮食量，青蛙王子是不是安徒生的童话，这三种为什么，怎 么样，是不是，类型的问题，以上方法无法解决，需要通过基于搜索的 FAQ 的方式，这里就不介绍了。\n\n\n\n### 2、应用场景：智能客服\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pdptldhgj20r30gyq5r.jpg)\n\n第二个是知识图谱到智能客服的场景。目前智能客服已经落地小米网和小米金融等业务场景下。PPT 中是智能客服团队用 NL2SQL 的方法在基金客服上的一个落地场景。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pds910aaj20ql0g440g.jpg)\n\n知识图谱在智能客服中的技术框图体系，第一层是数据标准化层，主要包括数据仓库，数据治理，数据融合，第二层是 AI 引擎层，有实体抽取引擎，属性集合引擎、知识图谱引擎等，第三层是数据共享交换层，第四层是数据服务，数据分析等。\n\n### 3、应用场景：小米商城&游戏中心\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pdtp2b4xj20r40eddi0.jpg)\n\n第三个应用场景是在小米商城和游戏中心的应用，目前商品图谱和游戏图谱已应用到小米商城，有品商城，游戏中心等业务下。已落在有品商城/小米商城的场景词搜索发现、用户 sug 引导、商品评价的用户观点的的用户观点的抽取及聚合，及游戏的评论的观点抽取及聚合业务上。在小米的商品图谱取得不错的效果，已助力商品转化率、用户购买转化率及游戏下载率至少有 30%的提升。\n\n\n\n### 4、应用场景：AI虚拟助手\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pdvsvjgtj20r90ff76b.jpg)\n\n另外小米知识图谱还在多模态图谱应用场景下做了尝试，与 AI 虚拟助手合作探索了图片态与文本态实体语义关联，目前已上线植物识图的功能，后面会持续的扩展。小米知识图谱的落地场景很多，这里只介绍了一部分，后面是小米知识图谱积累的一些关键技术。 \n\n\n\n## 02、小米知识图谱关键技术\n\n### 1、小米知识图谱赋能各业务场景\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pdz5tqkoj20py0f5dgr.jpg)\n\n目前小米知识图谱已经具备 20+关键能力，比如实体链接，实体融合，概念图谱， 实体推理，实体分类，知识理解，实体关联，用户理解等等，后面挑出实体链接，实体融合，概念图谱挖掘三个关键技术和大家分享探讨。 \n\n### 2、关键技术：实体链接\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pe07r7ocj20rf0dzdhl.jpg)\n\n**实体链接 ( Entity Linking )，**也叫实体链指，该任务要求我们将非结构化数据中的表示实体的词语(即所谓 mention，对某个实体的指称项)识别出来，并将从知识库 ( 领域词库，知识图谱等 ) 中找到 mention 所表示的那一个实体，所以实体 链接的任务定义：就是给定文本 mention,判定指代知识图谱中的实体首先第一个是实体链接 ( Entity Linking )\n\n​\t比如说刘德华的天下无贼主题曲那一天是谁唱的，实体链接需要把刘德华，天下无贼，那一天三个 mention 联接到知识图谱的实体上。以方便应用到如主题分析，语义的信息检索等更深度的应用场景下。 \n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pfsq7yi4j20qu0fzgnm.jpg)\n\n​\t常见的实体链接如 PPT 流程。包括中文的切词，命名实体识别，候选实体选取， 实体消歧，实体排序，判空几部分。第一步中文切词有很多方法，比如像结巴等一些开源的工具，我们的做法是整合了已有的实体名、实体同义词名，及开放锚文本信息做为词典，用维特比算法构造了切词功能。除了切词外我们还用的序列标注的方式做了命名实体识别，把实体词表与 NER的结果合并。\n\n​\t其中 NER 用的是 BERT+CRF。在 NER 的训练数据集构造上，起初用远程监督的方法构造训练集的方法，但是发现在句子中有多个实体词的情况，远程监督的方式只能标注出部分实体词，这样对模型的召回影响比较大。所以我们利用开放比赛的标注数据作为数据集，再加上部分远程监督的数据和人工标注的数据作为最终的训练样本。这种方式的训练结果比只有远程监督的样本训练的结果提升 10个点左右。 \n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pft2tkrwj20qu0fzgnm.jpg)\n\n接下来第二步是候选实体选取，我们离线挖掘了大量的同义词，别名，缩写词等， 在图谱实体。命中 label，alias，同义词，缩写的作为候选对象。但是调研中发现过多的候选词不一定有好的效果，比如：长尾的，互动比较少、丰富度比较少的实体引入会造成很多噪声并且很影响处理性能。因此我们利用用户使用的热度， 实体的流行度，实体丰富度等对候选实体做了筛选和过滤。精简后准确率提升了3%，召回下降 0.4%，预测速度提升50%。\n\n\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pftbj090j20rg0gntbd.jpg)\n\n接下来是实体消歧，实体排序，判空这三块。这三块不好解耦，所以可以一块来 说。这里用到了两种特征，**第一种是上下文无关的，第二种是语义相关联的特征。**\n\n上下文无关的特征包含：实体流行度，用户热度，实体丰富度等等。\n\n**语义相关的特征包括三部分：** \n\n① 对输入实体 mention 预测实体类型, 用到的 18 年 Raiman, J. R., & Raiman, O. M.发表的\"DeepType：用神经网的分类系统演化来做多语言实体链链接\"的方法，该方法基于当我们知道了候选实体的类型之后，这个消歧的任务便被解决得差不多了的假设将实体链接过程看成是分类获取的过程。分类的过程是针对知识库中的分类体系设计了一个 DeepType 的预测系统。具体是用输入数据文本通过bert 编码取 CLS 位置的向量、候选实体对应开始和结束位置对应的特征向量，三个向量连接，经过全连接层，最后 softmax 激活得到候选实体的类别得到分类。 \n\n② 是 DeepMatch 部分，参照 18 年 Le, P., & Titov, I 的一种通过候选实体与mention 之间的潜在关系建模来提升实体链接的效果。该文章提出了将实体链接问题转化为文本语义匹配问题，构建了一个 DeepMatch 模型来匹配输入语句的上下文和候选实体的描述信息对。把待消歧文本作为 text_a，每个候选实体的SPO 全部连接起来组成一段文本 text_b，计算 text_a 和 text_b 的相关性 。训练时选取连接到的实体作为正例，在候选实体里选取负例。两个句子长度最大选取为 256，负样本选取了 3 个。取 CLS 位置向量、候选实体对应开始和结束位置对应的特征向量，三个向量连接经过全连接层，最后 sigmoid 激活得到候选实体的概率得分。 \n\n③ 除了这两个特征外还有共现、协同推断等特征。最后把是否存在多个同义词指向 同 一 个 实 体 、 其 他 mention 是 否 出 现 在 该 实 体 的 信 息 里 、 LinkCount 、DeepMatch 模型的相似度、DeepType 模型的相似度等经过 MLP 得到一个分值，排序取 top1 的实体，如果 top1 的分值大于阈值就判定该实体，如果小于阈值则 为空。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pftha0cbj20qm0fst9g.jpg)\n\n小米知识图谱通过该方法参加了 2020CCKS 比赛，很荣幸拿到了总决赛的第一名，F1 的值达到了 0.8954。但是这种方法在我的业务场景，准确率召回可以达到 96%以上。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pftkfzoaj20qi0ghdgj.jpg)\n\n另外，除了效果，这里在业务上有会有处理性能的问题，所以这里用到三种方法加 速 ， 第 一 是 引 用 了 tensorflow 的 batching serving， 第二是把 bert中transformer 用 nvidia 的 faster Transformer 替换，第三是用 Fp16 的方法量化，这种加速效果比较明显的 QPS 从 30 提升到 1200。 \n\n### 3、关键技术：知识融合\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pftsemkfj20q90fiwgm.jpg)\n\n第二种关键技术，是知识融合，该任务的定义是，给定实体集合，识别并合并等价实体 ( 注：等价定义为待融实体指代了现实世界中同一事物或概念 ) 。举这个例子，花木兰电影有来自腾讯，爱奇艺，优酷，豆瓣，电视猫， 维基的数据。需要把实体化后的小实体，找到归一组，合并融合生成新的实体，更新至知识库图谱中这一过程中称为知识融合。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pfu1l1adj20ps0fognf.jpg)\n\n​\t基于任务定义，把这种任务，拆解成了实体对齐和实体择优两部分。 实体对齐的方法目前包含成对的实体对齐，集体实体对齐，大规模集体实体对齐及知识库与知识库之间的模式层的实体对齐。小米着重做的是成对对齐，现在用了就两种的方法： \n\n第一种方法是传统的方法，基于观察的先验，比如： \n\n① 类别间的属性重要度是不同的 ( 比如人物中，出生时间，出生地点，性别，职业很重要；地点类的，经度，纬度很重要；视频: 上映时间，演员，导演，角色很重要；生物：种属科目纲很重要等 )。 \n\n② 文本中的时间，地点很重要，( 比如一些 infoxbox 中未覆盖的事件的时间及地点等 ) 基于这两个经验，我用一些 tfidf 的方法计算一些属性在不同类中的重要性，并找文本中的时间/地点做为一个重要的文本特征，并计算对应属性值相似度，目前用对一些相似度主要是一基于字粒度的文文相似度，及 token 粒度的主题相似度等。 \n\n第二种方法用基于 embedding 的 deep Match 方法，主要参照了 2018 年 ACM SIGMOD 的方法做了一些改进，该方法把实体中的每个属性下的 O 的 Value concate 成一个句子，通过双向 LSTM 等一模型 encodeing 成向量，计算每个属性下的 emdming 的相似度，最后经过一个分类模型，判断是否是同一个实体。该方法没有考虑类别中的属性重要度的差异，所以准确与召回效果都不太理想，我们也在考虑更多的方法尝试改进。以上两种方法是针对对于结构化实体对齐的方法，如果是开放文本要依赖实体链接技术。\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pfu6hxijj20pq0etjse.jpg)\n\n知识融合第二部实体择优，是在经过实体对齐后，把实体属性的差异性或者冲突性做消解。目前的做法基于以下几个方面对实体的质量进行控制控制： \n\n-  实体的更新时效性 \n\n-  权威性，不同来源，权威性不同的，比如，人民网的权威性要比一般咨询类的站会要高 \n\n-  丰富性，不同来源 O 的值缺失程度是不同的 \n\n-  共现频次，当多源有冲突时，可以用投票的选出不同来源中出现最多的属性\n\n### 4、关键技术：概念图谱\n\n\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pfufsgvrj20r20g00ui.jpg)\n\n概念图谱的概挖掘目前小米图谱基本三种方式构建。\n\n**第一种是在本体模式层构建了分类体系**， 分类体系参照了一些开放的行业和分类 \n\n标准，还参考了一些人工整理的行业的标准体系。 \n\n**第二种是基于 autophrase 的方法**，是实例层的 ISA 关系的挖掘，该方法是 2017 \n\n年一篇论文中采用海量文本挖掘的方法，该方法通过主要是用短语挖掘的方法来 \n\n挖掘概念。这种方法需要满足四个条件： \n\n- 流行度：质量短语应该出现的频率足够高 \n\n-  一致性：token 在高质量短语中的搭配出现的概率明显高于预期 \n\n-  信息性：短语可以表达一个特定的主题或概念 \n\n-  完备性：一个短语可以在特定的文档上下文中解释为一个完整的语义单元\n\n\n\n这个模型的训练用实体的长文本和内容文本、远程的 Wikipedia/cn_probase 拿到的开放的的高质量的短语及根据不同领域标注的高质量的词语三个输入作为输入语料。第二步用 n_gram 的候选筛选，出正样本与负样本，正样本是 N_grame频率大于阈值和人工标注的领域短语及人工 cnproese 匹配的高质量短语；剩余是负样本。由于负样本中掺杂大量的正样本，所以后面是从负样本中使用集成分类器训练了多个基分器来从负样本中强化出正样本。为了保证概念短语的质量，方法通过词性分析过滤不符合语法的短语。\n\n\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pey1c7foj20qd0fxta5.jpg)\n\n**针对概念挖掘的第三种方法是基于序列标注的方法。**分为两步。第一步做一个分类，针对实体长文本描述进行句子拆分，之后判断 否有这个概念相关的一个实体词。第二步使用 Bert+BiLSTM+CRF 的方式作序列标注，标注出 SPO 的值。 \n\n上面三种方法都是概念挖掘，对于实体与概念的关联，可以用实体分类的方法把模式层的与实体挂接，用实体链接的方法把开放词中的短语与体挂接。\n\n### 5、关键技术：自动化构建技术\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pezi1t7uj20qu0g1jsu.jpg)\n\n除些之外呢，小米图谱还在工程构建已有了一套完成的自动化构建技术，可以支持用户定制，自动实体化，自动实体关系等。 \n\n## 03、小米行业知识图谱探索\n\n小米知识图谱的关键技术还有很多，我们在这里只给大家介绍典型的几个关键技术，有兴趣的话可以线下交流。最后我们看一下小米知识图谱在行业的一些探索。\n\n### **1. 商品图谱**\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pf3oyosrj20qv0g9wgr.jpg)\n\n**第一个业务场景的探索商品图谱，**主要的应用场景是小米商城，小米品的搜索和推荐场景，目标就是辅助电商平台精准的搜索。 \n\n现在商品图谱已在商品分类体系的建设、主商品词提取、商品同义词挖掘、上下位体系构建、场景概念挖掘五个方向构建完成。其中分类体系是在模式层的构建； \n\n主商品词提取和商品同义词挖掘用于精确匹配与召回；上下位体系结构用于用户推荐；场景概念挖掘用于搜索发现及场景推荐。 场景挖掘以泰国旅游为例，可以与沙滩鞋，电话卡，浮潜装备等商品有关联，烧烤场景可能与烧烤架，木炭，食材等商品关联。 \n\n目前商品图谱已把这五个方向的数据和技术落地到小米商城，有品商城上。用户转化率和商品转化率都有不错的提升。 \n\n### 2、上位词\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pff7wxxpj20qm0ftjtm.jpg)\n\n上位词挖掘的方法分为三部分： \n\n**第一部分是上位词判定**，用 bert 加上分类模型从用户日志的 query 中提取出来确识别是否是商品词或者上位词。**第二部分通过层次化的分类器**，对挖掘到的上位词合并到分类体系中，这里用的了 HMC 的多分类器。第三部分是把商品与上位词关联，用商品名做 texta, 上位词做 textb，把关联问题转化为文本分类问题。 \n\n目前用这种方法挖掘出的上位词，平均每个商品覆盖 10.5 个上位词。 \n\n### 3、同义词\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pfgp268bj20q70ft0up.jpg)\n\n商品图谱涉及到的还有一个就是同义词挖掘。我们是从商品标题中抽取同义词， 在调研中发现，很多商铺为了尽可能多的命中搜索词，会把可能多的把相同相近或者同义的词堆砌到商品名中。所以基于这个假设，我们把同义词的挖掘，转化为一个序列标注问题。 \n\n其中训练数据用人工标注+ ( 通用图谱+同义词库 ) 远程数据作为训练样本。商品 title 做为 texta, 候选的词做为 textb 最后标注出 BIOS。因为店铺除了堆砌到同名商品外，还会打包买一些东西，比如锅盘垫与炒锅盖打包卖，所以这样做会有准确的问题。为了这种问题，我用了以下三种方法去噪： \n\n-  检测上位关系是否冲突，锅盘垫->餐具->餐垫，炒锅盖->锅具->锅盖等 \n\n-  用词向量相似度 \n\n-  用 bert 相似度计算分类判断是否同义\n\n用三种方法过滤后我们的准确率达到 94%。\n\n### 4、金融图谱\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pfupy6vfj20rc0gn40j.jpg)\n\n第二个行业落地场景，是客服团队金融图谱在小米金融信贷及保险等业务的应用。 \n\n我们就业务场景中的身份核实的子功能举例：\n\n- 碰撞识别主查核实多个用户的公司地址是否为同一公司 \n\n- 关联方探查，是判断申请贷人与信息是否一致\n\n后面就是金融知识图谱的框图：\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1pfkrup55j20ql0g5tax.jpg)\n\n除了商品及游戏及金融行业的应用外，我们在更多的行业图谱的落地及更多的通用图谱的应用场景也在持续探索中。\n\n## 04、总结\n\n简单总结下，小米知识图谱已构建超百亿的知识，落地 10+的业务场景，拥有 20+ 个技术能力，拥有成熟的自动化构建流程，小米知识图谱已有多个行业知识图谱落地。最后，欢迎大家体验/使用小爱同学等小米的产品，也欢迎大家吐槽！","tags":["事件图谱","小米"],"categories":["AI"]},{"title":"百度事件图谱技术与应用","url":"/2022/05/19/百度事件图谱技术与应用/","content":"\n\n\n# 百度事件图谱技术与应用\n\n## 陈玉光 百度 资深研发工程师\n\n> 目前百度事件图谱已构建了千万级规模的事件图谱，在收录时效上达到分 \n>\n> 钟级。事件图谱技术已应用到搜索、信息流等百度内部的产品中，相关能力也对 \n>\n> 外输出到媒体等多个行业。另外，事件图谱的前沿推理技术在金融领域的探索也 \n>\n> 初步取得成果，并已形成一个金融事件归因和预测的推理平台。本文将为大家详 \n>\n> 细介绍百度的事件图谱技术与应用，主要内容包括：\n>\n> - 事件图谱概述\n> - 事件图谱技术\n> - 事件图谱应用\n> - 总结与展望\n\n### 事件图谱概述\n\n#### \t1、知识图谱\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1o868ynkaj21060kbju3.jpg)\n\n知识图谱构建的目的，其实是为了把世界上纷繁复杂的、各种各样的客观事实通过图谱的方式组织起来，让机器能够通过这个图谱更好地实现理解、抽取、推理等任务，从而提升我们处理信息的效率。最基础的知识图谱包含实体、关系、属性等图谱最核心的知识，而更广义的知识图谱则包含更多的知识，如概念、标签、评论等。**事件也是一种知识，一种比较复杂的知识。**事件知识的复杂性在于事件并不会对应到现实世界中的某一个具体的实体，它是一系列实体，以不同的角色在一段时间内参与活动的一个抽象。 \n\n百度于2017年底就开始了事件图谱方向的研究。经过了三年的发展，目前已经在多个不同的方向和领域落地。\n\n#### 2、事件图谱\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1o8co5td6j20q50f3mys.jpg)\n\n事件图谱是什么？事件图谱是包含事件、事件属性、事件间关联关系的以事件为 基本单位的知识网络。上图是以宋仲基、宋慧乔从相恋到结婚、再到离婚这么一 系列事件及其相关内容所组成的事件图谱的示意图。每个事件都有自己的事件类 型，不同的事件类型拥有着不同的角色。如双宋结婚事件，它在事件图谱中的内容就是，一个结婚类型的事件，这个类型的事件有两个角色，其中，丈夫角色的值是宋仲基，而妻子角色的值是宋慧乔。事件之间的关系也在事件图谱的构建围内，包括时序、因果、从属等。除了事件图谱的内容外，在上图中，我们还可 以看到事件图谱与实体图谱是有对应关系的，事件的发生是可能会触发实体与实体间关系或属性的变化的，例如，上面的例子中，结婚事件的发生导致了双宋的关系从男女朋友关系变成丈夫与妻子的关系。 \n\n#### 3、为什么要做事件图谱？\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1obx0f538j20qk0flq5i.jpg)\n\n为什么要做事件图谱？有两个方面原因： \n\n第一方面，事件其实更符合人类对客观世界的理解。例如，以桃园三结义这幅图为例。在没有知识或弱知识的时候，我们可以知道这幅图里有三个人，有酒和树。在有实体知识的时候，我们可以知道这三个人的姓名是刘备、关羽和张飞，树是桃树。直到拥有事件知识的时候，我们才可以知道具体时间在东汉末年，刘备、关羽和张飞三个人在桃园做了结义这一个事情。我们可以看到，只有到事件这个 层次上，才能说真正对这幅画有一个比较完整的把握。\n\n另一方面，事件图谱可以对动态的客观世界进行建模。实体图谱会记录了实体的属性和关系，当实体的属性或关系更新时，图谱只会保留它最新的状态。但通过事件图谱，我们能够对属性或关系的整个变化过程进行建模，并且可以将这些事件与变化的进行一一对应。\n\n#### 4、事件图谱是事件处理智能化的关键\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1oc5myi4qj20ql0extbv.jpg)\n\n​\t我们目前正处在一个信息爆炸的时代，无论是个人消费者看新闻，还是专业从业 者跟进行业事件消息，我们都不可避免地受到信息过载的影响。一方面存在太多 无意义的信息分散了我们的注意力，另一方面当我们关注某个事件时，却又无法 快速、全面地获得信息。怎么样去智能化地对事件进行处理，帮助我们更高效的**DataFunTalk：**专注于大数据、人工智能技术应用的分享与交流。 获取和处理事件信息呢？关键就是事件图谱。智能地对事件进行处理的三个方面：事件理解、资源组织、分析决策都需要事件图谱。 \n\n**事件理解**：以“山西饭店倒塌事件”为例，如果没有事件图谱，我们只能得到与 这个单独的事件相关的内容。而通过事件图谱，我们还能得到它的一些后续事件， 如“国务院督办”等。基于这样一种事件级别的理解，我们就可以把原来只是聚 焦于一个单一事件的内容升级为对事件的上下文、前因后果都能联系起来的一种 事件脉络的呈现，从而大大提升用户获取事件信息的效率。 \n\n**资源组织**：在没有事件图谱的情况下，事件的信息载体通常是新闻资讯，这些新 闻资讯常常存在重复、角度不一、来源混杂的情况。在事件图谱的基础上，我们可以将各种资源，包括图片、视频、资讯、观点等，围绕事件进行组织，从而帮 助用户更高效更全面地获取事件信息。 \n\n**分析决策**：在没有事件图谱辅助的情况下，决策常常是决策者结合现实情况和主 观经验做出的，在多数情况下，我们很难判断决策者是否已经考虑了所有的现实 情况，并且对未来可能发生的不同的方向进行了足够的考虑。而基于事件图谱， 我们能够让决策者清晰地了解一个事件可能的原因和结果，从而使决策者能够做 出更加全面及有根据的考虑。 \n\n\n\n### 事件图谱技术\n\n刚刚介绍了事件图谱的概述，包括是什么是事件图谱，为什么要做事件图谱。接下来将进一步介绍事件图谱的关键技术：\n\n#### 1、百度事件图谱技术概述\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1ocb3krd8j20qc0fidi0.jpg)\n\n这是百度事件图谱的技术概览图，主要分成四个层次：数据层、构建层、认知层和应用层。本次分享的重点是事件图谱的构建层技术。\n\n#### 2、事件图谱构建\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1ocf3zp5dj20qh0fhwgl.jpg)\n\n> 事件图谱跟传统的知识图谱在构建上的区别： \n\n​\t首先，就图谱的内容上而言，通用知识图谱主要包括实体、关系、概念、上下位 等，而事件图谱则主要包括事件、事件属性以及事件间的关系。 其次，就知识来源上而言，通用知识图谱常常存在一些优质的结构化来源 ( 如垂 类站点、百科等 )，而事件图谱的来源则主要是非结构化的新闻文本。 第三，在构建难度上，由于通用知识图谱存在结构化来源，知识相对来说是比较 容易进行抓取和构建的，构建主要的难度在于对不同来源的实体进行归一化。而 事件图谱则不太一样，绝大部分事件并不存在结构化来源，必须从新闻文本中抽 取，抽取难度更大。另外，事件图谱构建还有较高的时效性要求。 \n\n\n\n> 当前百度的事件图谱构建方案如下：\n\n​\t首先，进行事件检测。 其次，在事件检测的基础上归纳出事件表示体系。 最后，在事件表示体系的基础上进行事件抽取和关系抽取。 这个过程和传统知识图谱的构建不一样的地方在于：传统知识图谱会先将构建的 schema 定义出来，然后进行抽取与归一，最后再进行入库。而事件图谱由于缺 少可参考的结构化来源，需要先进行事件检测，在检测出大量事件的基础上，才 能进行后续的相关操作 ( 构建表示体系、抽取等 )。这是两者非常不同的一个地 方。\n\n#### 3、事件检测\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1ocmd9dy7j20qe0fg0vf.jpg)\n\n> 事件检测的目标是快、准、全地进行客观事件的收录，其主要挑战有三点：\n\n第一，网页资讯存在大量的非事件内容。 \n\n第二，资讯的事件粒度是比较复杂的。父事件与子事件往往同时在同一篇网页资讯中。我们用传统的方法，如聚类或资源突增的方法进行检测，很容易形成一个很大的簇，同时容易将小的簇或较少提及的事件漏掉。 \n\n第三，几乎所有事件的应用场景对时效性的要求都比较高，大部分场景的事件检测延迟需要达到分钟级。 \n\n\n\n我们的整体方案是一套基于多任务学习进行事件检测的流程。该流程主要分成三个部分： \n\n第一部分，我们会对信息流中的资讯进行事件识别，将非事件的内容排除在外。 \n\n第二部分，我们会对出现在资讯中的事件片段以事件名的形式抽取出来。 \n\n第三部分，我们会对事件进行归一。然后再将对应的资讯作为这个事件的资源关联起来。 \n\n通过这个检测过程，只要一篇文档中有一部分提到了某个事件就可以进行事件收录，结合百度的高时效新闻流，上面这种流程可以实现分钟级事件收录，并且整体的准确率与覆盖率都可以到 90+%。此外，由于上述流程中涉及多个策略模型，我们还尝试了多任务学习框架，上述所提及的多个策略模型在经过多任务学习后能得到相互增强，在资源消耗降低的同时，每个模型的效果都能得到进一步的提升。\n\n#### 4、知识表示\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1ocs618h5j20qe0f3gou.jpg)\n\n事件表示构建的目标是构建一个高准确、高覆盖的事件知识表示体系，以支持后面事件抽取任务。在这一个任务上，它跟实体的 Schema 挖掘也是有明显差异的。\n\n​\t实体的 Schema 挖掘绝大部分来源于百科或是网页中的结构化字段，只有少部分可能会涉及 Open-P 挖掘。以电影为例，这类实体很容易可以找到其对应实体的网页，如豆瓣。从网页上我们就可以清晰地知道一个电影类型的实体会有哪些字段，例如，导演、编剧、主演等。事件的表示就跟实体的 Schema 完全不同。绝大部分的事件类型其实并不存在一个这样的垂类站点去做这件事情。\n\n​\t因此，我们必须从正文中去挖掘事件类型的属性。比如，“破产”这个事件类型。 从类型名可以知道该事件可能会有一个“破产人”属性，但是“债权人”这个属性就必须从该类型相关的正文中才能找到。目前的解决方案是开放挖掘+人工校验。具体来说分成下面几个部分。首先是开放类型挖掘，基于事件检测得到的事件库、日志等数据，我们挖掘出事件对应的类型并进行层次构建。在得到这样一个事件类型体系之后，我们会针对事件类型进行相关资讯召回，再进行每个类型的角色挖掘。最后再进行人工校验。通过这种方式，我们在几十个领域中得到几千个事件类型，以及一万多的事件角色。 \n\n#### 5、事件抽取\n\n##### 1、事件要素结构化抽取\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1ocygqhltj20q80ew76r.jpg)\n\n​\t第三步是事件抽取的任务。事件抽取的目标是对事件的相关要素进行结构化的抽取。还是以宋仲基、宋慧乔结婚为例，抽取的目标是，将这个事件抽取出来，并且能够识别它是一个结婚的事件类型，并且根据结婚类型的角色，包括时间、地点、男方、女方等，单独抽取出每个角色的具体的值。\n\n事件抽取的技术特点是：事件抽取是事件级理解的基础。只有通过事件抽取，才能够将事件图谱和知识图谱结合起来，提升事件理解能力；\n\n事件抽取的结果可支持推理计算。基于事件类型、 角色、 论元，结合知识图谱可进行语义推理。 \n\n事件抽取是事件智能化的核心。其可以支持推荐、问答、 推理、基于事件的内容生成等不同的应用。 \n\n事件抽取是当前学术界的热点之一。近年来有很多优秀的事件抽取论文在自然语言处理顶会、顶刊上发表。然而，事件抽取的技术难度依然是较大的，学术上效果还没有达到直接可用的水平，目前在公开权威数据集 ACE2005 上论元分类任务只有 60%左右的 F1。 \n\n##### 2、挑战与方案\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1od1fz2g2j20q80f00v7.jpg)\n\n​\t目前百度通过类型角色设计、前后处理、模型优化方面的努力，在特定场景下效果能达到业务需求。但除了效果，事件抽取还有其他方面的挑战。 \n\n事件抽取必须面对多种多样的文本形态。事件抽取有时候需要从短文本中抽取，如标题、事件句；有时候需要从篇章的角度进行抽取；有时候还需要跨篇章围绕某事件进行抽取。 \n\n事件抽取需要具备灵活地根据角色抽取的能力。不同行业的场景常常存在不同的 \n\n事件类型、不同的事件角色的抽取需求，甚至同一个行业中，不同的客户对事件类型或者事件角色定义也有可能是不同的。 \n\n我们整体的解决方案如右图。针对不同的关注点和文本形态，我们均挖掘出核心的事件句。然后，基于这些事件句，针对不同场景使用不同的抽取技术进行抽取。\n\n目前我们已具备的抽取技术包括通用属性抽取、语义角色抽取和自定义的论元抽取。\n\n\n\n##### 3、通用属性抽取\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1od4fwy3yj20qa0f3juh.jpg)\n\n​\t通用属性抽取的目的是抽取触发词、时间，地点、参与者等通用的事件角色。由于技术是比较通用的，我们采取的是一个多属性联合抽取的模型。这里的 ERNIE是百度提出的一个预训练语言模型，在这基础上再加上 Bi-LSTM+CRF，同时抽取多个属性。如图所示，该流程是先进行一次事件句的识别，找到关于某个事件的句子。然后，再对这个句子进行多属性联合抽取，并对抽取的结果进行归一。地点归一到国家、省市区县等，时间进行规范化、参与者实体化等等。 \n\n##### 4、自定义论元抽取\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1od771mvlj20qr0f8dii.jpg)\n\n自定义论元抽取要解决的问题就是如何根据用户定义的角色少样本甚至零样本地进行事件抽取。我们的方案是基于阅读理解问答的事件抽取模型。 \n\n我们把整个事件抽取的流程分成三轮问答： \n\n第一轮，问触发词，抽取出输入事件句的触发词是什么。 \n\n第二轮，问整个输入句子的事件类型。 \n\n第三轮，是根据第二步抽取的结果，对该事件类型对应的每个角色进行问答抽取。 \n\n这个方案中，角色、事件类型这些本来跟模型耦合的东西都解耦了，新的角色和事件类型只需要把它们放到问题中就可以抽取，从而实现少样本、甚至零样本的抽取。相关研究成果已被 EMNLP Findings，2020 收录。\n\n##### 5、语义角色抽取\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1odakj4yej20qe0ft770.jpg)\n\n​\t刚才讲到了两种事件抽取的方式，一种是通用属性抽取，一种是基于问答的论元抽取。通用属性抽取的问题是无法灵活的泛化。阅读理解方案的问题则是效率， \n\n因为我们需要对每一个角色都进行单独提问抽取。有没有一种方法可以既具灵活性又具备高效率？我们对这个问题提出了多种不同的方案，其中一种方案是：语义角色抽取。 \n\n从左边中间这幅图我们可以看到常见的事件类型跟它的角色是存在一定的关联关系的，但实际上也可以把它们解耦成一个类型和一系列的语义角色的。例如，“收购”类型和角色“收购价格”、“被收购方”和“收购方”是耦合的。如果把所有如“收购价格”、“被收购方”、“收购方”这样的类型角色转化为如“数值”、“受事主体”、“实施主体”这样的与类型解耦的语义角色，我们整体的角色数量就可以就可以从 n*类型数这一数量级，降低到只有十几个语义角色的量 \n\n级。\n\n在这个基础之上我们还可以通过多层指针结构对不同的语义角色同时进行抽取。 \n\n如右图所示，整个句子只需要一次进入预训练语言模型后，再进入多层指针网络就可以同时抽取所有的角色。这样既保持了整体的灵活性也保持了抽取的高效率。\n\n##### 6、关系挖掘\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1odcv09u3j20qv0fiwgf.jpg)\n\n事件之间存在多种关系。我们目前定义了 4 种关系，包括从属关系、共指关系、 \n\n时序关系和因果关系。由于时间的关系，我本次只分享因果关系。\n\n\n\n**因果关系**\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1oddwrwhej20qp0f276t.jpg)\n\n> 因果关系的挖掘存在诸多挑战： \n\n第一个挑战是因果的主观性。对哲学有了解的朋友可能会知道，哲学家大卫·休谟提过这么一个命题——“明天的太阳未必会升起”。意思是逻辑上，人类是无法根据过去太阳一直都这么升起的经验，来推理出未来太阳也这么升起。也就是说，因果关系不是绝对的、客观的，它只是是人类对我们过去的经验的一种主观总结。 \n\n第二个挑战是因果的复杂性。一个事情可能是一因多果，也可能是一果多因或者是多因多果。要完整地将事件的所有因果抽取出来难度较大。 \n\n> 下面是一些我们解决这些问题的具体思路：\n\n首先，因果关系在客观上是非常难判定的，不同人对因果有不同理解。因此，我们把因果关系当作观点来看待，直接绕过这个正确性的问题。通过因果的出现次数、来源的权威性等方面来评价抽取出的因果的质量。 \n\n第二，将因果关系作为序列标注问题，同时去标注多因多果，部分解决上面提到的复杂的、多因多果问题。 \n\n第三，对存在因果稀疏的问题。虽然在不同的语料中存在的因果关系是比较少的，但通过归一的方法我们可以将单独存在于多个不同地方的因果表达聚合到同一个网络图谱中，从而形成一个完整的因果图谱。 \n\n具体而言，我们的方案如右图所示。通过因果事件的挖掘，我们可以从不同的篇章当中挖掘出大量的因果关系。比如，“生猪存栏环比回升”导致了“猪肉价格明显回落”是一个因果。然后，再对这些抽取出来的因果 Pair 中的每个事件节点再进行归一，将不同篇章中描述的相同事件归一在一起。归一结束后，我们就可以得到一个能支持多步因果的因果图谱。我们目前已经形成一个节点正确率90+%，节点数、关系数达到百万量级的因果图谱。 \n\n##### 7、事理图谱\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1odgubizaj20qm0g5dia.jpg)\n\n​\t·此基础上我们就可以进行因果推理了。比如，想知道某个事情是由什么原因导致的？它可能会有什么后果？我们只需要在因果图谱中先定位到这个节点，然后就可以在图谱上面进行基于拓扑的推理。 \n\n问题来了，如果说我们需要进行推理的事件在因果语料中不存在，那我们应该怎么样进行推理和挖掘呢？目前我们正在尝试的 **3 个思路**如下： \n\n**基于事理的推理。**我们认为的事件是客观世界中发生的某个具体事件，抽象事件则是对一类事件的抽象，而事理则是这些抽象事件的因果规律。比如，“某一天美国加息”，就是美国在具体某一天的加息，而“美国加息”事件则是对这一类事件的抽象。由于“美国加息”这类事件常常会导致“美元的汇率上升”这样的抽象事件发生，那么，新的一次美国加息事件，也非常有可能导致汇率上升。这种抽象后的事件规律，我们会把它放在事理图谱层次，来支持泛化的推理。\n\n**基于规则的推理。**举个例子，“生猪库存不足”会导致“猪肉价格上涨”。在这里面，两个主体“生猪”和“猪肉”，其实是存在产业链关系的。“生猪”是上游，“猪肉”是下游。通过观察一系列的类似的因果，我们可以挖掘出这样一个规则：上游的库存不足，可能会导致下游的价格上涨。那么面对一种新事件的时候，比如说钢铁库存不足，我们就可以推理预测下游一些钢铁产品的价格可能会上涨。 \n\n**基于图神经网络的因果的预测。**上面两种方法中，本质上都可以认为是一种简单的规则方法，当事件较为复杂，或者需要多步推理时，上面的方法就不能适用。例如，“网曝黄光裕即将出狱”可能导致了“国美”股票大涨。这中间存在了多步推理关系，首先是“黄光裕”出狱的这个事件的“出狱者”是“黄光裕”，而“黄光裕”是“国美”的一个重要高管，然后 “国美”对应的股票出现“大涨”事件。这是一个多步推理的过程，比较难以通过事件抽象或是规则进行建模。我们正在尝试将每一个事件节点都放到整个图当中去进行考虑。通过图的方式对这里面的事件节点进行边的预测。 \n\n### 事件图谱应用\n\n上面我已经讲了事件图谱的检测、表示、抽取以及因果关系，下面我将介绍事件 \n\n图谱在百度以及行业中的具体应用：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1odsph8dfj20qt0frta4.jpg)\n\n目前百度事件图谱的相关应用主要有这四块。第一块是热点发现，第二块是事件脉络，第三块是资源关联，第四块是因果推理。 \n\n##### 1、多维度热点事件发现\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1odu1oamtj20qv0ft764.jpg)\n\n在热点发现这一块，我们能够做到分钟级的热点发现。对于行业热点、地域热点等中长尾事件也可以做到分钟级的发现，因此我们可以支持很多对中长尾、小事件敏感的应用场景。目前我们的事件检测已能够支持二十多个行业、三百多个省市的分钟级的热点发现，并在人民网，齐鲁网等数十家媒介机构中落地。\n\n##### 2、时间脉络\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1odvqqrn2j20qw0fcmz6.jpg)\n\n其次是事件脉络。通过事件图谱的整个事件的关联关系，我们在搜索上上线了热点事件脉络，可以将热点事件的前因后果进行关联，并以脉络的形式呈现给用户，大大提升了用户了解热点前因后果的效率。在疫情中，我们也通过事件脉络支持了疫情相关的事件脉络的查询。目前脉络相关的产品也支持了百度智能创作平台、事件脉络视频等众多应用。 \n\n##### 3、资源关联\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1odwzpruxj20qy0fmwge.jpg)\n\n第三块是资源关联。事件图谱围绕事件将相关的资源，如资讯、图片、视频等进行关联。目前我们通过这样的技术在“百家号的创作大脑”上面进行了应用，通过事件中心资源组织，辅助作者的内容生产分发效率大幅提升。\n\n##### 4、因果推理\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1ody4cfevj20qy0f5wg8.jpg)\n\n最后一块是因果推理。目前，我们在金融领域做了一些因果推理的尝试。我们将 5 年历史数据的相关因果进行挖掘和提取，构成了一个因果图谱。该图谱目前的规模接近两百万节点与关系。我们能够进行在线、实时的溯因推理。支持推理的 证据呈现，也支持多步推理。同时还可以融合企业与供应链图谱，进行上下游事件推理。以 “非洲猪瘟爆发”为例，该事件爆发会导致 “生猪存栏处于低位”，同时也会导致下游“玉米需求”下降，最终影响到与“玉米”行业相关的公司。目前在这个方向上，我们还处于探索阶段，非常欢迎金融圈感兴趣的朋友们跟我们一起合作&探讨。 \n\n##### 5、事件抽取数据集及评测任务\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1oe0r2i1pj20qn0f9q5i.jpg)\n\n在推动学界、业界事件相关技术发展方面，我们在 2020 语言与智能技术竞赛中发布了目前最大的中文事件抽取数据集——DuEE，并吸引了一千多支团队报名。 \n\n该数据集目前已经对外开源，在千言项目当中可以进行公开下载，同时我们也提 \n\n供了一个公开的测评榜单，欢迎大家打榜参与，共同推动技术的发展。\n\n### 总结与展望\n\n##### 1、总结\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1oe25xp6nj20rp0fljsf.jpg)\n\n​\t在本次分享中，我首先对事件图谱的价值进行了分享，包括认知方面的价值以及应用方面的价值。然后介绍了事件图谱的构建技术，包括检测、表示、抽取以及因果关系。同时也介绍了我们在事理图谱上的一些想法。最后分享了事件图谱的相关应用示例，包括热点发现、事件脉络、资源聚合以及因果推理等。\n\n##### 2、展望\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h1oe2o2zb8j20r10g2wfd.jpg)\n\n在未来，百度事件图谱的发展方向主要包括三个方面： \n\n在技术方面，我们会进一步完善事件的构建能力，包括抽取、事件关系与事理。 \n\n在应用方面，我们还会进一步强化应用方面的能力，包括基于事件的理解、资源组织和辅助决策等能力。 \n\n在行业化方面，我们会面向行业进行进一步的深入探索，努力通过事件图谱的技术赋能行业。","tags":["百度","事件图谱"],"categories":["AI"]},{"title":"基于询问笔录的智慧化辅助办案系统","url":"/2022/05/17/智慧笔录技术路线文档/","content":"\n\n\n# 基于询问笔录的智慧化辅助办案系统\n\n## 技术方案\n\n本技术方案是一个在传统的智慧询问笔录模板应用系统基础上，采用语音对话自动文本分析的深度学习框架，通过对相关法律法规关系和海量案件分析抽取，构建辅助办案系统的神经网络和案件知识图谱库。在实战应用中，通过对笔录对话文本语音/语义信息的读取和分析对比，相关法律条款的定性和定量分析，历史案件和相关案件的参考比对，实现实时的辅助办案智慧化引导提示。\n\n### 一、系统的业务流程\n\n### （一）业务流程图\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8o9h94c8mj324t0u046b.jpg) \n\n### 二、系统的技术方案\n\n#### （一）技术流程图\n\n![笔录流程图第三版](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8w8r6wey1j30u018642n.jpg) \n\n#### （二）技术模组结构及功能\n\n本系统由四大模组构成。\n\n##### 1、文本分类模组\n\n文本分类模组文本分类算法模型及文本分类模型库构成。\n\n文本分类模组主要功能是分析判断所要新建的询问（讯问）笔录是否与历史笔录有关联，比如是再询问案件，系统提示有历史案卷可参考使用。\n\n##### 2、语音转文本模组及模板调用模组\n\n语音转文本模组主由加载语言ASR和连续重复词去重算法模型及语音模型库构成。\n\n语音转文本模组主要功能是实现语音直接转换文本，供办案人员选用。\n\n模板调用模组由模板调用算法模型和模板库组成，根据被询问人的回答判断，调用对应的适用模板。\n\n##### 3、信息抽取模组\n\n信息抽取模组由实体抽取模型、关系抽取模型、时间抽取模型和结构化数据库组成。\n\n信息抽取模组的功能是为会话记忆单元Memory-unit模组提供技术支撑。\n\n##### 4、会话记忆单元Memory-unit模组\n\n会话记忆单元Memory-unit模组由文本匹配模型、BERT联合抽取模型和会话要点（案件要件、要素、疑点）库、知识数据（文本结构）库组、应用数据（知识图谱、关系图谱、事件图谱）库组构成。\n\n会话记忆单元Memory-unit的功能是用于指导会话流、记录审案要素点、流程记录点、案件查询点等。\n\n文本匹配模型包括有条件的语义匹配召回模块和增加场景特征的领域自适应排序模块。\n\n文本匹配模型的功能是用于分析理解和处理被询问人（被讯问人）回答的内容文本以匹配法律依据库、事件图谱库进行组织调配语义文本，并根据上下文信息给询问人（讯问人）分析生成匹配的引导提示。\n\n##### 5、案件要素分类\n\n在笔录分析中将案件中包含的信息一般归纳为 “何时（时间）、何地（地点）、何事（事情）、何物（作案工具）、何情（作案情形）、何故（动机目的）、何人（嫌疑人）”等七个方面的内容，每个方面又可以更细化到具体的研究分类。\n\n以下简单描述要素的分类：\n\n###### 5.1、案件类别（何事）\n\n（1）在刑事警情中，会将一般侵财类案件划分为：盗窃、抢劫、抢夺、诈骗……等类别；\n\n（2）在案件类别中，为研究作案时侵犯对象的规律，又会将某一类案件细分为多种形态：如盗窃类会包含，入室盗窃、盗窃企事业单位、盗窃商业门店、扒窃、拎包盗窃、盗窃汽车、盗窃摩托车、盗窃电动车、盗窃车内财物、盗窃路财，以及其他类等；\n\n（3）在具体的一种盗窃形态中，为研究作案的手段特点和规律，则又会根据作案手段细化分类：如入室盗窃中会划分，翻窗、技术开锁、插片开锁、撬门和其他类。\n\n###### 5.2、时间（时段）\n\n（1）对于具体案件，会有报警时间、发生时间、发现时间、处警时间等多个时间概念。报警时间指事主打110报警的时间点；发生时间指该警情实际发生时间（发案时段，此时间往往不能准确反应，可能为一个时间段）；发现时间为事主发现该警情的时间（往往是发生时段的时间止点）；处警时间为民警处置或处理该警情的时间；\n\n（2）发案时段，根据对警情发生时间的分析，判断该警情发生在那一个时段，主要分为上午、下午、晚上、凌晨等4个要素\n\n（3）案件发生时间是否为工作日或周末；\n\n###### 5.3、地点\n\n（1）对地点主要区分不同的部位，然后针对不同部位再更具不同的场所性质进行细分。如部位主要有：车站、村湾、道路、公共场所、交通工具、居民住宅、企业单位、商业场所、学校、医院等；\n\n（2）对某一部位，如居民住宅，又会细分为：城中村私房、小区等\n\n###### 5.4、作案工具：\n\n常见的如撬杠、螺丝刀（起子）、弹弓、液压钳、开锁工具、切割机、徒手等；\n\n###### 5.5、嫌疑人特征提取\n\n对有嫌疑人体貌特征描述的内容，需要从人员数量、性别、身高、衣着、发型、脸型、肤色、口音、体表特殊标记、配饰、行为特征、是否使用交通工具等方面进行要素的提取。\n\n###### 5.6、天气环境\n\n对案件发生时的自然天气环境进行记录。\n\n##### 6.分类要素提取方案\n\n###### 6.1、使用通用信息抽取模型\n\nUIE(Universal Information Extraction)：Yaojie Lu等人在ACL-2022中提出了通用信息抽取统一框架UIE。该框架实现了实体抽取、关系抽取、事件抽取等任务的统一建模，使得不同任务间具备良好的迁移和泛化能力。该模型可以支持不限定行业领域和抽取目标的关键信息抽取，实现零样本快速冷启动，并具备优秀的小样本微调能力，快速适配特定的抽取目标，提取文本中的警情要素。\n\n（1）数据量：需要资源方提供10万—100万条文档，标注其中的10%。取90%作为训练集，10%作为测试集。\n\n数据示例：文档形式样式（以下示例已脱敏处理）：\n\n********\n\n20xx年1月1日5时许，报警人吴桃枝报警称，其于2015年1月1日4时许发现位于杨园四美塘122号的嘉福零健康房门面被盗（经查，被盗现金9000元）。\n\n********\n\n事主，陈x，男，1971年1月20日生，身份证号：xx0****9710120065x 被盗现金200元，手机一部\n\n********\n\n20xx年1月1日6时许，报警人张xx称其租住的xx省xx市xx区xx台58号2楼家中被盗，损失现金6000余元人民币，及xxx手机一部，已受理。\n\n（2）标注流程：\n\n使用数据标注平台doccano 进行数据标注，doccano导出数据后，可以实现模型训练的无缝衔接。比如对于原文：\n\n20xx年1月1日6时许，报警人张xx称其租住的xx省xx市xx区xx台58号2楼家中被盗，损失现金6000余元人民币，及xxx手机一部，已受理。\n\n标注如下：\n\n![](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8o9hf4987j31x20u0wgj.jpg) \n\n（3）模型训练流程：\n\n信息抽取模型选用了T5的模型结构并进行了参数初始化，为了让模型能够获得通用的信息抽取能力，需要做预训练，预训练目标包括以下3个部分。 \n\n一、Text-to-Structure 对于每一个pair(x,y)，其中x是原文本，y是要抽取的信息结构y（人名-触发关系-物品）。通过抽取y中的spot跟关联类型构造相应的正确的schema，同时自动构建错误的schema，让模型的编码器跟解码器根据原文本跟schema去生成相应的要抽取的信息结构y（人名-触发关系-物品）。通过预训练让模型学到基本的text-to-structure映射能力，可以根据schema跟原文本去生成对应的需要抽取的信息结构。\n\n二、这部分是为了让整个模型学习到语言模型基本的能力，所以使用了Mask掩码语言模型常用的训练目标。总的来说就是用打乱的原文本去预测打乱的目标问题。如20xx年1月1日6时许，报警人张xx称其租住的xx省xx市xx区xx台58号2楼家中被【Mask掩码】，损失现金6000余元人民币，其中被掩码的字符为“盗”，模型可以根据掩码token的上下文去预测该字为“盗”的语义，进而让模型学习到理解语义的能力。\n\n三、在完成信息抽取模型的预训练后，为了适配不同的信息抽取任务，需要对模型做进一步的微调。需要给定少量的场景标注语料{(s, x, y)}，其中s是任务对应的schema，x是原文本，y是要抽取的信息结构，通过交叉熵损失优化模型，在模型预训练学习到海量的语义表示后，针对该少量的场景语料进行参数的微调以达到融合该语义场景的目的。这样在无监督语料（不必进行标注的语料）大规模的训练完毕后，只需要少样本的场景标注语料就可以学习到很好的语义特征表示，从而适配更多的少样本语料的任务。\n\n###### 6.2、联合实体关系抽取\n\n关系性事实通常表示为一个三元组，由两个实体（主体和客体）和它们之间的语义关系组成。早期的工作集中在关系的分类判断上，也就是假设实体对已经预先确定，然后再根据两个实体之间去判断关系，这限制了他们的应用准确率和召回率，因为忽略了实体的提取，忽略了实体提取和关系分类的相关性。\n\n如\n\nA：【***\\*张x\\*******\\*x】\\****称其居住的{xx省xx市xx区xx台58号2楼}家中被盗。\n\nB：【***\\*李xx\\****】称在{xx省xx市xx区xx号x楼}家里实施盗窃。\n\n \n\n【】中标记实体类型的可能是报警人或者是嫌疑人，他与一个地点的关系可以是居住或者入室（盗窃）关系。一旦关系被确认，实体类型就很容易被识别，反之亦然。例如，知道关系是居住，那么实体类型应该是报案人，实体提取和关系分类可以相互受益，如果分开考虑则会影响语义判断准确度，为了克服上述缺点，本项目使用联合抽取模型，在BERT的预训练中引入一个语义增强的任务来进一步优化BERT。并引入了大规模的裁判文书语料库进行实体识别预训练。使用了嵌入字码，以有效地在实体识别和关系抽取之间传递信息。\n\n例如，关于盗窃案件，我们定义出一个警情关系:\n\n{'报警人姓名','性别','年龄','身份证号码',\n\n'被盗时间','地点',\n\n'被盗物品','是否有监控'\n\n'嫌疑人人数','体貌特征'}\n\n输入文档： \"text\": \"20xx年1月1日6时许，报警人张xx称其租住的xx省xx市xx区xx台58号2楼家中被盗，损失现金6000余元人民币，及xxx手机一部，已受理。\"\n\n输出三元组： \"spo_list\": [{\"predicate\": \"报警人姓名\", \"object_type\": \"人物\", \"subject_type\": \"盗窃\", \"object\": \"张xx\", \"subject\": \"盗窃\"}, {\"predicate\": \"被盗物品\", \"object_type\": \"手机\", \"subject_type\": \"盗窃\", \"object\": \"xx手机yy型号\", \"subject\": \"盗窃\"}],...}\n\n先使用bert搭建关系的分类模型，是一个多标签分类任务，类别就是上述的那几种关系接着用预测出来的关系和文本，使用bert搭建一个实体抽取的模型，是一个分类模型，类别是：\n\n[\"[Padding]\",\"[category]\",\"[##WordPiece]\",\"[CLS]\",\"[SEP]\",\"B-SUB\",\"I-SUB\",\"B-OBJ\",\"I-OBJ\",\"O\"]\n\nSUB对应的就是subject，B-SUB就是第一个实体开始的位置，后续的是I-SUB，OBJ就是第二个实体，所以第二个模型就是预测每一个tokens的标示，最后根据标示可提取出实体对。\n\n第二个模型是一个多分类的单标签任务，一句话中有可能有多个三元组，为此在进行第二个模型的时候，是先依据第一个模型预测出来的关系类如当前句子预测出3个关系，那么就重复该句话分成3个样本，那么3个样本就对应的是3个多分类单标签任务，为了使实体对和关系对应，所以第二个模型在计算loss的时候是综合考虑了关系和tokens标示的预测的。过程中所有结果都会生成保存在out文件夹下。\n\n##### 7、关系图谱概述\n\n###### 7.1、案件法律关系图谱\n\n（1）盗窃\n\n![法律判断依据示意图](https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8w8rjpjiwj310q0u042k.jpg)            \n\n##### 8、技术流程步骤解析\n\n**步骤一**、在进入系统之前加载本地数据内容（类型模版预设数据）和预训练模型（深度学习模型，如语义模型、语音模型）后，标志位为启动状态，检查是否有案件文本信息（包括本案预设信息或者同案件其他信息如他人笔录信息，如报案人手机号、报案人定位、历史报案信息、报案人姓名、类型、案发地点、案发时间、作案情节等），判断结果为是的话加载案件分类模型（ernie+bert Fine-tune）判断模板类型，判断结果为否的话支持手动选择模版类型。至此步骤，模板类型由多类确定唯一类。\n\n**步骤二**、确定模版类型后，确定是否开启实时录音转换功能。\n\n确认开启后加载语音库、语音ASR、连续重复词去重等模型；\n\n确认不开启后，不加载语音模型，不启用语音转换功能。\n\n**步骤三**、开始进如办案流程，选择预设询问文本（常规询问）后，询问人按照预设询问文本对被询问人进行询问。\n\n开启实时录音转换功能，则被询问人的回答作为流式语音输入到系统中的语音模型，实时生成笔录文本供办案人选择使用。\n\n如果未选择开启实时录音转换功能，则办案人需手动录入被询问人回答文本，生成笔录文本。\n\n在协同办案中，对于同一个案件多个嫌疑人进行询问笔录时：\n\n支持多个系统查看共享不同被询问人的笔录文本\n\n²支持对笔录文本进行身份证信息、信息库匹配\n\n²支持对笔录文本的对话信息抽取\n\n**步骤四**、将被询问人回答文本输入到信息抽取、实体抽取、关系抽取、事件抽取模型中，输出结构化信息实体类型后，经过数据管道输入到会话记忆单元Memory-unit后存储，该会话记忆单元Memory-unit机制使本系统能够准确地提取和持续更新长期角色记忆，存储来自用户和被询问人的：历史角色信息 审案要素点 流程记录点 案件查询点等等，会话记忆单元Memory-unit负责对案件中的案件要素、以及事件脉络进行整理抽取。\n\n**步骤五**、该会话记忆单元还支持单独生成本案件图谱，暂存在内存数据库中，并与全国人口基本信息资源库、全国出入境人员资源库、全国机动车/驾驶人信息资源库、全国在逃人员信息资源库、全国违法犯罪人员信息资源库、全国被盗抢汽车信息资源库、全国安全重点单位信息资源库七大库进行比对检索，生成更新数据支持后续的下游任务操作流程，在整个笔录流程结束后，存储位置由暂存数据库转为历史案件图谱库中。\n\n**步骤六**、会话记忆单元Memory-unit在记录识别历史角色信息 审案要素点 流程记录点 案件查询点后触发系统设置好的Actions后输出相关信息到文本匹配模型STS中进行有条件的对比学习查询，文本匹配模型STS由1.3E裁判文书网数据以及警用对话数据、悬案疑案库等数据训练，支持对会话记忆单元输出的文本进行召回（相似度匹配）后，添加业务场景特征进行领域自适应（设定不同特征权重）如被盗物品的权重比盗窃场所的高等等，加入业务特征的领域自适应后对召回的文本进行重新的估值排序（领域特征匹配），支持设定阈值得到匹配文本。会话记忆单元支持：\n\n-  记录主客体描述及客观行为\n\n-  记录隐匿要件要素\n\n-  记录主客体描述矛盾\n\n-  记录实体信息\n\n-  记录证据信息\n\n-  记录犯罪情节\n\n-  记录犯罪证据等等\n\n步骤七、将匹配文本输入到由裁判文书网数据、警用对话数据等等联合抽取生成的事件案件图谱库中，经由图神经网络关系链路预测算法输出询问文本Templates，后对被询问人进行询问文本Templates的提问，至此步骤，一轮对话问讯结束。事件案件图谱库支持：\n\n-  新增本地处理案件\n\n- 实体发现\n\n  -  证据发现\n\n  -  证据发现\n\n  - 旧案发现\n\n- 实体信息解释\n\n  -  要件内容提问\n\n  - 证据信息提问 \n\n  - 旧案信息关联\n\n- 涉案人主观动作分析\n\n  - 身份关联分析\n\n  - 主观意向\n\n  - 前科分析\n\n-  隐匿要件要素识别\n\n- 主客体描述矛盾识别等等\n\n**步骤八**、被询问人接收到算法输出询问文本Templates后，重新输出流式语音，后经语音模型或者手动录入文本，再经输入信息抽取模型UIE中，抽取完毕后的结果增添至会话记忆单元Memory-unit中，累积信息后重新进行案件事件图谱匹配，对被询问人进行多轮对话的问询及问询结果的分析检索。\n\n**步骤九**、在本系统中，案件事件图谱库输出的询问文本停止向被询问人问询，多轮对话环节结束，支持进行后续下游警务子任务。\n\n \n\n## 三、应用场景解析\n\n本系统核心技术的特征是将语义分析功能和在现有笔录智慧模板系统有机的结合起来。在笔录过程中，实时地进行分类要素提取等自动处理。也可以在案件信息录入数据库后，由此服务进行关键要素的分析、拆分和提取，再另行存入案件分析数据库供分析使用。\n\n同时系统也可以提供拓展服务。基于本系统构建建设相应的刑事警情和案件信息语义分析系统，对已有的警情和案事件信息进行自动处理，并提供统计、分析等功能，以API方式将结果供其他刑侦业务系统使用。\n\n（1）在接受案件报警后，民警完成处警信息的采集和录入，语义分析功能则对处警信息进行核心语言含义的分析和关键要素的拆分、提取，并按确定的要素分类进行结构化的存储，为进一步对刑事警情进行深入的规律、趋势分析和统计分析、串并案件提供基础；\n\n（2）对案事件信息中简要案情和破案信息的语言描述进行核心语言含义的分析和关键要素的拆分、提取，并按确定的要素分类进行结构化的存储，为案件研究和串并案件提供基础；\n\n（3）对现勘信息、案件信息、询（讯）问笔录和刑事违法犯罪人员、物品采集信息进行语义分析，按规定要素比对所采集信息是否符合信息采集标准要求。\n\n（4）对于目前已有的警情案件等数据可以进行语料分析，作为对新出现的记录要素提取作为支撑。\n\n \n\n四、下一步的开发内容及需求\n\n***\\*（一）下一步的开发任务\\****\n\n下一步的开发任务的内容主要有以下几方面：\n\n1、确定项目一期建设的案件内容。\n\n2、根据确定的案件构建案件相关法律知识库。\n\n3、根据确定的案件构建案件相关判决文书库。\n\n4、根据确定的案件构建案件的法律关系图谱。\n\n5、根据确定的案件罪名，筛选出较为典型的历史案件笔录样本，每个2级类目不少于10个笔录样本。\n\n6、按照2级分类分别构建分类要素库。\n\n7、按照2级分类分别构建事件图谱库。\n\n8、按照2级分类分别构建关系图谱库。\n\n9、利用上述数据库组，训练实体抽取算法模型。\n\n10、利用上述数据库组，训练关系抽取算法模型。\n\n11、利用上述数据库组，训练事件抽取算法模型。\n\n12、根据确定的案件罪名，建立提示引导模板库。\n\n13、调试训练语音转换文本模型。\n\n***\\*（二）下一步的开发需求\\****\n\n1、需要崂山公安确定项目一期建设的案件罪种类名及2级分类明细。\n\n2、需要崂山公安提供与2级分类明细相关的历史案件笔录样本，每种2级类目数量不少于10个笔录样本。\n\n3、需要崂山公安提供与2级分类明细相关的判决文书，每种2级分类数量不少于10个判决文书样本。\n\n \n\n \n","tags":["信息抽取","UIE","应用"],"categories":["AI"]},{"title":"交管-智巡魔方","url":"/2022/05/16/青岛交管-智巡魔方/","content":"\n\n\n# 交管-智巡魔方\n\n---------------\n\n## 一、业务需求\n\n### 业务需求-现状与问题\n\n#### \t巡逻警力规模： 6辆警车（24h）\n\n#### \t\t\t\t\t\t\t\t12辆铁骑（8-19）\n\n#### \t\t\t\t\t\t\t\t8辆机关车（18:30 - 20:30）\n\n\n\n#### \t业务瓶颈问题：\n\n- ##### 问题一： 警力动态巡逻分布均匀性\n\n  ##### \t\t现有分区巡逻模式下，某些情况易出现警力聚集现象，导致较远警情得不到及时响应。\n\n- ##### 问题二： 警力静态停靠分布均匀性\n\n  ##### \t\t后半夜静态停靠的能力，如何根据警情估计进行合理空间分布的设计问题。\n\n- ##### 问题三： 出警补位问题\n\n  ##### \t\t出警时，如何合理利用现有警力补位巡逻路线空档，降低出警等待体验。\n\n- ##### 问题四：最低警力规模边界\n\n  ##### \t\t在既定平均响应时间条件下，如何计算出每个巡区所需的最小警力，作为工作依据。\n\n\n\n\n\n#### 业务需求-建设思路\t\t\n\n1. ##### 总体目标为：基于电子地图实现巡区或路线的**可视化、可查询、可** **编辑、可排班**，提高指挥中心处置效率，并支持任务信息可下发至交警手台，让交警便捷地明确自己的巡区和路线。\n\n2. ##### 系统可以根据当日出勤警力规模，在满足响应时间前提下，推荐合理的巡逻任务配置方案。\n\n3. ##### 系统支持将变更后的工作路线与任务下发至对应的交警手台，出勤警力明确自身巡防区域、重点位置\n\n\n\n### 业务需求-关键问题\n\n#### \t**单位巡区最优警力分配** - 思路：\n\n> 单位巡区的警力配置范围为 **{1...x...N},**即最小为1， 最多为N，希望在满足指定响应时间下，得到最小的警力配置 x： \n>\n> 存在的因变量为：巡区面积、巡区道路长度、警情规模与发生时间分布、历史平均响应时间与警力位置分布等。通过历史数据结合地图计算，可以提炼出警力匹配模型，作为警力推荐方案\n\n\n\n\n\n## 二、解决方案\n\n### 解决方案-需求分析\n\n​\t![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2abp6zy1jj21oh0u07an.jpg)\n\n### 解决方案-需求分析-关键性场景\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2abrgntk8j21pd0u00wz.jpg)\n\n### 解决方案-需求分析-关键性场景\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2abt7faa4j21oh0u0wk7.jpg)\n\n### 解决方案 - 总体方案\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2abuplyomj21pg0tq42s.jpg)\n\n### 解决方案 - 建设内容\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2abwwsdvvj21lj0u042c.jpg)\n\n### 解决方案-系统逻辑架构设计\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2abxqsyccj21jt0u0jvy.jpg)\n\n### 解决方案 - 地图可视化管理\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2ac9i3finj21l10u0n69.jpg)\n\n### 解决方案 - 片区规划\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2acawfhx9j21o60tg416.jpg)\n\n### 解决方案 - 警用终端\n\n<img src=\"https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2ace8y902j214s0u0jut.jpg\" alt=\"image-20220516164948863\" style=\"zoom: 50%;\" />\n\n### 解决方案 - 巡区编辑\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2acilanh4j21jr0u0q99.jpg)\n\n### 解决方案 - 预案编辑\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2acolnenfj21jq0u0tfl.jpg)\n\n\n\n\n\n### 三、预期成效\n\n#### ![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2ae8r7r5kj21eq0u0104.jpg)\n\n### 四、实施计划 - 建设内容\n\n\n\n![](https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/e6c9d24ely1h2ae9uxqjkj21oy0ts7ax.jpg)\n\n\n\n\n\n\n\n# 新增加：\n\n-  重点部位的临时添加、突发情况\n\n- 一两个常用的预案进行模型的学习，尽量让其合理化，（领导组织部门到位，管理车辆到位，警力装备到位，-》。怎么部署 个性化较多\n\n  正常情况下，巡逻的路线预案，普适性的，以及变化的过程如何处理的。\n\n  （重大交通事故，情况不一样处置不一样，只能落实原则性的问题，具体操作性的东西难以界定。）\n\n  \n\n常态、战时两种模式的方案，片区，方案。\n\n补充预案\n\n\n\n### 时间节点：\n\n​\t\n\n20号后对接，0\n","tags":["路径规划","交警"],"categories":["AI"]},{"title":"Neo4j图形化数据库","url":"/2022/03/26/Neo4j图形化数据库/","content":"\n## Neo4j图形化数据库\n\n\n\n# Neo4j - 需要图形数据库\n\n> 图数据库用于存储更多的连接数据。 如果我们使用 RDBMS 数据库来存储更多连接的数据，那么它们不能提供用于遍历大量数据的适当性能。 在这些情况下，Graph Database 提高了应用程序性能。\n\n> 什么是连接数据？ 以及这些应用程序如何与某些实时应用程序存储数据。\n\n## 方案1：Google+\n\n使用 Google+（GooglePlus）应用程序来了解现实世界中 Graph 数据库的需求。 观察下面的图表。 在这里，我们用圆圈表示了 Google+应用个人资料。 ![img](https://atts.w3cschool.cn/attachments/day_161224/201612241627042269.jpg) 在上图中，轮廓“A”具有圆圈以连接到其他轮廓：家庭圈（B，C，D）和朋友圈（B，C）。\n\n再次，如果我们打开配置文件“B”，我们可以观察以下连接的数据。 ![img](https://atts.w3cschool.cn/attachments/day_161224/201612241630423807.jpg) 像这样，这些应用程序包含大量的结构化，半结构化和非结构化的连接数据。 在 RDBMS 数据库中表示这种非结构化连接数据并不容易。\n\n如果我们在 RDBMS 数据库中存储这种更多连接的数据，那么检索或遍历是非常困难和缓慢的。\n\n所以要表示或存储这种更连接的数据，我们应该选择一个流行的图数据库。\n\n图形DBMS非常容易地存储这种更多连接的数据。 它将每个配置文件数据作为节点存储在内部，它与相邻节点连接的节点，它们通过关系相互连接。\n\n他们存储这种连接的数据与上面的图表中的相同，这样检索或遍历是非常容易和更快的。\n\n## 方案2：Facebook\n\n利用 Facebook 应用程序了解现实世界中 Graph 数据库的需求。 ![img](https://atts.w3cschool.cn/attachments/image/20210109/1610172532775982.png) 在上面的图中，Facebook Profile“A”已经连接到他的朋友，喜欢他的一些朋友，发送消息给他的一些朋友，跟随他喜欢的一些名人。\n\n这意味着大量的连接数据配置文件A.如果我们打开其他配置文件，如配置文件B，我们将看到类似的大量的连接数据。\n\n注-通过观察上述两个应用程序，它们有很多更多的连接数据。 它是非常容易存储和检索，这种更连接的数据与图形数据库。\n\n[========]\n\n# Neo4j - CQL简介\n\nCQL代表Cypher查询语言。 像Oracle数据库具有查询语言SQL，Neo4j具有CQL作为查询语言。\n\n## Neo4j CQL -\n\n- 它是Neo4j图形数据库的查询语言。\n- 它是一种声明性模式匹配语言\n- 它遵循SQL语法。\n- 它的语法是非常简单且人性化、可读的格式。\n\n## 如Oracle SQL -\n\n- Neo4j CQL 以命令来执行数据库操作。\n- Neo4j CQL 支持多个子句像在哪里，顺序等，以非常简单的方式编写非常复杂的查询。\n- NNeo4j CQL 支持一些功能，如字符串，Aggregation.In 加入他们，它还支持一些关系功能。\n\n## Neo4j CQL命令/条款\n\n常用的Neo4j CQL命令/条款如下：\n\n| S.NO | CQL命令/条        | 用法                         |\n| ---- | ----------------- | ---------------------------- |\n| 1    | CREATE创建        | 创建节点，关系和属性         |\n| 2    | MATCH匹配         | 检索有关节点，关系和属性数据 |\n| 3    | RETURN返回        | 返回查询结果                 |\n| 4    | WHERE哪里         | 提供条件过滤检索数据         |\n| 5    | DELETE删除        | 删除节点和关系               |\n| 6    | REMOVE移除        | 删除节点和关系的属性         |\n| 7    | ORDER BY以...排序 | 排序检索数据                 |\n| 8    | SET组             | 添加或更新标签               |\n\n## Neo4j CQL 函数\n\n以下是常用的Neo4j CQL函数：\n\n| S.NO | 定制列表功能     | 用法                                             |\n| ---- | ---------------- | ------------------------------------------------ |\n| 1.   | String字符串     | 它们用于使用String字面量。                       |\n| 2.   | Aggregation聚合  | 它们用于对CQL查询结果执行一些聚合操作。          |\n| 3.   | Relationship关系 | 他们用于获取关系的细节，如startnode，endnode等。 |\n\n后面的章节中详细讨论所有Neo4j CQL命令，子句和函数语法，用法和示例。\n\n## Neo4j CQL数据类型\n\n这些数据类型与Java语言类似。 它们用于定义节点或关系的属性 Neo4j CQL支持以下数据类型：\n\n| S.NO | 定制列表功能 | 用法                            |\n| ---- | ------------ | ------------------------------- |\n| 1.   | boolean      | 用于表示布尔文字：true，false。 |\n| 2.   | byte         | 用于表示8位整数。               |\n| 3.   | short        | 用于表示16位整数。              |\n| 4.   | int          | 用于表示32位整数。              |\n| 5.   | long         | 用于表示64位整数。              |\n| 6.   | float        | 用于表示32位浮点数。            |\n| 7.   | double       | 用于表示64位浮点数。            |\n| 8.   | char         | 用于表示16位字符。              |\n| 9.   | String       | 用于表示字符串。                |\n\n## Neo4j CQL - CREATE命令\n\nNeo4j使用CQL- create命令\n\n- 创建没有属性的节点\n- 使用属性创建节点\n- 在没有属性的节点质检创建关系\n- 使用属性创建节点之间的关系\n- 为节点和关系创建单个和多个标签\n\n接下来将创建一个没有属性的节点：\n\n### Neo4j CQL创建一个没有属性的节点\n\ncreate命令用于创建没有属性的节点，只是创建一个没有任何数据的节点。\n\n#### CREATE命令语法\n\n```\nCREATE ( <node-name>:<label-name>)\n```\n\n语法说明\n\n| 语法元素 | 描述                   |\n| -------- | ---------------------- |\n| CREATE   | 是一个Neo4j CQL命令    |\n|          | 是我们要创建的节点名称 |\n|          | 是一个节点标签名称     |\n\n注意事项 -\n\n1、Neo4j数据库服务器使用此将此节点详细信息存储在Database.As中作为Neo4j DBA或Developer，我们不能使用它来访问节点详细信息。\n\n2、Neo4j数据库服务器创建一个作为内部节点名称的别名。作为Neo4j DBA或Developer，我们应该使用此标签名称来访问节点详细信息。\n\n### Neo4j CQL创建具有属性的节点\n\nNeo4j CQL“CREATE”命令用于创建带有属性的节点。 它创建一个具有一些属性（键值对）的节点来存储数据。\n\n#### CREATE命令语法：\n\n```\nCREATE (\n\t<node-name>:<label-name>\n\t{\n\t\t<Property1-name>:<Property1-Value>\n\t\t........\n\t\t<Propertyn-name>:<Propertyn-name>\n\t}\n)\n```\n\n语法说明：\n\n| 语法元素 | 描述                                            |\n| -------- | ----------------------------------------------- |\n|          | 它是我们将要创建的节点名称。                    |\n|          | 它是一个节点标签名称                            |\n| ...      | 属性是键值对。 定义将分配给创建节点的属性的名称 |\n| ...      | 属性是键值对。 定义将分配给创建节点的属性的值   |\n\n##### 例如：\n\n此示例演示如何创建具有一些属性（deptno，dname，位置）的Dept节点。 按照下面给出的步骤 -\n\n**步骤1** - 打开Neo4j数据浏览器。\n\n**步骤2** - 在数据浏览器中的dollar提示符下键入以下命令。\n\n```\nCREATE (dept:Dept { deptno:10,dname:\"Accounting\",location:\"Hyderabad\" })\n```\n\n这里dept是一个节点名 Dept是dept节点的标签名称 这里的属性名称是deptno，dname，location\n\n属性值为10，\"Accounting\",\"Hyderabad\"\n\n正如我们讨论的，属性一个名称 - 值对。\n\nProperty = deptno:10\n\n因为deptno是一个整数属性，所以我们没有使用单引号或双引号定义其值10。\n\n由于dname和location是String类型属性，因此我们使用单引号或双引号定义其值。 **注意 - ** 要定义字符串类型属性值，我们需要使用单引号或双引号。\n\n#### Neo4j CQL - MATCH命令\n\nNeo4j CQL MATCH 命令用于\n\n- 从数据库获取有关节点和属性的数据\n- 从数据库获取有关节点，关系和属性的数据\n\n##### MATCH 命令语法：\n\n```\nMATCH\n(\n\t<node-name>:<label-name>\n)\n```\n\n语法说明:\n\n| 语法元素 | 描述                         |\n| -------- | ---------------------------- |\n|          | 这是我们要创建一个节点名称。 |\n|          | 这是一个节点的标签名称       |\n\n注意事项 -\n\nNeo4j 数据库服务器使用此 将此节点详细信息存储在 Database.As 中作为 Neo4j DBA 或 Developer，我们不能使用它来访问节点详细信息。\n\nNeo4j 数据库服务器创建一个 作为内部节点名称的别名。作为 Neo4j DBA 或 Developer，我们应该使用此标签名称来访问节点详细信息。 **注意-**我们不能单独使用 MATCH Command 从数据库检索数据。 如果我们单独使用它，那么我们将 InvalidSyntax 错误。\n\n##### 例如：\n\n```\nMATCH (dept： Dept)\n```\n\n这里 -\n\ndept 是节点名称 Dept 是 emp 节点的标签名称\n\n> 报错信息：Neo.ClientError.Statement.SyntaxError Query cannot conclude with MATCH (must be RETURN or an update clause) (line 1, column 1 (offset: 0)) \"match(dept:Dept)\" ^\n\n> 使用match （n) return n\n\n```\n# 查询Dept下的内容\nMATCH (dept:Dept) return dept\n\n# 查询Employee标签下 id=123，name=\"Lokesh\"的节点\nMATCH (p:Employee {id:123,name:\"Lokesh\"}) RETURN p\n\n## 查询Employee标签下name=\"Lokesh\"的节点，使用（where命令）\nMATCH (p:Employee)\nWHERE p.name = \"Lokesh\"\nRETURN p\n```\n\n#### Neo4j CQL - RETURN子句\n\nNeo4j CQL RETURN子句用于 -\n\n- 检索节点的某些属性\n- 检索节点的所有属性\n- 检索节点和关联关系的某些属性\n- 检索节点和关联关系的所有属性\n\n##### RETURN命令语法：\n\n```\nRETURN \n   <node-name>.<property1-name>,\n   ........\n   <node-name>.<propertyn-name>\n```\n\n语法说明:\n\n| 语法元素 | 描述                                            |\n| -------- | ----------------------------------------------- |\n|          | 它是我们将要创建的节点名称。                    |\n| ...      | 属性是键值对。 定义要分配给创建节点的属性的名称 |\n\n## **我们不能单独使用RETURN子句。我们应该既MATCH使用或CREATE命令。**\n\n#### MATCH & RETURN匹配和返回\n\n在Neo4j CQL中，我们不能单独使用MATCH或RETURN命令，因此我们应该合并这两个命令以从数据库检索数据。\n\nNeo4j使用CQL MATCH + RETURN命令 -\n\n- 检索节点的某些属性\n- 检索节点的所有属性\n- 检索节点和关联关系的某些属性\n- 检索节点和关联关系的所有属性\n\n##### MATCH RETURN命令语法：\n\n```\nMATCH Command\nRETURN Command\n```\n\n语法说明：\n\n| 语法元素   | 描述                       |\n| ---------- | -------------------------- |\n| MATCH命令  | 这是Neo4j CQL MATCH命令。  |\n| RETURN命令 | 这是Neo4j CQL RETURN命令。 |\n\n##### MATCH命令语法：\n\n```\nMATCH \n(\n   <node-name>:<label-name>\n)\n```\n\n语法说明：\n\n| 语法元素 | 描述                         |\n| -------- | ---------------------------- |\n|          | 它是我们将要创建的节点名称。 |\n|          | 它是一个节点标签名称         |\n\n要点 -\n\n- Neo4j数据库服务器使用此将此节点详细信息存储在Database.As中作为Neo4j DBA或Developer，我们不能使用它来访问节点详细信息。\n- Neo4j数据库服务器创建一个作为内部节点名称的别名。作为Neo4j DBA或Developer，我们应该使用此标签名称来访问节点详细信息。\n\n##### RETURN命令语法：\n\n```\nRETURN \n   <node-name>.<property1-name>,\n   ...\n   <node-name>.<propertyn-name>\n```\n\n语法说明：\n\n| 语法元素 | 描述                                            |\n| -------- | ----------------------------------------------- |\n|          | 它是我们将要创建的节点名称。                    |\n| ...      | 属性是键值对。 定义将分配给创建节点的属性的名称 |\n\n例如： 本示例演示如何从数据库检索Dept节点的一些属性（deptno，dname）数据。\n\n注-结点包含3个属性：deptno，dname，location。 然而在这个例子中，我们感兴趣的是只查看两个属性数据。 按照下面给出的步骤 -\n\n**步骤1** -打开Neo4j的数据浏览器。\n\n**步骤2** -在数据浏览器中的dollar提示符下键入以下命令。\n\n```\nMATCH (dept: Dept)\nRETURN dept.deptno,dept.dname\n```\n\n这里\n\n- dept是节点名称\n- Dept是一个节点标签名\n- deptno是dept节点的属性名称\n- dname是dept节点的属性名\n\n如果观察到数据浏览器消息，它将显示有关两个属性的Dept节点的数据：deptno，dname。 它返回Neo4j数据库中可用的两个节点（行）。\n\n------\n\n例如： 此示例演示如何从数据库检索Dept节点的数据，而无需指定其属性。 注-结点包含3个属性：deptno，dname，location。 按照下面给出的步骤 -\n\n步骤1 -打开Neo4j数据浏览器。\n\n步骤2 -在数据浏览器中的dollar提示符下键入以下命令。\n\n```\nMATCH (dept: Dept)\nRETURN dept\n```\n\n这里dept是一个节点名\n\n这里Dept是一个节点标签名\n\n------\n\n#### CREATE+MATCH+RETURN命令\n\n> 在Neo4j CQL中，我们不能单独使用MATCH或RETURN命令，因此我们应该结合这两个命令从数据库检索数据。\n\n演示如何使用属性和这两个节点之间的关系创建两个节点。\n\n注-我们将创建两个节点：客户节点 (Customer) 和信用卡节点 (CreditCard)。\n\n- 客户节点包含：ID，姓名，出生日期属性\n- CreditCard节点包含：id，number，cvv，expiredate属性\n- 客户与信用卡关系：DO_SHOPPING_WITH\n- CreditCard到客户关系：ASSOCIATED_WITH\n\n我们将在以下步骤中处理此示例： -\n\n- 创建客户节点\n- 创建CreditCard节点\n- 观察先前创建的两个节点：Customer和CreditCard\n- 创建客户和CreditCard节点之间的关系\n- 查看新创建的关系详细信息\n- 详细查看每个节点和关系属性 注-我们将在本章讨论前三个步骤。我们将在以后的章节中讨论其余的步骤\n\n##### 创建客户节点\n\n```\nCREATE(e: Customer{id:\"1001\",name:\"ABC\",dob:\"01/10/1982\"})\n```\n\n这里 -\n\n- e是节点名称\n- 在这里Customer是节点标签名称\n- id，name和dob是Customer节点的属性名称\n\n##### 创建CreditCard节点\n\n```\nCREATE (cc:CreditCard{id:\"5001\",number:\"1234567890\",cvv:\"888\",expiredate:\"20/17\"})\n```\n\n这里cc是一个节点名\n\n这里CreditCard是节点标签名称\n\nid，number，cvv和expiredate是CreditCard节点的属性名称\n\n##### 观察节点\n\n现在我们创建了两个节点：Customer和CreditCard\n\n我们需要使用带有RETURN子句的Neo4j CQL MATCH命令查看这两个节点的详细信息\n\n###### 查看客户节点详细信息\n\n**步骤1** -打开Neo4j数据浏览器\n\n**步骤2** -在数据浏览器中的美元提示符下键入以下命令。\n\n```\nMATCH (e:Customer)\nRETURN e.id,e.name,e.dob\n```\n\n这里e是节点名\n\n在这里Customer是节点标签名称\n\nid，name和dob是Customer节点的属性名称\n\n###### 查看CreditCard节点详细信息\n\n步骤1 -打开Neo4j数据浏览器\n\n步骤2 -在数据浏览器中的dollar提示符下键入以下命令。\n\n```\nMATCH (cc:CreditCard)\nRETURN cc.id,cc.number,cc.cvv,cc.expiredate\n```\n\n这里cc是一个节点名\n\n这里CreditCard是节点标签名称\n\nid，number，cvv，expiredate是CreditCard节点的属性名称\n\n------\n\n#### 关系基础\n\nNeo4j图数据库遵循属性图模型来存储和管理其数据。\n\n根据属性图模型，关系应该是定向的。 否则，Neo4j将抛出一个错误消息。\n\n基于方向性，Neo4j关系被分为两种主要类型。\n\n- 单向关系\n- 双向关系 在以下场景中，我们可以使用Neo4j CQL CREATE命令来创建两个节点之间的关系。 这些情况适用于Uni和双向关系。\n- 在两个现有节点之间创建无属性的关系\n- 在两个现有节点之间创建有属性的关系\n- 在两个新节点之间创建无属性的关系\n- 在两个新节点之间创建有属性的关系\n- 在具有WHERE子句的两个退出节点之间创建/不使用属性的关系 **注意 **-\n\n我们将创建客户和CreditCard之间的关系，如下所示：\n\n#### CREATE创建标签\n\n##### Neo4j CQL创建节点标签\n\n###### 单个标签到节点\n\n```\nCREATE (<node-name>:<label-name>)\n```\n\n| S.No. | 语法元素   | 描述                      |\n| ----- | ---------- | ------------------------- |\n| 1     | CREATE创建 | 它是一个Neo4j CQL关键字。 |\n| 2     | <节点名称> | 它是一个节点的名称。      |\n| 3     | <标签名称> | 这是一个节点的标签名称。  |\n\n> 例如： 本示例演示如何为“GooglePlusProfile”节点创建单个标签。\n\n```\nCREATE (google1:GooglePlusProfile)\n```\n\n这里google1是一个节点名\n\nGooglePlusProfile是google1node的标签名称\n\n###### 多个标签到节点\n\n```\nCREATE (<node-name>:<label-name1>:<label-name2>.....:<label-namen>)\n```\n\n> 例如： 本示例演示如何为“Cinema”节点创建多个标签名称。 我们的客户提供的多个标签名称：Cinema,Film,Movie,Picture。\n\n```\nCREATE (m:Movie:Cinema:Film:Picture)\n```\n\n这里m是一个节点名\n\nMovie, Cinema, Film, Picture是m节点的多个标签名称\n\n------\n\n###### 单个标签到关系\n\n```\nCREATE (<node1-name>:<label1-name>)-\n\t[(<relationship-name>:<relationship-label-name>)]\n\t->(<node2-name>:<label2-name>)\n```\n\n语法说明\n\n| S.No. | 语法元素       | 描述                      |\n| ----- | -------------- | ------------------------- |\n| 1     | CREATE创建     | 它是一个Neo4J CQL关键字。 |\n| 2     | <节点1名>      | 它是From节点的名称。      |\n| 3     | <节点2名>      | 它是To节点的名称。        |\n| 4     | <LABEL1名称>   | 它是From节点的标签名称。  |\n| 5     | <LABEL2名称>   | 它是To节点的标签名称。    |\n| 6     | <关系名称>     | 它是一个关系的名称。      |\n| 7     | <相关标签名称> | 它是一个关系的标签名称。  |\n\n注意 -\n\n- 我们应该使用colon（:)运算符来分隔节点名和标签名。\n- 我们应该使用colon（:)运算符来分隔关系名称和关系标签名称。\n- 我们应该使用colon（:)运算符将一个标签名称分隔到另一个标签名称。\n- Neo4J数据库服务器使用此名称将此节点详细信息存储在Database.As中作为Neo4J DBA或开发人员，我们不能使用它来访问节点详细信息。\n\nNeo4J Database Server创建一个标签名称作为内部节点名称的别名。作为Neo4J DBA或Developer，我们应该使用此标签名称来访问节点详细信息。\n\n------\n\n**例如：** 本示例演示如何为关系创建标签\n\n步骤1 -打开Neo4J数据浏览器\n\n步骤2 -在数据浏览器上键入以下命令\n\n```\nCREATE (p1:Profile1)-[r1:LIKES]->(p2:Profile2)\n```\n\n这里p1和profile1是节点名称和节点标签名称“From Node”\n\np2和Profile2是“To Node”的节点名称和节点标签名称\n\nr1是关系名称\n\nLIKES是一个关系标签名称\n\n#### WHERE子句\n\n像SQL一样，Neo4j CQL在CQL MATCH命令中提供了WHERE子句来过滤MATCH查询的结果。\n\n###### 简单WHERE子句语法\n\n```\nWHERE <condition>\n```\n\n###### 复杂WHERE子句语法\n\n```\nWHERE <condition> <boolean-operator> <condition>\n```\n\n我们可以使用布尔运算符在同一命令上放置多个条件。 请参考下一节，了解Neo4j CQL中可用的布尔运算符。\n\n###### 语法：\n\n```\n<property-name> <comparison-operator> <value>\n```\n\n语法说明：\n\n| S.No. | 语法元素     | 描述                                                         |\n| ----- | ------------ | ------------------------------------------------------------ |\n| 1     | WHERE        | 它是一个Neo4J CQL关键字。                                    |\n| 2     | <属性名称>   | 它是节点或关系的属性名称。                                   |\n| 3     | <比较运算符> | 它是Neo4j CQL比较运算符之一。请参考下一节查看Neo4j CQL中可用的比较运算符。 |\n| 4     | <值>         | 它是一个字面值，如数字文字，字符串文字等。                   |\n\n###### Neo4j CQL中的布尔运算符\n\nNeo4j支持以下布尔运算符在Neo4j CQL WHERE子句中使用以支持多个条件。\n\n| S.No. | 语法元素 | 描述                                   |\n| ----- | -------- | -------------------------------------- |\n| 1     | AND      | 它是一个支持AND操作的Neo4j CQL关键字。 |\n| 2     | OR       | 它是一个Neo4j CQL关键字来支持OR操作。  |\n| 3     | NOT      | 它是一个Neo4j CQL关键字支持NOT操作。   |\n| 4     | XOR      | 它是一个支持XOR操作的Neo4j CQL关键字。 |\n\n###### Neo4j CQL中的比较运算符\n\n| S.No. | 布尔运算符 | 描述                                  |\n| ----- | ---------- | ------------------------------------- |\n| 1     | =          | 它是Neo4j CQL“等于”运算符。           |\n| 2     | <>         | 它是一个Neo4j CQL“不等于”运算符。。   |\n| 3     | <          | 它是一个Neo4j CQL“小于”运算符。       |\n| 4     | >          | 它是一个Neo4j CQL“大于”运算符。       |\n| 5     | >          | 它是一个Neo4j CQL“小于或等于”运算符。 |\n| 6     | >          | 它是一个Neo4j CQL“大于或等于”运算符。 |\n\n**例如** 此示例演示如何在MATCH Command中使用CQL WHERE子句根据员工名称检索员工详细信息。\n\n```\nMATCH (emp:Employee)\nRETURN emp.empid,emp.name,emp.salary,emp.deptno\nMATCH (emp:Employee) \nWHERE emp.name = 'Abc'\nRETURN emp\n```\n\n**例如：** 此示例演示如何在MATCH Command中的CQL WHERE子句中使用多个条件与布尔运算符，以根据员工名称检索员工详细信息。\n\n```\nMATCH (emp:Employee)\nRETURN emp.empid,emp.name,emp.salary,emp.deptno\nMATCH (emp:Employee) \nWHERE emp.name = 'Abc' OR emp.name = 'Xyz'\nRETURN emp\n```\n\n###### 使用WHERE子句创建关系\n\n在Neo4J CQL中，我们可以以不同的方式创建拖曳节点之间的关系。\n\n创建两个现有节点之间的关系\n\n一次创建两个节点和它们之间的关系\n\n使用WHERE子句创建两个现有节点之间的关系\n\n我们已经讨论了前两章中的前两种方法。 现在我们将在本章中讨论“使用WHERE子句创建两个现有节点之间的关系”。\n\n```\nMATCH (<node1-label-name>:<node1-name>),(<node2-label-name>:<node2-name>) \nWHERE <condition>\nCREATE (<node1-label-name>)-[<relationship-label-name>:<relationship-name>\n       {<relationship-properties>}]->(<node2-label-name>) \n\n```\n\n语法说明：\n\n|S.No.\t|语法元素|\t描述|\n|-----|------|\n|1\t|MATCH,WHERE,CREATE\t|他们是Neo4J CQL关键字。|\n|2|\t<node1-label-name>|\t它是一个用于创建关系的节点一标签名称。|\n|3|\t<node1-name>\t|它是一个用于创建关系的节点名称。|\n|4|\t<node2-label-name>\t|它是一个用于创建关系的节点一标签名称。|\n|5|\t<node2-name>\t|它是一个用于创建关系的节点名称。|\n|6|\t<condition>\t|它是一个Neo4J CQL WHERE子句条件。 它可以是简单的或复杂的。|\n|7|\t<relationship-label-name>|\t这是新创建的节点一和节点二之间的关系的标签名称。|\n|8\t|<relationship-name>\t|这是新创建的节点1和节点2之间的关系的名称。|\n|9\t|<relationship-properties>\t|这是一个新创建节点一和节点二之间关系的属性列表（键 - 值对）。|\n\n**例如：**\n此示例演示如何使用WHERE子句创建两个现有节点之间的关系。\n\n\n\n- **步骤1** -打开Neo4J数据浏览器\n\n\n\n- **步骤2** -在数据浏览器上键入以下命令，以验证我们的Neo4J数据库中是否存在所需的客户节点。\n```\n\nMATCH (cust:Customer) RETURN cust.id,cust.name,cust.dob\n\n```\n**\n键入以下命令以创建客户和CreditCard节点之间的关系。**\n```\n\nMATCH (cust:Customer),(cc:CreditCard) WHERE cust.id = \"1001\" AND cc.id= \"5001\" CREATE (cust)-[r:DO_SHOPPING_WITH{shopdate:\"12/12/2014\",price:55000}]->(cc) RETURN r\n\n```\n输入以下查看两个节点之间的关系：\n```\n\nMATCH (cust:Customer),(cc:CreditCard) WHERE cust.id = \"1001\" AND cc.id= \"5001\" return cust,cc\n\n```\n-----------------\n\n#####  DELETE删除\n>Neo4j使用CQL DELETE子句\n删除节点。\n删除节点及相关节点和关系。\n我们将在本章中讨论如何删除一个节点。 我们将在下一章讨论如何删除节点和相关的节点和关系。\n\n###### 删除节点 -\n可以从数据库永久删除节点及其关联的属性。\n###### DELETE节点子句语法\n```\n\nDELETE\n\n```\n|S.No.| 语法元素| 描述|\n|-----|-------|-----|\n|1|DELETE|它是一个Neo4j CQL关键字。|\n|2|<node-name-list>|它是一个要从数据库中删除的节点名称列表。\n|\n\n注意 -\n\n我们应该使用逗号（，）运算符来分隔节点名。\n\n例如：\n此示例演示如何从数据库中永久删除节点。\n\n\n\n步骤1 - 打开Neo4j数据浏览器。\n\n\n\n步骤2 - 在数据浏览器上键入以下命令\n```\n\nMATCH (e: Employee) RETURN e\n\n```\n注意 -\n\n- MATCH (e: 'Employee') RETURN e\n\n- MATCH (e: \"Employee\") RETURN e\n\n- MATCH (e: Employee) RETURN e\n\n所有三个命令都相同，我们可以选择这些命令中的任何一个。\n\n###### 删除命令\n```\n\nMATCH (e: Employee) DELETE e\n\n```\n###### DELETE节点和关系子句语法\n```\n\nDELETE ,,\n\n```\n|S.No.| 语法元素| 描述|\n|-----|-------|-----|\n|1|DELETE|它是一个Neo4j CQL关键字。|\n|2|\t<node1-name>|它是用于创建关系<relationship-name>的一个结束节点名称。|\n|3|\t<node2-name>|它是用于创建关系<relationship-name>的另一个节点名称。|\n|4|<relationship-name>|它是一个关系名称，它在<node1-name>和<node2-name>之间创建。|\n\n注意 -\n\n我们应该使用逗号（，）运算符来分隔节点名称和关系名称。\n\n例如：\n此示例演示如何从数据库永久删除节点及其关联节点和关系。\n```\n\nMATCH (cc:CreditCard)-[r]-(c:Customer)RETURN r\n\n```\n\n```\n\nMATCH (cc: CreditCard)-[rel]-(c:Customer) DELETE cc,c,rel\n\n```\n现在检查DELETE操作是否成功完成。\n```\n\nMATCH (cc:CreditCard)-[r]-(c:Customer) RETURN r\n\n```\n这里我们可以看到从数据库返回的零行。\n\n--------\n\n#####  REMOVE删除\n有时基于我们的客户端要求，我们需要向现有节点或关系添加或删除属性。\n\n我们使用Neo4j CQL SET子句向现有节点或关系添加新属性。\n\n我们使用Neo4j CQL REMOVE子句来删除节点或关系的现有属性。\n\nNeo4j CQL REMOVE命令用于\n\n- **删除节点或关系的标签**\n- **删除节点或关系的属性**\n\nNeo4j CQL DELETE和REMOVE命令之间的主要区别 \n- DELETE操作用于删除节点和关联关系。\n- REMOVE操作用于删除标签和属性。\n\nNeo4j CQL DELETE和REMOVE命令之间的相似性 \n- **这两个命令不应单独使用。**\n- **两个命令都应该与MATCH命令一起使用。**\n\n###### 删除节点/关系的属性\n我们可以使用相同的语法从数据库中永久删除节点或关系的属性或属性列表。\n\n##### REMOVE属性子句语法\n```\n\nREMOVE\n\n```\n|S.No.|语法元素|描述|\n|---|----|---|\n|1.|\tREMOVE|它是一个Neo4j CQL关键字。|\n|2.|<property-name-list>|它是一个属性列表，用于永久性地从节点或关系中删除它。|\n\n###### <property-name-list> <属性名称列表>语法\n```\n\n., ., .... .\n\n```\n语法说明：\n\n|S.No.|语法元素|描述|\n|---|----|---|\n|1.|\t\t<node-name>|它是节点的名称。|\n|2.|<property-name>|它是节点的属性名称。|\n\n注意 \n\n- 我们应该使用逗号（，）运算符来分隔标签名称列表。\n- 我们应该使用dot（。）运算符来分隔节点名称和标签名称。\n\n例如：\n此示例演示如何创建节点并从数据库中永久删除此节点的属性。\n```\n\nCREATE (book:Book {id:122,title:\"Neo4j Tutorial\",pages:340,price:250})\n\n```\n它类似于以下两个SQL命令在一个镜头。\n```\n\nCREATE TABLE BOOK( id number, title varchar2(20), pages number, price number ); INSERT INTO BOOK VALUES (122,'Neo4j Tutorial',340,250);\n\n```\n这里我们可以观察到一个标签和一个节点有4个属性被成功创建。\n```\n\nMATCH (book { id:122 }) REMOVE book.price RETURN book\n\n```\n它类似于下面的SQL命令。\n```\n\nALTER TABLE BOOK REMOVE COLUMN PRICE; SELECT * FROM BOOK WHERE ID = 122;\n\n```\n在这里，我们只能看到节点书的3个属性，因为“价格”属性被删除。\n\n有时基于客户端要求，我们需要删除一些现有的属性到节点或关系。\n\n我们需要使用REMOVE子句来删除一个属性或一组属性。\n\n**例如**\n此示例演示如何从数据库中永久删除现有节点的属性。\n```\n\nMATCH (dc:DebitCard) RETURN dc\n\n```\n\n```\n\nMATCH (dc:DebitCard) REMOVE dc.cvv RETURN dc\n\n```\n###### 删除节点/关系的标签\n我们可以使用相同的语法从数据库中永久删除节点或关系的标签或标签列表。\n###### REMOVE一个Label子句语法：\nREMOVE <label-name-list> \n\n|S.No.|语法元素|描述|\n|---|----|---|\n|1.|\t\tREMOVE|它是一个Neo4j CQL关键字。|\n|2.|\t<label-name-list>|它是一个标签列表，用于永久性地从节点或关系中删除它。|\n\n#### SET子句\n有时，根据我们的客户端要求，我们需要向现有节点或关系添加新属性。\n\n要做到这一点，Neo4j CQL 提供了一个SET子句。\n\nNeo4j CQL 已提供 SET 子句来执行以下操作。\n- 向现有节点或关系添加新属性\n- 添加或更新属性值\n##### SET子句语法\n```\n\nSET\n\n```\n|S.No.|语法元素|描述|\n|---|----|---|\n|1.|\t\t\tSET|它是一个Neo4j CQL关键字。|\n|2.|\t<property-name-list>|它是一个属性列表，用于执行添加或更新操作以满足我们的要求。|\n\n#### <属性名称列表>语法：\n```\n\n., ., .... .\n\n```\n##### 语法说明：\n\n|S.No.|语法元素|描述|\n|---|----|---|\n|1.|\t\t\t<node-label-name><节点标签名称>|这是一个节点的标签名称。|\n|2.|\t<property-name><属性名称>|它是一个节点的属性名。|\n\n\n我们应该使用逗号（，）运算符来分隔属性名列表。\n\n示例：演示如何向现有 DebitCard 节点添加新属性。\n\n步骤1 -打开 Neo4j 数据浏览器\n\n步骤2 -在数据浏览器上键入以下命令\n```\n\nMATCH (book:Book) RETURN book\n\n```\n\n```\n\nMATCH (book:Book) SET book.title = 'superstar' RETURN book\n\n```\n#### ORDER BY排序\nNeo4j CQL ORDER BY子句\nNeo4j CQL在MATCH命令中提供了“ORDER BY”子句，对MATCH查询返回的结果进行排序。\n\n我们可以按升序或降序对行进行排序。\n\n默认情况下，它按升序对行进行排序。 如果我们要按降序对它们进行排序，我们需要使用DESC子句。\n```\n\nORDER BY [DESC]\n\n```\n|S.No.|语法元素|描述|\n|---|----|---|\n|1.|\t\t\tORDER BY|这是一个节点的标签名称。|\n|2.|\t<property-name-list>|它是用于排序的属性列表|\n|3.|DESC|它是一个Neo4j CQL关键字，用于指定降序。它是可选的。|\n\n<property-name-list>语法：\n```\n\n., ., .... .\n\n```\n语法说明：\n\n|S.No.|语法元素|描述|\n|---|----|---|\n|1.|\t\t\t<node-label-name>|它是节点的标签名称。|\n|2.|\t<property-name>|它是节点的属性名称。|\n\n注意 -\n\n我们应该使用逗号（，）运算符来分隔属性名列表。\n\n\n例如：\n此示例演示如何按照**升序排序**“员工名称”结果。\n```\n\nMATCH (emp:Employee) RETURN emp.empid,emp.name,emp.salary,emp.deptno\n\n```\n\n```\n\nMATCH (emp:Employee) RETURN emp.empid,emp.name,emp.salary,emp.deptno ORDER BY emp.name\n\n```\n此示例演示如何按照员工名称按**降序**使用排序结果。\n```\n\nMATCH (emp:Employee) RETURN emp.empid,emp.name,emp.salary,emp.deptno\n\n```\n\n```\n\nMATCH (emp:Employee) RETURN emp.empid,emp.name,emp.salary,emp.deptno ORDER BY emp.name DESC\n\n```\n#### UNION合并\n与SQL一样，Neo4j CQL有两个子句，将两个不同的结果合并成一组结果\n\n- UNION\n- UNION ALL\n\n##### UNION子句\n它将两组结果中的公共行组合并返回到一组结果中。 它不从两个节点返回重复的行。\n\n###### 限制：\n结果列类型和来自两组结果的名称必须匹配，这意味着列名称应该相同，列的数据类型应该相同。\n\n##### UNION子句语法\n```\n\nUNION \n\n\\```\n\n| S.No. | 语法元素 | 描述                                   |\n| ----- | -------- | -------------------------------------- |\n| 1.    |          | 它是CQL MATCH命令，由UNION子句使用。   |\n| 2.    |          | 它是CQL MATCH命令两个由UNION子句使用。 |\n| 3.    | UNION    | 它是UNION子句的Neo4j CQL关键字。       |\n\n注意 -\n\n如果这两个查询不返回相同的列名和数据类型，那么它抛出一个错误。\n\n在本章中，我们将采取一个银行应用程序的节点：信用卡式和借记卡解释UNION子句\n\n#### 信用卡式节点数据\n\n```\nMATCH (cc:CreditCard) RETURN cc\n```\n\n###### 借记卡数据的节点\n\n第1步 -打开Neo4j的数据浏览器\n\n第2步 -在数据浏览器的美元提示符处键入以下命令。\n\n```\nMATCH (dc:DebitCard) RETURN dc\n```\n\n我们将利用这些数据来解释的Neo4j CQL UNION与实例的使用\n\n本例说明：如果UNION子句的这两个查询确实有相同的名称或相同的数据类型及其列会发生什么。\n\n```\nMATCH (cc:CreditCard) RETURN cc.id,cc.number\nUNION\nMATCH (dc:DebitCard) RETURN dc.id,dc.number\n```\n\n这表明，这两个查询应具有相同的列名。\n\n首先查询有：cc.id，cc.number。\n\n第二个查询有：dc.id，dc.number。\n\n这里既有信用卡式和借记卡具有相同的属性名：身份证和号码，但他们有不同的节点名称前缀。这就是为什么UNION命令显示此错误消息。为了避免这种错误，Neo4j的CQL提供“AS”子句。\n\n像CQL，CQL Neo4j的“AS”子句用于给一些别名。\n\n**此示例演示如何使用UNION子句从两个节点检索数据。**\n\n```\nMATCH (cc:CreditCard)\nRETURN cc.id as id,cc.number as number,cc.name as name,\n   cc.valid_from as valid_from,cc.valid_to as valid_to\nUNION\nMATCH (dc:DebitCard)\nRETURN dc.id as id,dc.number as number,dc.name as name,\n   dc.valid_from as valid_from,dc.valid_to as valid_to\n```\n\n在这里，因为UNION子句过滤它们，我们可以看到该命令返回9行没有重复的行。\n\n------\n\n##### UNION ALL子句\n\n它结合并返回两个结果集的所有行成一个单一的结果集。它还返回由两个节点重复行。\n\n###### 限制\n\n###### UNION ALL子句语法\n\n```\n<MATCH Command1>\nUNION ALL\n<MATCH Command2>\n```\n\n> 注意 - 如果这两个查询不返回相同的列名和数据类型，那么它抛出一个错误。 在本章中，我们将采取一个银行应用程序的节点：信用卡式和借记卡解释UNION子句\n\n##### 信用卡式节点数据\n\n```\nMATCH (cc:CreditCard) RETURN cc\n```\n\n##### 借记卡数据的节点\n\n```\nMATCH (dc:DebitCard) RETURN dc\n```\n\n我们将利用这些数据来解释的Neo4j CQL UNION与实例的使用\n\n**例子** 本例说明：如果UNION子句的这两个查询确实有相同的名称或相同的数据类型及其列会发生什么。\n\n```\nMATCH (cc:CreditCard)\nRETURN cc.id as id,cc.number as number,cc.name as name,\n   cc.valid_from as valid_from,cc.valid_to as valid_to\nUNION ALL\nMATCH (dc:DebitCard)\nRETURN dc.id as id,dc.number as number,dc.name as name,\n   dc.valid_from as valid_from,dc.valid_to as valid_to\n```\n\n** 在这里，我们可以观察到这个命令返回10行，因为与UNION ALL子句不过滤它们重复行。如果我们使用UNION子句，它将返回只有9行。详情请参阅UNION子句章节进行检查。**\n\n**UNION 和 UNION ALL的区别： UNION联合，过滤重复。 UNION ALL 联合 ，不过滤重复**\n\n------\n\n##### LIMIT和SKIP子句\n\nNeo4j CQL已提供“LIMIT”子句来过滤或限制查询返回的行数。 它修剪CQL查询结果集底部的结果。\n\n如果我们要修整CQL查询结果集顶部的结果，那么我们应该使用CQL SKIP子句。 请参考本章的下一节CQL SKIP子句。\n\n```\nLIMIT <number>\n```\n\n| S.No. | 语法元素 | 描述                      |\n| ----- | -------- | ------------------------- |\n| 1。   | LIMIT    | 它是一个Neo4j CQL关键字。 |\n| 2。   |          | 它是一个跨值。            |\n\n本示例演示如何使用CQL LIMIT子句减少MATCH + RETURN查询返回的记录数。\n\n```\nMATCH (emp:Employee) \nRETURN emp\n```\n\n**做限制：**\n\n```\nMATCH (emp:Employee) \nRETURN emp\nLIMIT 2\n```\n\n它只返回Top的两个结果，因为我们定义了limit = 2。这意味着前两行。\n\n##### **SKIP子句**\n\nNeo4j CQL已提供“SKIP”子句来过滤或限制查询返回的行数。 它修整了CQL查询结果集顶部的结果。\n\n如果我们要从CQL查询结果集底部修整结果，那么我们应该使用CQL LIMIT子句。 请参阅本章的上一节CQL LIMIT子句。\n\n###### SKIP子句语法：\n\n```\nSKIP <number>\n```\n\n|S.No.| 语法元素| 描述| |-----|-----| |1。| SKIP| 它是一个Neo4j CQL关键字。| |2。| | 它是一个间隔值。|\n\n**例如：** 此示例演示如何使用CQL SKIP子句减少MATCH + RETURN查询返回的记录数。 在带有SKIP子句的数据浏览器上键入以下命令\n\n```\nMATCH (emp:Employee) \nRETURN emp\nSKIP 2\n```\n\nskip跳过两个节点，因此我们定义了skip = 2。这意味着**最后两行。**\n\n##### 合并\n\nNeo4j使用CQL MERGE命令 -\n\n- 创建节点，关系和属性\n- 为从数据库检索数据\n\nMERGE命令是CREATE命令和MATCH命令的组合。\n\n```\nMERGE = CREATE + MATCH\n```\n\nMERGE命令在图中搜索给定模式，如果存在，则返回结果\n\n如果它不存在于图中，则它创建新的节点/关系并返回结果。\n\n###### MERGE语法\n\n```\nMERGE (<node-name>:<label-name>\n{\n   <Property1-name>:<Pro<rty1-Value>\n   .....\n   <Propertyn-name>:<Propertyn-Value>\n})\n```\n\n|S.No.| 语法元素| 描述| |-----|-----| |1。| MERGE| 它是一个Neo4j CQL关键字。| |2。| | 它是一个间隔值。| |2。| | 它是节点或关系的标签名称。| |2。| <property_name>| 它是节点或关系的属性名称| |2。| <property_value>| 它是节点或关系的属性值。| |2。| ：| 使用colon（:)运算符来分隔节点或关系的属性名称和值。|\n\n###### CREATE示例\n\n例子 此示例通过使用CREATE，MATCH和RETURN命令创建Google+个人资料，执行上述所有操作。\n\n**操作（1）：创建具有属性：Id，Name的Profile节点**\n\n```\nCREATE (gp1:GoogleProfile1 {Id: 201401, Name:\"Apple\"})\n```\n\n**操作（2）：创建具有相同属性的同一个Profile节点：Id，Name。**\n\n```\nCREATE (gp1:GoogleProfile1 {Id: 201401, Name:\"Apple\"})\n```\n\n**操作（3）：检索所有Profile节点详细信息并观察结果。**\n\n```\nMATCH  (gp1:GoogleProfile1) \nRETURN gp1.Id,gp1.Name\n```\n\n如果我们观察到上面的查询结果，它显示2行重复的值。\n\nCQL CREATE命令检查此节点是否可用，它只是在数据库中创建新节点。 通过观察这些结果，我们可以说CREATE命令总是向数据库添加新的节点。\n\n##### MERGE示例\n\n此示例通过使用MERGE和RETURN命令创建Google+个人资料，执行相同的上述操作。\n\n###### 操作（1）：创建具有属性：Id，Name的Profile节点\n\n```\nMERGE (gp2:GoogleProfile2{ Id: 201402,Name:\"Nokia\"})\n```\n\n如果我们观察上面的查询结果，它只显示一行，因为CQL MERGE命令检查该节点在数据库中是否可用。 如果它不存在，它创建新节点。 否则，它不创建新的。\n\n通过观察这些结果，我们可以说，CQL MERGE命令将新的节点添加到数据库，只有当它不存在。\n\n##### NULL值\n\nNeo4j CQL将空值视为对节点或关系的属性的缺失值或未定义值。\n\n当我们创建一个具有现有节点标签名称但未指定其属性值的节点时，它将创建一个具有NULL属性值的新节点。\n\n让我们用一个例子来看这个。\n\n此示例演示CREATE命令如何将NULL值设置为未定义属性。 如何检索没有NULL行的节点的所有行。\n\n```\nMATCH (e:Employee) \nRETURN e.id,e.name,e.sal,e.deptno\n```\n\n创建任何属性到Employee节点。\n\n```\nCREATE (e:Employee)\nMATCH (e:Employee) \nRETURN e.id,e.name,e.sal,e.deptno)\n```\n\n观察这些结果，则以前的CREATE命令通过将其所有属性值设置为NULL来插入Employee节点。\n\n```\nMATCH (e:Employee) \nWHERE e.id IS NOT NULL\nRETURN e.id,e.name,e.sal,e.deptno\n```\n\n观察这些结果，它不返回NULL值行，因为我们提供了一个WHERE子句来过滤该行，即Id属性不应该包含NULL值。\n\n```\nMATCH (e:Employee) \nWHERE e.id IS NULL\nRETURN e.id,e.name,e.sal,e.deptno\n```\n\n观察这些结果，它只返回NULL值行，因为我们提供了一个WHERE子句来检查ID值为NULL。\n\n------\n\n##### IN操作符\n\n与SQL一样，Neo4j CQL提供了一个IN运算符，以便为CQL命令提供值的集合。\n\n###### IN操作符语法\n\n```\nIN[<Collection-of-values>]\n```\n\n语法说明：\n\n| S.No. | 语法元素 | 描述                                  |\n| ----- | -------- | ------------------------------------- |\n| 1。   | IN       | 它是一个Neo4j CQL关键字。             |\n| 2。   | [        | 它告诉Neo4j CQL，一个值的集合的开始。 |\n| 3。   | ]        | 它告诉Neo4j CQL，值集合的结束。       |\n| 4。   |          | 它是由逗号运算符分隔的值的集合。      |\n\n此示例演示如何使用IN运算符检索Employee节点详细信息。\n\n```\nMATCH (e:Employee) \nRETURN e.id,e.name,e.sal,e.deptno\nMATCH (e:Employee) \nWHERE e.id IN [123,124]\nRETURN e.id,e.name,e.sal,e.deptno\n```\n\n此查询仅返回在IN运算符中指定的id匹配的两行。\n\n------\n\n##### 图形字体\n\n我们使用Neo4j数据浏览器来执行和查看Neo4j CQL命令或查询的结果。\n\nNeo4j数据浏览器包含两种视图来显示查询结果 -\n\n- UI查看\n- 网格视图\n\n我们将讨论如何在UI视图中更改节点或关系的字体。\n\n当我们在数据浏览器中执行Neo4j CQL RETURN子句时，它会在网格视图或UI视图中显示结果。\n\n默认情况下，Neo4j数据浏览器以小字体显示节点或关系图，并在UI视图中显示默认颜色。 如果我们想要在更大的字体或不同的颜色中查看它们，那么如何增加他们的字体（大小）或如何改变他们的颜色。\n\n------\n\n##### ID属性\n\n在Neo4j中，“Id”是节点和关系的默认内部属性。 这意味着，当我们创建一个新的节点或关系时，Neo4j数据库服务器将为内部使用分配一个数字。 它会自动递增。\n\n**例如**： 此示例演示了Neo4j DB服务器如何为节点分配Id属性以及如何查看此属性值。\n\n```\nCREATE (tweet:Tweet{message:\"Hello\"})\nMATCH (tweet:Tweet{message:\"Hello\"})\nRETURN tweet\n```\n\n以相同的方式，Neo4j数据库服务器为关系分配一个默认Id属性。\n\n- 节点的Id属性的最大值约为35亿。\n- Id的最大值关系的属性的大约35亿。\n\n------\n\n##### Caption标题\n\n在Neo4j数据中，当我们在Neo4j DATA浏览器中执行MATCH + RETURN命令以查看UI视图中的数据时，通过使用它们的Id属性显示节点和/或关系结果。 它被称为“CAPTION”的id属性。\n\n我们可以通过使用它的其他属性值来更改节点或关系的CAPTION。、\n\n##### 方向关系\n\n在Neo4j中，两个节点之间的关系是有方向性的。 它们是单向或双向的。\n\n由于Neo4j遵循属性图数据模型，它应该只支持方向关系。 如果我们尝试创建一个没有任何方向的关系，那么Neo4j DB服务器应该抛出一个错误。\n\n在本章中，我们将提供一个例子来证明这一点。\n\n我们使用以下语法来创建两个节点之间的关系。\n\n```\nCREATE (<node1-details>)-[<relationship-details>]->(<node2-details>)\n```\n\n这里 -\n\n- 是“From Node”节点详细信息\n- 是“到节点”节点详细信息\n- relationship-details>是关系详细信息\n\n如果我们观察上面的语法，它使用一个箭头标记：（） - []→（）。 它表示从左侧节点到右侧节点的方向。\n\n如果我们尝试使用相同的语法，没有箭头标记like（） - [] - （），这意味着没有方向的关系。 然后Neo4j DB服务器应该抛出一个错误消息\n\n[========]\n\n### Neo4J CQL - 字符串函数\n\n与SQL一样，Neo4J CQL提供了一组String函数，用于在CQL查询中获取所需的结果。\n\n这里我们将讨论一些重要的和经常使用的功能。\n\n字符串函数列表\n\n|S.No.| 功能 |描述| |-----|---------| |1。| ~~UPPER~~ |它用于将所有字母更改为大写字母。| |2。| ~~LOWER~~ |它用于将所有字母改为小写字母。| |3。 |toupper |它用于将所有字母更改为大写字母。| |4。 |tolower |它用于将所有字母改为小写字母。| |5。| SUBSTRING| 它用于获取给定String的子字符串。| |6。 |REPLACE |它用于替换一个字符串的子字符串。|\n\n##### UPPER\n\n它需要一个字符串作为输入并转换为大写字母。 所有CQL函数应使用“（）”括号。\n\n**函数语法**\n\n```\nUPPER (<input-string>)\n```\n\n注意：-\n\n可以是来自Neo4J数据库的节点或关系的属性名称。\n\n**示例-**\n\n此示例演示如何使用CQL UPPER String函数以大写形式检索Employee节点的Ename属性详细信息。\n\n步骤1 -在数据浏览器的美元提示符处键入以下命令。\n\n```\nMATCH (e:Employee) \nRETURN e.id,e.name,e.sal,e.deptno\n```\n\n将e.name改成大写（旧版本中已失效）\n\n```\nMATCH (e:Employee) \nRETURN e.id,UPPER(e.name),e.sal,e.deptno\n```\n\n将e.name改成大写（新版本中生效）\n\n```\nMATCH (e:Employee) \nRETURN e.id,TOUPPER(e.name),e.sal,e.deptno\n```\n\n##### TOLOWER\n\n它需要一个字符串作为输入并转换为小写字母。 所有CQL函数应使用“（）”括号。\n\n```\nMATCH (e:Employee) \nRETURN e.id,toLOWER(e.name),e.sal,e.deptno\n```\n\n##### SUBSTRING\n\n它接受一个字符串作为输入和两个索引：一个是索引的开始，另一个是索引的结束，并返回从StartInded到EndIndex-1的子字符串。 所有CQL函数应使用“（）”括号。\n\n###### 函数的语法\n\n```\nSUBSTRING(<input-string>,<startIndex> ,<endIndex>)\n```\n\n**注意：-**\n\n在Neo4J CQL中，如果一个字符串包含n个字母，则它的长度为n，索引从0开始，到n-1结束。\n\n是SUBSTRING函数的索引值。\n\n是可选的。 如果我们省略它，那么它返回给定字符串的子串从startIndex到字符串的结尾。\n\n让我们用一个例子来研究一下。\n\n示例-\n\n此示例演示如何检索所有员工详细信息的名称属性的前两个字母。\n\n```\nMATCH (e:Employee) \nRETURN e.id,SUBSTRING(e.name,0,2),e.sal,e.deptno\n```\n\n它使用SUBSTRING（）String函数打印Employee名称的前两个字母。\n\n------\n\n##### **AGGREGATION聚合**\n\n和SQL一样，Neo4j CQL提供了一些在RETURN子句中使用的聚合函数。 它类似于SQL中的GROUP BY子句。\n\n我们可以使用MATCH命令中的RETURN +聚合函数来处理一组节点并返回一些聚合值。\n\n聚合函数列表\n\n|S.No.| 聚集功能| 描述| |---------|------------| |1。| COUNT |它返回由MATCH命令返回的行数。| |2。| MAX |它从MATCH命令返回的一组行返回最大值。| |3。 |MIN |它返回由MATCH命令返回的一组行的最小值。| |4。| SUM| 它返回由MATCH命令返回的所有行的求和值。|| |5。 |AVG |它返回由MATCH命令返回的所有行的平均值。|\n\n###### 计数\n\n它从MATCH子句获取结果，并计算结果中出现的行数，并返回该计数值。 所有CQL函数应使用“（）”括号。 ####### 函数语法\n\n```\nCOUNT(<value>)\n```\n\n注意 -\n\n可以是*，节点或关系标签名称或属性名称。\n\n示例-\n\n此示例演示如何使用COUNT（*）函数返回数据库中可用的Employee节点数。\n\n```\nMATCH (e:Employee) RETURN COUNT(*)\n```\n\n------\n\n##### MAX\n\n它采用一组行和节点或关系的作为输入，并从给定行的give 列中查找最大值。\n\n###### 函数语法\n\n```\nMAX(<property-name> )\n```\n\n##### MIN\n\n它采用一组行和节点或关系的作为输入，并从给定行的give 列中查找最小值。\n\n###### 函数语法\n\n```\nMIN(<property-name> )\n```\n\n注意 -\n\n应该是节点或关系的名称。\n\n**示例-**\n\n此示例演示如何从所有员工节点中查找最高和最低工资值\n\n```\nMATCH (e:Employee) \nRETURN MAX(e.sal),MIN(e.sal)\n```\n\n##### AVG\n\n它采用一组行和节点或关系的作为输入，并从给定行的give 列中查找平均值。\n\n```\nAVG(<property-name> )\n```\n\n##### SUM\n\n它采用一组行和节点或关系的作为输入，并从给定行的give 列中查找求和值。\n\n###### 函数的语法\n\n```\nSUM(<property-name> )\n```\n\n**例子** 此示例演示如何查找所有员工节点的总和平均薪水值\n\n```\nMATCH (e:Employee) \nRETURN SUM(e.sal),AVG(e.sal)\n```\n\n#### 关系函数\n\nNeo4j CQL提供了一组关系函数，以在获取开始节点，结束节点等细节时知道关系的细节。\n\n在这里，我们将讨论一些重要的和经常使用的功能。 关系函数列表\n\n|S.No.| 功能| 描述| |----|-----| |1。| STARTNODE| 它用于知道关系的开始节点。| |2。 |ENDNODE |它用于知道关系的结束节点。| |3。| ID |它用于知道关系的ID。| |4。| TYPE |它用于知道字符串表示中的一个关系的TYPE。|\n\n##### STARTNODE和ENDNODE\n\n它需要一个字符串作为输入并为大写格式， 所有CQL函数应使用“（）”括号。\n\n###### 函数语法\n\n```\nSTARTNODE (<relationship-label-name>)\nENDNODE (<relationship-label-name>)\n```\n\n注意： 可以是来自Neo4j数据库的节点或关系的属性名称。\n\n示例-\n\n此示例演示如何使用CQL STARTNODE关系函数来检索关系的开始节点详细信息。\n\n在关系“ACTION_MOVIES”上执行STARTNODE（）函数之前，我们将检查其详细信息\n\n```\nMATCH (a)-[movie:ACTION_MOVIES]->(b) \nRETURN STARTNODE(movie)\nMATCH (a)-[movie:ACTION_MOVIES]->(b) \nRETURN ENDNODE(movie)\n```\n\n##### ID和TYPE\n\nID和TYPE关系函数来检索关系的Id和类型详细信息。\n\n```\nMATCH (a)-[movie:ACTION_MOVIES]->(b) \nRETURN ID(movie),TYPE(movie)\nMATCH (a)-[movie:ACTION_MOVIES]->(b) \nRETURN STARTNODE(movie)\n```\n\n它使用STARTNODE（）关系函数打印关系“ACTION_MOVIES”的开始节点。\n\n**示例-**\n\n此示例演示如何使用CQL ENDNODE关系函数来检索关系的结束节点详细信息。\n\n在关系“ACTION_MOVIES”上执行ENDNODE（）函数之前，我们将检查它的详细信息\n\n```\nMATCH (video1:YoutubeVideo1)-[movie:ACTION_MOVIES]->(video2:YoutubeVideo2) \nRETURN movie\n```\n\n在这里，我们可以观察到关系的结束节点“ACTION_MOVIES”是“YoutubeVideo2”。 让我们检查这个与功能。\n\n```\nMATCH (a)-[movie:ACTION_MOVIES]->(b) \nRETURN ENDNODE(movie)\n```\n\n**示例-**\n\n此示例演示如何使用CQL ID和TYPE关系函数来检索关系的Id和类型详细信息。\n\n在关系“ACTION_MOVIES”上执行ID和TYPE函数之前，我们将检查其详细信息\n\n```\nMATCH (a)-[movie:ACTION_MOVIES]->(b) \nRETURN ID(movie),TYPE(movie)\n```\n\n[========]\n\n### Neo4j - 数据库备份和恢复\n\n在实时应用程序中，我们应定期备份应用程序数据库，以便在任何故障点恢复到某种工作状态。\n\n此规则适用于RDBMS和无SQL数据库。\n\n在本节中，我们将讨论两个重要的DBA任务。\n\n- 如何备份Neo4j数据库。\n- 如何将Neo4j数据库还原到特定的备份。\n\n**注意：-**\n\n这些步骤仅适用于Windows操作系统。 我们应该使用类似的命令在其他操作系统中执行相同的步骤。\n\n##### Neo4j数据库备份\n\n（空）\n\n##### 索引\n\n> Neo4j SQL支持节点或关系属性上的索引，以提高应用程序的性能。 我们可以为具有相同标签名称的所有节点的属性创建索引。 我们可以在MATCH或WHERE或IN运算符上使用这些索引列来改进CQL Command的执行。\n\n###### Neo4J索引操作\n\n- Create Index 创建索引\n- Drop Index 丢弃索引 我们将在本章中用示例来讨论这些操作。\n\n###### 创建索引语法：\n\nNeo4j的CQL提供“CREATE INDEX”命令创建的节点或关系的属性索引。\n\n###### 创建索引的语法：\n\n```\nCREATE INDEX ON :<label_name> (<property_name>)\n```\n\n**注意**：-\n\n冒号（:)运算符用于引用节点或关系标签名称。\n\n上述语法描述它在节点或关系的<label_name>的<property_name>上创建一个新索引。\n\n**示例**-\n\n此示例演示如何在CreditCard节点的number属性上创建INDEX。\n\n步骤1 -在数据浏览器上键入以下命令\n\n```\nCREATE INDEX ON :Customer (name)\n```\n\n------\n\n##### Drop Neo4j索引\n\nNeo4j CQL已提供“DROP INDEX”命令删除NODE或Relationship的属性的现有索引。\n\n###### Drop Index语法：\n\n```\nDROP INDEX ON :<label_name> (<property_name>)\n```\n\n注意：-\n\n冒号（:)运算符用于引用节点或关系标签名称。\n\n上述语法描述它删除在节点或关系的<label_name>的<property_name>上创建的现有索引。\n\n示例-\n\n此示例演示如何删除CreditCard节点的number属性上的INDEX。\n\n```\nDROP INDEX ON :Customer (name)\n```\n\n> **创建了索引怎么使用呢？**\n\n------\n\n#### Neo4j CQL - UNIQUE约束\n\n> 在Neo4j数据库中，CQL CREATE命令始终创建新的节点或关系，这意味着即使您使用相同的值，它也会插入一个新行。 根据我们对某些节点或关系的应用需求，我们必须避免这种重复。 然后我们不能直接得到这个。 我们应该使用一些数据库约束来创建节点或关系的一个或多个属性的规则。 像SQL一样，Neo4j数据库也支持对NODE或Relationship的属性的UNIQUE约束\n\nUNIQUE约束的优点\n\n- 避免重复记录。\n- 强制执行数据完整性规则。\n\nNeo4j CQL UNIQUE约束操作\n\n- 创建UNIQUE约束\n- 丢弃UNIQUE约束。\n\n创建UNIQUE约束 Neo4j CQL已提供“CREATE CONSTRAINT”命令，以在NODE或关系的属性上创建唯一约束。\n\n创建唯一约束语法\n\n```\nCREATE CONSTRAINT ON (<label_name>)\nASSERT <property_name> IS UNIQUE\n```\n\n语法说明：\n\n|S.No.| 语法元素| 描述 |---------|----------| |1。| CREATE CONSTRAINT ON| 它是一个Neo4j CQL关键字。| |2。| <label_name>| 它是节点或关系的标签名称。|| |3。| ASSERT |它是一个Neo4j CQL关键字。| |4。| <property_name> |它是节点或关系的属性名称。| |5。| IS UNIQUE |它是一个Neo4j CQL关键字，通知Neo4j数据库服务器创建一个唯一约束。|\n\n此示例演示如何在CreditCard节点的number属性上创建UNIQUE约束。\n\n```\nCREATE CONSTRAINT ON (cc:CreditCard)\nASSERT cc.number IS UNIQUE\n```\n\n这表明，创建具有相同CreditCard.number的重复节点是不可能的，因为它有唯一约束\n\n#### Neo4j CQL - DROP UNIQUE\n\n我们已经讨论了使用前一章中的示例创建UNIQUE约束操作。 现在我们将讨论使用本章中的示例删除UNIQUE约束操作。\n\n##### 删除UNIQUE约束\n\nNeo4j CQL提供了“DROP CONSTRAINT”命令，以从NODE或Relationship的属性中删除现有的Unique约束。\n\n###### 删除UNIQUE约束语法：\n\n```\nDROP CONSTRAINT ON (<label_name>)\nASSERT <property_name> IS UNIQUE\n```\n\n语法说明\n\n|S.No.| 语法元素| 描述| |---|----| |1。| DROP CONSTRAINT ON |它是一个Neo4j CQL关键字。| |2。 |<label_name> |它是节点或关系的标签名称。| |3。| ASSERT |它是一个Neo4j CQL关键字。| |4。| <property_name> |它是节点或关系的属性名称。| |5。| IS UNIQUE |它是一个Neo4j CQL关键字，通知Neo4j数据库服务器创建一个唯一约束。|\n\n**注意 -**\n\n上述语法描述它从节点或关系的<label_name>的<property_name>中删除唯一约束。\n\n**示例** 此示例演示如何从CreditCard节点的number属性删除现有UNIQUE约束。\n\n```\nMATCH (cc:CreditCard) \nRETURN cc.id,cc.number,cc.name,cc.expiredate,cc.cvv\n```\n\n删除UNIQUE约束\n\n```\nDROP CONSTRAINT ON (cc:CreditCard)\nASSERT cc.number IS UNIQUE\n```\n\n#### Neo4j - Java简介\n\nNeo4j提供JAVA API以编程方式执行所有数据库操作。\n\n它支持两种类型的API：\n\n- Neo4j的原生的Java API\n- Neo4j Cypher Java API Neo4j原生Java API是一种低级别的纯JAVA API，用于执行数据库操作。 Neo4j Cypher Java API是简单而强大的JAVA API，用于执行所有CQL命令以执行数据库操作。\n\n#### 原生Java API\n\nNeo4j原生Java API示例 此示例演示如何在Eclipse IDE中开发Java应用程序以开发和测试Neo4j原生ava API示例\n\n- 第1步 -在同一个Java项目中创建一个Java程序 现在开始编写Neo4j Java API编码以执行Neo4j DB操作\n- 第2步 -创建Neo4j数据库\n\n```\nGraphDatabaseFactory dbFactory = new GraphDatabaseFactory();\nGraphDatabaseService db= dbFactory.newEmbeddedDatabase(\"C:/TPNeo4jDB\");\n```\n\n它在指定的路径为我们创建一个Schema / Database，如下所示。这类似于Oracle SQL的“CREATE DATABASE”命令。\n\n- 第3步 -启动Neo4j数据库事务以提交我们的更改\n\n```\ntry (Transaction tx = graphDb.beginTx()) {\n\t// Perform DB operations\t\t\t\t\n\ttx.success();\n}\n```\n\n所以对于你的Java程序源代码看起来像\n\n```\npackage com.tp.neo4j.java.examples;\nimport org.neo4j.graphdb.GraphDatabaseService;\nimport org.neo4j.graphdb.Transaction;\nimport org.neo4j.graphdb.factory.GraphDatabaseFactory;\npublic class Neo4jJavaAPIDBOperation {\n  public static void main(String[] args) {\n\tGraphDatabaseFactory dbFactory = new GraphDatabaseFactory();\n\tGraphDatabaseService db = dbFactory.newEmbeddedDatabase(\"C:/TPNeo4jDB\");\n\ttry (Transaction tx = db.beginTx()) {\n\t\t// Perform DB operations\t\n\t\ttx.success();\n\t}\t\n }\n}\n```\n\n- 第4步 -要创建节点，我们需要标签名称。 通过实现Neo4j Java API“Label”接口创建一个枚举。\n\n```\npackage com.tp.ne4oj.java.examples;\nimport org.neo4j.graphdb.Label;\npublic enum Tutorials implements Label {\n\tJAVA,SCALA,SQL,NEO4J;\n}\n```\n\n- 第5步 -创建节点并为其设置属性 创建两个节点\n\n```\nNode javaNode = db.createNode(Tutorials.JAVA);\nNode scalaNode = db.createNode(Tutorials.SCALA);\n```\n\n设置属性\n\n```\njavaNode.setProperty(\"TutorialID\", \"JAVA001\");\njavaNode.setProperty(\"Title\", \"Learn Java\");\njavaNode.setProperty(\"NoOfChapters\", \"25\");\njavaNode.setProperty(\"Status\", \"Completed\");\t\nscalaNode.setProperty(\"TutorialID\", \"SCALA001\");\nscalaNode.setProperty(\"Title\", \"Learn Scala\");\nscalaNode.setProperty(\"NoOfChapters\", \"20\");\nscalaNode.setProperty(\"Status\", \"Completed\");\n```\n\n- 第6步 -要创建关系，我们需要关系类型。 通过实现Neo4j“关系类型”创建枚举。\n\n```\npackage com.tp.neo4j.java.examples;\nimport org.neo4j.graphdb.RelationshipType;\npublic enum TutorialRelationships implements RelationshipType{\n\tJVM_LANGIAGES,NON_JVM_LANGIAGES;\n}\n```\n\n- 第7步 -创建节点之间的关系并设置它的属性。 创建从Java节点到Scala节点的关系\n\n```\nRelationship relationship = javaNode.createRelationshipTo(scalaNode,\n\tTutorialRelationships.JVM_LANGIAGES);\n```\n\n将属性设置为此关系\n\n```\nrelationship.setProperty(\"Id\",\"1234\");\nrelationship.setProperty(\"OOPS\",\"YES\");\nrelationship.setProperty(\"FP\",\"YES\");\n```\n\n- 第8步 -最终源代码。\n\n```\nimport org.neo4j.graphdb.GraphDatabaseService;\nimport org.neo4j.graphdb.Node;\nimport org.neo4j.graphdb.Relationship;\nimport org.neo4j.graphdb.Transaction;\nimport org.neo4j.graphdb.factory.GraphDatabaseFactory;\n\npublic class Neo4jJavaAPIDBOperation {\npublic static void main(String[] args) {\n\tGraphDatabaseFactory dbFactory = new GraphDatabaseFactory();\n\tGraphDatabaseService db= dbFactory.newEmbeddedDatabase(\"C:/TPNeo4jDB\");\n\ttry (Transaction tx = db.beginTx()) {\n\n\t\tNode javaNode = db.createNode(Tutorials.JAVA);\n\t\tjavaNode.setProperty(\"TutorialID\", \"JAVA001\");\n\t\tjavaNode.setProperty(\"Title\", \"Learn Java\");\n\t\tjavaNode.setProperty(\"NoOfChapters\", \"25\");\n\t\tjavaNode.setProperty(\"Status\", \"Completed\");\t\t\t\t\n\t\t\n\t\tNode scalaNode = db.createNode(Tutorials.SCALA);\n\t\tscalaNode.setProperty(\"TutorialID\", \"SCALA001\");\n\t\tscalaNode.setProperty(\"Title\", \"Learn Scala\");\n\t\tscalaNode.setProperty(\"NoOfChapters\", \"20\");\n\t\tscalaNode.setProperty(\"Status\", \"Completed\");\n\t\t\n\t\tRelationship relationship = javaNode.createRelationshipTo\n\t\t(scalaNode,TutorialRelationships.JVM_LANGIAGES);\n\t\trelationship.setProperty(\"Id\",\"1234\");\n\t\trelationship.setProperty(\"OOPS\",\"YES\");\n\t\trelationship.setProperty(\"FP\",\"YES\");\n\t\t\n\t\ttx.success();\n\t}\n\t   System.out.println(\"Done successfully\");\n}\n}\n```\n\n- 第9步 -在执行此Java程序之前，检查您的Neo4j是否处于关闭模式。 如果没有，请点击“停止”按钮展开它。\n- 第10步 -执行Java程序并在Eclipse IDE控制台中观察输出。 一旦此数据库成功启动，通过单击“http：// localhost：7474”链接访问Neo4j浏览器来观察我们的数据。\n- 第11步 -在Neo4j数据浏览器的$ prompt下键入以下命令\n\n```\nMATCH (a)-[r:JVM_LANGIAGES]->(b)\nRETURN r\n```\n\n注意 -\n\n如果我们的Neo4j服务器通过引用我们新创建的数据库启动和运行，那么我们不能执行我们的程序，因为服务器已经锁定了这个数据库。\n\n所以当我们执行我们以前的程序时，我们会得到一些错误堆栈跟踪\n\njava.io.IOException：C:\\TPNeo4jDB\\lock because another process already holds the lock.\n\n为了避免这个问题，首先停止我们的服务器，然后执行程序。\n\n因为默认情况下Neo4j DB Server一次只接受一个锁。 在实时应用程序中，Ne04J DBA人员将更新数据库属性以允许一次允许一些数量的锁。\n\n------\n\n#### Neo4j Cypher - API示例\n\n本章中讨论Neo4j Cypher Java API。 如果你观察到Neo4j Native Java API方法，开发大型应用程序是非常乏味和麻烦的。 所以为了避免这种复杂性，Neo4j引入了另一组Java API。\n\n此Java API用于直接执行Neo4j CQL命令。 它类似于JDBC API直接执行SQL命令。\n\n##### Neo4j Cypher Java API示例\n\n> 本示例演示如何在Eclipse IDE中开发Java应用程序，以开发和测试Neo4j Cypher Java API示例\n\n- 第1步 创建Java类JavaNeo4jCQLRetrivalTest\n\n现在开始编写Neo4j Java API编码以执行Neo4j DB操作\n\n- 第2步 -创建Neo4j数据库\n\n```\nGraphDatabaseFactory dbFactory = new GraphDatabaseFactory();\nGraphDatabaseService db= dbFactory.newEmbeddedDatabase(\"C:/TPNeo4jDB\");\n```\n\n它在指定的路径为我们创建一个Schema / Database，如下所示。这类似于Oracle SQL的“CREATE DATABASE”命令。\n\n- 第3步 -创建Neo4j Cypher执行引擎。它用于在Java应用程序中执行Neo4j CQL命令。\n\n```\nExecutionEngine execEngine = new ExecutionEngine(graphDb);\n```\n\n- 第4步 - 通过使用Neo4j Cypher Execution Engine，执行Neo4j CQL Command以检索CQL MATCH命令的结果。\n\n```\nExecutionResult execResult = execEngine.execute\n   (\"MATCH (java:JAVA) RETURN java\");\n```\n\n- 第5步 -获取CQL命令结果的字符串，以在控制台中打印结果。\n\n```\nString results = execResult.dumpToString();\nSystem.out.println(results);\n```\n\n- 第6步 -最终源代码。\n\n```\npackage com.tp.neo4j.java.cql.examples;\n\nimport org.neo4j.cypher.javacompat.ExecutionEngine;\nimport org.neo4j.cypher.javacompat.ExecutionResult;\nimport org.neo4j.graphdb.GraphDatabaseService;\nimport org.neo4j.graphdb.factory.GraphDatabaseFactory;\n\npublic class JavaNeo4jCQLRetrivalTest {\n    \n   public static void main(String[] args) {\n      GraphDatabaseFactory graphDbFactory = new GraphDatabaseFactory();\n\n      GraphDatabaseService graphDb = graphDbFactory.newEmbeddedDatabase(\"C:/TPNeo4jDB\");\n    \n      ExecutionEngine execEngine = new ExecutionEngine(graphDb);\n      ExecutionResult execResult = execEngine.execute(\"MATCH (java:JAVA) RETURN java\");\n      String results = execResult.dumpToString();\n      System.out.println(results);\n   }\n}\n```\n\n- 第7步 -在Neo4j数据浏览器的$ prompt下输入以下命令\n\n```\nMATCH (java:JAVA) RETURN java.TutorialID,java.Title,\n   java.NoOfChapters,java.Status\n```\n\n注意 -\n\n像这样，我们可以使用Neo4j JAVA API执行任何CQL命令。\n\n如果我们的Neo4j服务器通过引用我们新创建的数据库启动和运行，那么我们不能执行我们的程序，因为服务器已经锁定了这个数据库。\n\n所以当我们执行我们以前的程序时，我们会得到一些错误堆栈跟踪\n\njava.io.IOException: Couldn't lock lock file C:\\TPNeo4jDB\\lock because another process already holds the lock.\n\n为了避免这个问题，首先停止我们的服务器，然后执行程序。\n\n因为默认情况下Neo4j DB Server一次只接受一个锁。 在实时应用程序中，Ne04J DBA人员将更新数据库属性以允许一次允许一些数量的锁。\n\n[========]\n\n#### Spring DATA Neo4J - 简介\n\n#### Spring DATA Neo4J - 环境\n\n在本章中，我们将讨论如何在Eclipse IDE中设置Maven Java项目，以使用Spring DATA Neo4j模块开发Spring Framework应用程序。\n\n- 第1步 - 在Eclipse IDE中创建Maven项目\n- 第2步-在Eclipse IDE中打开pom.xml文件，并添加以下主要依赖关系\n\nSpring DATA Neo4j模块Jar文件\n\n```\n<dependency>\n   <groupId> org.springframework.data </groupId>\n   <artifactId> spring-data-neo4j </artifactId>\n   <version> 3.1.2.RELEASE </version>\n</dependency>\n```\n\nNeo4j Jar文件，由Spring DATA Neo4j模块Jar文件内部使用\n\n```\n<dependency>\n   <groupId> org.neo4j </groupId>\n   <artifactId> neo4j-kernel </artifactId>\n   <version> 2.1.3 </version>\n</dependency>\n```\n\nJava事务API jar文件，由Spring DATA Neo4j模块Jar文件内部使用\n\n```\n<dependency>\n   <groupId> javax.transaction </groupId>\n   <artifactId> jta </artifactId>\n   <version> 1.1 </version>\n</dependency>\n```\n\nJava验证API jar文件，由Spring DATA Neo4j模块Jar文件内部使用\n\n```\n<dependency>\n   <groupId> javax.validation </groupId>\n   <artifactId> validation-api </artifactId>\n   <version> 1.0.0.GA </version>\n</dependency>\n```\n\n第3步-完成pom.xml文件\n\n```\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" \n   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n   xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 \n   http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n\n   <modelVersion> 4.0.0 </modelVersion>\n   <groupId> com.tp.neo4j </groupId>\n   <artifactId> springdata-neo4j </artifactId>\n   <version> 1.0 </version>  \n\n   <dependencies>\n      <dependency>   \n         <groupId> org.springframework.data </groupId>\n         <artifactId> spring-data-neo4j </artifactId>\n         <version> 3.1.2.RELEASE </version>\n      </dependency>\n      \n      <dependency>\n         <groupId> org.neo4j </groupId>\n         <artifactId> neo4j-kernel </artifactId>\n         <version> 2.1.3 </version>\n      </dependency>  \n      \n      <dependency>\n         <groupId> javax.transaction </groupId>\n         <artifactId> jta </artifactId>\n         <version> 1.1 </version>\n      </dependency>\n      \n      <dependency>\n         <groupId>javax.validation</groupId>\n         <artifactId>validation-api</artifactId>\n         <version>1.0.0.GA</version>\n      </dependency>\n\n   </dependencies>   \n</project>\n```\n\n#### Spring DATA Neo4j - 示例\n\n> 我们将讨论如何开发一个 Spring 框架项目来使用 Neo4j 数据库。\n\n##### Spring DATA Neo4j 模块注释\n\n我们将使用以下 Spring Framework 注释来开发此应用程序。\n\n|S.No.| Spring DATA Neo4j注释 |用法 |--------|----------| |1| @GraphEntity |定义域类Neo4j Entity| |2| @GraphID |定义节点或关系id| |3| @GraphProperty |定义节点或关系属性| 在开发应用程序之前，请参考“Neo4j Spring DATA环境设置”一章来设置 Maven Eclipse IDE 项目。\n\n（空）\n\n------\n\n### 常见CQL语句\n\n```\n新增：\n\nCREATE (emp:Employee)         CREATE（节点名：标签名）\n\nCREATE (dept:Dept { deptno:10,dname:\"Accounting\",location:\"Hyderabad\" })             创建一个带有属性的标签名\n\nCREATE (fb1:FaceBookProfile1)-[like:LIKES]→(fb2:FaceBookProfile2)                     新增关系\n\n查询：\n\nMATCH (n:`hx/sor_config`) RETURN n LIMIT 25   查询sor_config中id前25\n\nMATCH (dept:Dept) RETURN dept.deptno   查询Dept\n\nMATCH (n:`hx/sor_config`) WHERE n.name=\"ruleset3\" RETURN n  条件查询\n\nMATCH (p1:`hx/sor_config`{name:\"手术程序\"}),(p2:`hx/sor_config`{name:\"structured_situation\"}), p=shortestpath((p1)-[*..10]->(p2)) RETURN p                   查询p1到p2的最短路径\n\nMATCH (n:`hx/sor_config`{name:\"手术名称\"})-[r*..1]-(m) return n,r,m      查询某个节点的直接关系的其他节点\n\n删除：\n\nMATCH (e: Employee) DELETE e   删除标签\n\nmatch (n:FaceBookProfile1)-[r]-(m:FaceBookProfile2)    delete r   删除关系\n\nMATCH (n)    OPTIONAL MATCH (n)-[r]-()    DELETE n,r   清空数据库\n\nMATCH (n:`hx/sor_config`{name:\"有无袖式成型切除\"}) DETACH DELETE n            删除某一节点及其所有关系\n\n修改：\n\nMATCH (n:Empty)   SET n.atm_pin = 358   RETURN n\n```\n\n### Python代码\n\n```\n#coding:utf-8\nfrom py2neo import Graph,Node,Relationship\n\n##连接neo4j数据库，输入地址、用户名、密码\ngraph = Graph('http://localhost:7474',username='neo4j',password='password')\n\n##创建结点\ntest_node_1 = Node('ru_yi_zhuan',name='皇帝')\ntest_node_2 = Node('ru_yi_zhuan',name='皇后')\ntest_node_3 = Node('ru_yi_zhuan',name='公主')\ngraph.create(test_node_1)\ngraph.create(test_node_2)\ngraph.create(test_node_3)\n\n##创建关系\n#分别建立了test_node_1指向test_node_2和test_node_2指向test_node_1两条关系，关系的类型为\"丈夫、妻子\"，两条关系都有属性count，且值为1。\nnode_1_zhangfu_node_1 = Relationship(test_node_1,'丈夫',test_node_2)\nnode_1_zhangfu_node_1['count'] = 1\nnode_2_qizi_node_1 = Relationship(test_node_2,'妻子',test_node_1)\nnode_2_munv_node_1 = Relationship(test_node_2,'母女',test_node_3)\n\nnode_2_qizi_node_1['count'] = 1\n\ngraph.create(node_1_zhangfu_node_1)\ngraph.create(node_2_qizi_node_1)\ngraph.create(node_2_munv_node_1)\n```\n","tags":["知识图谱"],"categories":["自然语言处理"]},{"title":"小秋","url":"/2022/03/26/小秋/","content":"\n·\n\n> 有一个人会在一个特别的季节里留给你她最美的样子，她会一直和这个季节一起，在你的生命中时常浮现，而又四季轮回。\n\n《小秋》\n\n盲目的人在独处\n\n孤独的人在忙碌\n\n多情的小雨它朝朝暮暮\n\n执着的风在赶路\n\n姑娘的晚风楚楚\n\n少年的晨曦浅梦\n\n我站在风里抱着自己\n\n裹着有你的秘密\n\n往事如风的秋天里\n\n你穿着你浅色的毛衣\n\n在北方的秋凉里\n\n吹着温柔的牧笛\n\n画如水墨的冬天里你裹着你红色的围脖\n\n像灰白的画卷里一团红色的火\n\n故乡的人已入梦\n\n远方的人已匆匆\n\n离家的叶子它划过天空\n\n落入了你的怀中\n\n姑娘的晚风楚楚\n\n少年的晨曦浅梦\n\n我站在风里抱着自己\n\n裹着有你的秘密\n\n往事如风的秋天里\n\n你穿着你浅色的毛衣\n\n在北方的秋凉里\n\n吹着温柔的牧笛\n\n画如水墨的冬天里你裹着你红色的围脖\n\n像灰白的画卷里一团红色的火\n\n往事如风的秋天里\n\n你穿着你浅色的毛衣\n\n在北方的秋凉里\n\n吹着温柔的牧笛\n\n画如水墨的冬天里你裹着你红色的围脖\n\n像灰白的画卷里一团红色的火\n\n像灰白的画卷里一团红色的火\n\n2022年3月26日夜1:01 晚安\n","tags":["生活随笔"],"categories":["live"]},{"title":"第一篇博客","url":"/2022/01/10/第一篇博客/","content":"\n\n# 简单写点东西","tags":["技术笔记"],"categories":["AI"]},{"title":"推荐系统之评估方法和评价指标PR、ROC、AUC","url":"/1984/01/24/基于物品的协同过滤算法（ItemCF）原理以及代码实践的副本/","content":"\n\n\n# 推荐系统文档\n\n## 1、推荐系统之评估方法和评价指标PR、ROC、AUC\n\n> 链接：https://www.jianshu.com/p/5b0bc79e3d75\n\n# 简介\n\n推荐系统的评估相关的知识比重在整个推荐系统的知识框架中占比不大，但是其重要程度不言而喻，因为采用的评价指标直接影响到了推荐系统的优化方向是否正确。**评价指标主要用于评价推荐系统各方面的性能**，按照应用场景可以分为离线评估和线上测试。其中离线评估的主要方法包括**Holdout检验、交叉检验、留一验证、自助法**等，评价指标主要包括**用户满意度、预测准确度、召回率、覆盖率、多样性、新颖性、流行度、均方根误差、对数损失、P-R曲线、AUC、ROC曲线**等等。线上测试的评估方法主要包括**A/B测试、Interleaving方法**等，评价指标主要包括**点击率、转化率、留存率、平均点击个数**等等。本文将着重介绍**离线评估相关方法和指标**，尤其是**P-R曲线、AUC、ROC曲线**等，这些评价指标是最常用的也是最基本的，出现在各类推荐相关的论文中，因此需要重点掌握。\n\n-------------\n\n# 离线评估方法和评价指标\n\n在推荐系统的评估过程中，离线评估往往被当做最常用也是最基本的评估方法。顾名思义，离线评估是指在将模型部署于线上环境之前，在离线环境中进行的评估。由于不用部署到生产环境，离线评估没有线上部署的工程风险，也无须浪费宝贵的线上流量资源，而且具有测试时间短，同时进行多组并行测试、能够利用丰富的线下计算资源等诸多优点。\n\n## 离线评估的主要方法\n\n- Holdout检验\n  Holdout检验是基础的离线评估方法，它将原始的样本集合随机划分为训练集和测试集两部分。举例来说，对于一个推荐模型，可以把样本按照70%~30%的比例随机分成两部分，70%的样本用于模型的训练，30%的样本可以用于模型的评估。\n  在实际应用中，有很多方便的库可以帮助我们进行样本集合的划分，比如scikit-learn中提供的train_test_split函数，下面进行个简单展示：\n\n\n\n```python\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nx,y = np.arange(10).reshape((5,2)), range(5)\nprint(\"data: \\n\", x)\nprint(\"labels: \", list(y))\n\n# 对数据集进行划分，设置测试集占比30%，训练集占比70%\nX_train, X_test,Y_train,Y_test = train_test_split(x, y, test_size=0.3, random_state=100)\nprint(\"Train Data: \", X_train, Y_train)\nprint(\"Test Data: \", X_test, Y_test)\n```\n\n输出：\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31xahrs25j20cc0bkglx.jpg)\n\nHoldout检验的缺点也很明显，即在验证集上计算出来的评估指标与训练集合验证集的划分有直接关系，如果仅仅进行少量Holdout检验，则得到的结论存在较大的随机性。为了消除这种随机性，“交叉检验”的思想被提出。\n\n- 交叉验证\n  **k-fold交叉验证**：先将全部的样本划分为k个大小相等的样本子集；依次遍历这k个子集，每次都把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后将所有的k次的评估指标的平均值作为最终的评估指标。在实验中，k经常取10。同样,scikit-learn中提供了KFold函数可以使用，例子如下：\n\n\n\n```python\nfrom sklearn.model_selection import KFold\n\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([1, 2, 3, 4])\n\nkf = KFold(n_splits=4)\nkf.get_n_splits(X)\n\nfor train_index, test_index in kf.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n```\n\n输出：![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31xaws6m1j20bo04e74g.jpg)\n\n**留一验证**：每次留下1个样本作为验证集，其余所有样本作为测试集。样本总数为n，依次遍历所有n个样本，进行n次验证，在将评估指标求平均得到最终指标，在样本总数较多的情况下，留一验证法的时间开销极大。事实上，留一验证是留p验证的特例。留p验证是指每次留下p个样本作为验证集，而从n个元素中选取p个元素共有<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31xbd6tkkj201m0183y9.jpg\" alt=\"image-20220609132353358\" style=\"zoom:50%;\" />种可能，因此它的时间开销远超留一验证，故很少在实际工程中使用。\n同样，scikit-learn中提供了LeaveOneOut方法可使用，例子如下：\n\n```swift\nimport numpy as np\n\nfrom sklearn.model_selection import LeaveOneOut\nX = np.array([[1, 2], [3, 4], [5,6]])\ny = np.array([1, 2, 3])\n\nloo = LeaveOneOut()\nloo.get_n_splits(X)\n\nfor train_index, test_index in loo.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    print(X_train, X_test, y_train, y_test)\n```\n\n结果：\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31xbt3o5oj20cs090q3d.jpg)\n\n- 自助法\n  不管是Holdout检验还是交叉检验，都是基于划分训练集和测试集的方法进行模型评估的。然而，当样本规模比较小的时候，将样本集进行划分会让训练集进一步减小，这可能会影响到模型的训练效果。有没有能维持训练集样本规模的验证方法呢？“自助法”可以在一定程度上解决这个问题。\n  自助法是基于自助采样法的检验方法：对于总数为n的样本集合，进行n次有放回的随机抽样，得到大小为n的训练集。在n次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集进行模型验证，这就是自助法的验证过程。\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31xdawo9tj210q098dhc.jpg\" alt=\"image-20220609132533595\" style=\"zoom:50%;\" />\n\n## 离线评估的指标\n\n- 均方根误差\n  很多推荐网站都会提供一个让用户给物品打分的功能，如果知道了用户对物品的历史评分数据，就可以从中学习到用户的兴趣模型，并预测该用户在将来浏览到未曾见过的物品时，会给这个物品打多少分。评分预测可以看做是回归问题，评分预测的预测准确度一般是通过均方差误差(RMSE)和平均绝对误差(MAE)计算。对于测试集中的一个用户<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31xdtnvikj20120180hl.jpg\" alt=\"image-20220609132615154\" style=\"zoom:50%;\" />和物品<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31xeak7anj201201e0jd.jpg\" alt=\"image-20220609132641992\" style=\"zoom:50%;\" />，我们令<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31xf1hsscj201m01e0r9.jpg\" alt=\"image-20220609132724384\" style=\"zoom:50%;\" />代表用户<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31xdtnvikj20120180hl.jpg\" alt=\"image-20220609132615154\" style=\"zoom:50%;\" />对物品<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31xeak7anj201201e0jd.jpg\" alt=\"image-20220609132641992\" style=\"zoom:50%;\" />的实际评分，而<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31xgkgib7j201q01i0s9.jpg\" alt=\"image-20220609132853052\" style=\"zoom:50%;\" />代表我们的推荐算法给出的预测评分。\n  综上，可以得出RSME的定义为：    \n\n  ​\t\t\t\t\t\t\t\t\t\t\t ![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31xh51u1cj207802jq2s.jpg)     \n\n​\t同理，MAE采用绝对值计算预测误差，定义如下：:\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31yosny0bj206t02jweb.jpg)\n\n其中|T|代表测试集的大小。\n一般情况下，RMSE能够很好滴反映回归模型预测值与真实值的偏离程度。但在实际应用时，如果存在个别偏离程度非常大的离群点，那么即使离群点数量非常少，也会让RSME指标变得很差。为了解决这个问题，可以使用鲁棒性更强的平均绝对百分比误差（Mean Absolute Percent Error，MAPE）进行类似的评估，MAPE的定义如下：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31yps1eklj20io05eaa2.jpg\" alt=\"image-20220609141219926\" style=\"zoom:50%;\" />\n\n相比RSME，MAPE相当于把每个点的误差都进行了归一化，降低了个别离群点带来的绝对误差的影响。\n\n- 覆盖率\n\n  覆盖率是描述一个推荐系统对物品长尾的发掘能力，即推荐系统做到了雨露均沾，对商城中的每一个物品都有涉及，而不是只推荐那些热门的商品。据此，覆盖率的一个简单定义为推荐系统能够推荐出来的物品占总物品集合的的比例。假设系统的用户集合<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31yqp5ujvj201601a0ml.jpg\" alt=\"image-20220609141312721\" style=\"zoom:50%;\" />，推荐系统给每个用户推荐了一个长度为N的物品列表<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31yr312qdj202g01emwx.jpg\" alt=\"image-20220609141333960\" style=\"zoom:50%;\" />，那么覆盖率的计算公式为：:\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h32g5rg0m5j20e2052jrb.jpg\" alt=\"image-20220609141354307\" style=\"zoom:50%;\" />\n\n其中｜I｜是商品的总数。\n\n\n\n- 新颖度与平均流行度\n\n  我们使用推荐列表中全部物品的平均流行度衡量推荐结果的新颖度。如果推荐的物品都很热门，那么说明推荐的新颖度比较低。反之，说明推荐结果比较新颖。\n\n  流行度的定义如下：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31ysonuj7j20go0583yh.jpg\" alt=\"image-20220609141507747\" style=\"zoom: 50%;\" />\n\n其中p是每个物品的流行度，可以通过该物品在测试集中出现的次数来简单计算，N是推荐物品集合的总数。\n\n这里在计算平均流行度的时候对每个物品的流行度取对数，这是因为物品的流行度分布满足长尾分布，取对数后，流行度的平均值更加稳定。\n\n\n\n- 对数损失函数\n\n  对数损失函数（LogLoss）也是经常在离线评估中使用的指数，在一个二分类问题中，LogLoss的定义如下：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31ytfdu1bj20q604yaa5.jpg\" alt=\"image-20220609141550454\" style=\"zoom:50%;\" />\n\n其中，yi为输入实例xi的真实类别，pi为预测输入实例xi是正样本的概率，N是样本总数。\n\n> LogLoss就是逻辑回归的损失函数，而大量深度学习模型的输出层正式逻辑回归或者Softmax，因此采用LogLoss作为评估指标能够非常直观地反应模型损失函数的变化。\n\n- 准确率\n\n  对于分类问题，比如CTR问题，准确率（Accuracy）是指分类正确的样本占总样本个数的比例，即：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31yv659pkj20di04ejra.jpg\" alt=\"image-20220609141730897\" style=\"zoom:50%;\" />\n\n其中，n_corret代表被正确分类的个数，n_total代表总样本个数。准确率是分类任务中较为直观的评价指标，虽然具有较强的可解释性，但是也存在明显的缺陷：当不同类别的样本的比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。例如，如果负样本占比99%，那么分类器将所有样本都预测为负样本，也可以取得99%的准确率。\n\n\n\n- 精准率和召回率\n\n  精准率（Precision）是分类正确的正样本个数占分类器判定为正样本的样本个数的比例。召回率（Recall）是分类正确的正样本个数占真正的正样本个数的比例。\n\n  在排序模型中，通常没有一个确定的阈值把预测结果直接判定为正样本或负样本，而是采用TopN排序结果的精准率（Precision@N）和召回率（Recall@N)来衡量排序模型的性能，即认为模型排序的TopN的结果就是模型判定的正样本，然后分别计算Precision@N和Recall@N。\n\n以TopN推荐为例，令R(u)代表模型根据用户在训练集上的行为给用户计算出的推荐列表，而T(u)代表用户在测试集上的真实喜爱列表。那么推荐结果的精准率的定义如下：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31ywln0cpj20lc074weo.jpg\" alt=\"image-20220609141853524\" style=\"zoom:50%;\" />\n\n从公式上看，它是把用户真实喜爱列表和推荐列表的交集的大小去除以推荐列表的大小，它的意义是计算在所预测的推荐列表中究竟有多少物品是用户感兴趣的。\n召回率的定义如下:\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31ywv0m2fj20jg06st8v.jpg\" alt=\"image-20220609141907865\" style=\"zoom:50%;\" />\n\n可以看到它与精准率的定义非常相似，唯一不同的是分母变成了用户真实喜爱列表大小。它的意义在于用户真实喜爱列表中的物品中有多少是被推荐算法预测出来的，即真实列表的召回率。\n维基百科上的图片很好地展示了Precision和Recall的计算公式，方便记忆:\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31yxddde4j20m814f764.jpg)\n\n- 上图中圆圈内的代表被选出的样本，用其中的正样本除以被选中的样本总数就是Precision，用其中的正样本除以所有的正样本数量就是Recall。\n\n  \n\n> 注意准确率（Accuracy）和精准率（Precision）的区别。\n\n精准率和召回率是矛盾统一的两个指标：为了提高精准率，分类器需要尽量在“更有把握时”才把样本预测为正样本，即降低了精准率计算公式中的分母部分。但往往会因为过于保守而漏掉很多“没有把握”的正样本，导致召回率过低。\n以挑选西瓜为例，若希望将好瓜尽可能多地挑选出来，则可通过增加选瓜的数量来实现，如果将所有的西瓜都选上，那么所有的好瓜也必然都被选上了，这样就会导致Precision很低，但是Recall就会相对较高。若希望选出的瓜中好瓜比例尽可能高，则可只挑选最有把握的瓜，但这样就难免会漏掉不少好瓜，使得Recall较低。\n为了综合反映Precision和Recall的结果，可以使用F1-score，F1-score是精准率和召回率调和平均值，定义如下：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31yxwhwrkj20e604gaa3.jpg\" alt=\"image-20220609142008816\" style=\"zoom:50%;\" />\n\n用一张图总结一下：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31zehwvulj20xc0d0dgu.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n上图左边是混淆矩阵，右边分别是精准率、召回率、F1-score、准确率的计算公式。\n\n\n\n> 关于混淆矩阵，在下一节有详细介绍。\n\n\n\n- P-R曲线\n\n  P-R曲线，顾名思义，其中P代表Precision，R代表Recall。P-R曲线就是根据精确率和召回率而绘制的曲线，一般横轴选择召回率，纵轴选择精确率。对于一个排序模型来说，其P-R曲线上的一个点代表“在某一阈值下，模型将大于该阈值的结果判定为正样本，将小于该阈值的结果判定为负样本时，排序结果对应的召回率和精确率”。整条P-R曲线是通过从高到低移动正样本的阈值生成的。如下图所示，其中包含了3个模型的P-R曲线，其中横轴0点附近代表阈值最大时模型的Precision和Recall。\n\n\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31zeovw2gj20xa0pogn9.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\nP-R图直观地显示出模型在样本总体上的Precision、Recall。在进行比较的时候，若一个模型的P-R曲线被另外一个模型的P-R曲线完全“包住”，则可断言后者的性能优于前者。如上图中模型A的性能就优于模型C；如果两个模型的P-R曲线出现了交叉，如上图汇总的A和B，则难以一般性地断言两者孰优孰劣，只能在具体的Precision和Recall条件下进行比较。\n\n\n\n- **ROC曲线**\n\n  ROC曲线的全称是“the Receiver Operating Characteristic”曲线，中文译为“受试者工作特征曲线”。ROC曲线最早诞生于军事领域，而后在医学领域应用甚广，“受试者工作特征曲线”这一名称也正是来自于医学领域。\n\n  在正式介绍ROC曲线之前，我们先来彻底理解一下混淆矩阵的定义。混淆矩阵中有Positive、Negative、True、False等概念，意义如下：\n\n​\t称预测类别为1的为Positive（阳性），预测类别为0的为Negative（阴性）\n\n​\t预测正确的为True（真），预测错误的为False（伪）\n\n对上述概念进行组合，就产生了如下的混淆矩阵：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31zerkkb7j20id0dg3yy.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n然后，由此引出True Positive Rate（真阳率TPR）、False Positive Rate（伪阳率FPR）两个概念，计算方式如下：\n\n![image-20220609142427911](https://tva1.sinaimg.cn/large/e6c9d24ely1h31z2eftscj204p02z0sk.jpg)\n\n仔细观察上面的两个式子，发现两个式子的分子其实对应了混淆矩阵的第二行，即预测类别为1的那一行。另外可以发现TPR就是用TP除以TP所在的列，FPR就是用FP除以FP所在的列。二者的含义如下：\n\n- TPR代表在所有真实类别为1的样本中，预测类别为1的比例\n- FPR代表在所有真实类别为0的样本中，预测类别为1的比例\n\n如果我们计算出了TPR和FPR，那么ROC曲线的绘制就很简单了，ROC曲线的横轴是FPR、纵轴是TPR，当二者相等时，绘制出的曲线是一条直线，如下：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31zevqc3pj20ky0gidg9.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n表示的意义是：对于不论真实类别是0还是1的样本，模型预测样本为1的概率都是相等的。\n换句话说，模型对正例和负例毫无区分能力，做决策和抛硬币没啥区别。因此，我们认为AUC的最小值为0.5（当然也存在预测相反这种极端的情况，AUC小于0.5，这种情况相当于分类器总是把对的说成错的，错的认为是对的，那么只要把预测类别取反，便得到了一个AUC大于0.5的分类器）。\n\n而我们希望模型达到的效果是：对于真实类别为1的样本，模型预测为1的概率（即TPR），要大于真实类别为0而预测类别为1的概率（即FPR），即y＞x，因此大部分的ROC曲线长成下面这个样子：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31zey6bt4j20kw0gijrw.jpg\" alt=\"img\" style=\"zoom: 50%;\" />\n\n最理想的情况下，既没有真实类别为1而错分为0的样本——TPR一直为1，也没有真实类别为0而错分为1的样本——FPR一直为0，AUC为1，这便是AUC的极大值。\n下面举一个小例子，以分类问题为例，预测类别为离散标签，假设8个样本的预测情况如下：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31zf0x57dj20xc04dt8z.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n\n\n得到的混淆矩阵如下：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31zf3r9uuj20gw094t8y.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n进而计算得到TPR=3/4，FPR=2/4，得到ROC曲线：\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31zf5o9y0j20jg0eumxm.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n可以看到这实际上式两段直线组成的曲线，是因为我们只画出了一个关键点。\n如果对于CTR任务，预测的结果是一个概率值，那应该如何画出ROC曲线呢？比如预测结果如下：\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31z4nawqsj20xc04fmxi.jpg)\n\n这时，需要设置阈值来得到混淆矩阵，不同的阈值会影响得到的TPR，FPR。如果阈值取0.5，小于0.5的为0，否则为1，那么我们就得到了与之前一样的混淆矩阵。其他的阈值就不再啰嗦了。依次使用所有预测值作为阈值，得到一系列TPR，FPR，然后画出关键点，再连线即可得到ROC曲线。\n因此，ROC曲线跟P-R曲线一样，也是通过不断地移动模型正样本阈值来生成的。\n\n- **AUC**\n\n  AUC（Area Under Curve）的意思是曲线下的面积。它通常被定义为[ROC曲线](https://links.jianshu.com/go?to=https%3A%2F%2Fbaike.baidu.com%2Fitem%2FROC%E6%9B%B2%E7%BA%BF%2F775606)下与[坐标轴](https://links.jianshu.com/go?to=https%3A%2F%2Fbaike.baidu.com%2Fitem%2F%E5%9D%90%E6%A0%87%E8%BD%B4%2F9763108)围成的[面积](https://links.jianshu.com/go?to=https%3A%2F%2Fbaike.baidu.com%2Fitem%2F%E9%9D%A2%E7%A7%AF%2F100551)，显然这个面积的数值不会大于1（但是这个曲线也不一定是ROC，也可以是前面提及的P-R曲线）。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。AUC越接近1.0，检测方法真实性越高;等于0.5时，则真实性最低，无应用价值。我们往往使用AUC值作为模型的评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。\n\n  综上，**AUC是衡量二分类模型优劣的一种评价指标，表示预测的正例排在负例前面的概率。**\n\n- **mAP**\n\n  平均精度均值（mean Average Precision， mAP）是另一个在推荐系统、信息领域中常用的评估指标。该指标其实是对平均精度（Average Precision，AP）的再次平均，因此在计算mAP之前，我们先了解一下什么是平均精度。\n\n  假设推荐系统对某一用户测试集的排序结果如下：\n\n| 推荐序列 | N=1  | N=2  | N=3  | N=4  | N=5  | N=6  |\n| -------- | ---- | ---- | ---- | ---- | ---- | ---- |\n| 真实标签 | 1    | 0    | 0    | 1    | 1    | 1    |\n\n其中，1代表正样本，0代表负样本。我们来计算下它们的Precision。如下表所示：\n\n| 推荐序列    | N=1  | N=2  | N=3  | N=4  | N=5  | N=6  |\n| ----------- | ---- | ---- | ---- | ---- | ---- | ---- |\n| 真实标签    | 1    | 0    | 0    | 1    | 1    | 1    |\n| Precision@N | 1/1  | 1/2  | 1/3  | 2/4  | 3/5  | 4/6  |\n\nAP的计算只取正样本处的Precision进行平均，即AP = (1/1+2/4+3/5+4/6)/4=0.6917。如果推荐系统对测试集中每个用户都进行样本排序，那么每个用户都会计算出一个AP值，再对所有用户的AP值进行平均，就得到了mAP。也就是说，mAP是对精确度平均的平均。\n值得注意的是，mAP的计算方法和P-R曲线、ROC曲线的计算方式完全不同，因为mAP需要对每个用户的样本进行分用户排序，而P-R曲线和ROC曲线均是对全量测试样本进行排序。\n\n# \n\n# 实例\n\n下面以一个经典的莺尾花分类的例子来展示各种指标的计算。\n导入莺尾花数据，使用Holdout检验，将数据集随机划分成训练集和测试集：\n\n\n\n```python\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Add noisy features\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# Limit to the two first classes, and split into training and test\nX_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n                                                    test_size=.5,\n                                                    random_state=random_state)\n```\n\n创建一个线性SVM分类器，计算测试数据到决策平面的距离以及对测试数据进行预测：\n\n\n\n```python\n# Create a simple classifier\nclassifier = svm.LinearSVC(random_state=random_state)\nclassifier.fit(X_train, y_train)\ny_score = classifier.decision_function(X_test)\ny_predict = classifier.predict(X_test)\n```\n\n计算准确率：\n\n\n\n```python\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_predict)\nprint(\"Accuracy: \", accuracy)\n```\n\n\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31z5xdv3pj208c01o0sj.jpg)\n\n计算精准率：\n\n\n\n```python\nfrom sklearn.metrics import precision_score\nprecision = precision_score(y_test, y_predict)\nprint(\"Precision: \", precision)\n```\n\n\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31z672ruxj208s01udfn.jpg)\n\n计算召回率：\n\n\n\n```python\nfrom sklearn.metrics import recall_score\nrecall = recall_score(y_test, y_predict)\nprint(\"Recall: \", recall)\n```\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31z6uhwbkj20d401ot8l.jpg)\n\n计算F1-Score：\n\n\n\n```python\nfrom sklearn.metrics import f1_score\nF1_score = f1_score(y_test, y_predict)\nprint(\"F1-score: \", F1_score)\n```\n\n\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31z7rbrxhj20em01smx1.jpg)\n\n计算精确率均值AP：\n\n\n\n```python\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_score)\nprint('Average precision: {0:0.2f}'.format(average_precision))\n```\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31z812ynpj20c401ojr8.jpg)\n\n计算混淆矩阵：\n\n\n\n```python\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_predict)\nprint(\"Confusion Matrix: \\n\", confusion_matrix)\n```\n\n\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31z8af21uj208m03ggli.jpg)\n\n绘制P-R曲线，并且计算AUC：\n\n\n\n```python\nfrom sklearn.metrics import precision_recall_curve, auc\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndisp = plot_precision_recall_curve(classifier, X_test, y_test)\ndisp.ax_.set_title('P-R Example')\n\nprecision, recall, _thresholds = precision_recall_curve(y_test, y_predict)\nauc = auc(recall, precision)\nprint(\"AUC: \", auc)\n```\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31zfd59juj20ue0ksq3p.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31z8tjgbcj20ge01yt8n.jpg)\n\n绘制ROC曲线并且计算AUC：\n\n\n\n```python\nfrom sklearn.metrics import roc_auc_score, auc, roc_curve\nimport matplotlib.pyplot as plt\n\nfpr, tpr, thresholds = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)  #auc为Roc曲线下的面积\n\n#开始画ROC曲线\nplt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.1])\nplt.ylim([-0.1,1.1])\nplt.xlabel('FPR') #横坐标是fpr\nplt.ylabel('TPR')  #纵坐标是tpr\nplt.title('ROC Example')\nplt.show()\n```\n\n<img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h31z99o8w6j20rc0jc0tf.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n# A/B测试与线上评估指标\n\n无论离线评估如何仿真线上环境，终究无法完全还原线上的所有变量。对几乎所有的互联网公司来说，线上A/B测试都是验证新模块、新功能、新产品是否有效的主要测试方法。\n\n## A/B测试\n\n\n\n\n\nA/B测试又称为“分流测试”或“分桶测试”，是一个随机实验，通常被分为实验组和对照组。在利用控制变量法保持单一变量的前提下，将A、B两组数据进行对比，得出实验结论。具体到互联网场景下的算法测试中，可以将用户随机分成实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以旧模型，比较实验组和对照组在各线上评估指标上的差异。可以由下图来展示：\n\n![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h31z9vompuj20gt085glv.jpg)\n\n上图中用户被随机均分成两组，橘色和绿色代表被控制的变量，最右侧是转化率。通过这种方式可以看到，系统中单个变量对系统产生的整体影响。\n相对离线评估而言，线上A/B测试无法被替代的原因主要有以下三点：\n\n- **离线评估无法完全消除数据有偏现象的影响，因此得出的离线评估结果无法完全替代线上评估结果。**\n- **离线评估无法完全还原线上的工程环境。**一般来说，离线评估往往不考虑线上的延迟、数据丢失、标签缺失等情况。因此，离线评估环境只能说是理想状态下的工程环境，得出的评估结果存在一定的失真现象。\n- **线上系统的某些商业指标在离线评估中无法计算**。离线评估一般针对模型本身进行评估，无法直接获得与模型相关的其他指标，特别是商业指标。也新的推荐模型为例，离线评估关注的往往是ROC曲线、PR曲线等的改进，而线上评估可以全面了解该推荐模型带来的用户点击率、留存时长、PV访问量等的变化。这些都需要由A/B测试进行全面评估。\n\n\n\n## 线上A/B测试的评估指标\n\n一般来讲，A/B测试都是模型上线前的最后一道测试，通过A/B测试检验的模型将直接服务于线上用户，完成公司的商业目标。因此，A/B测试的指标与线上业务的核心指标保持一致。\n下表列出了电商类推荐模型、新闻类推荐模型、视频类推荐模型的线上A/B测试的主要评估指标：\n\n| 推荐系统类别   | 线上A/B测试评估指标                                          |\n| -------------- | ------------------------------------------------------------ |\n| 电商类推荐模型 | 点击率、转化率、客单价（用户平均消费金额）                   |\n| 新闻类推荐模型 | 留存率（x日后仍活跃的用户数/x日前的用户数）、平均停留时长、平均点击个数 |\n| 视频类推荐模型 | 播放完成率（播放时长/视频时长）、平均播放时长、播放总时长    |\n\n线上A/B测试的指标与离线评估指标有较大差异。离线评估不具备直接计算业务核心指标的条件，因此退而求其次，选择了偏向于技术评估的模型相关指标。但在公司层面，更关心能够驱动业务发展的核心指标。因此，在具备线上测试环境时，利用A/B测试验证模型对业务核心指标的提升效果是有必要的。从这个意义上讲，线上A/B测试的作用是离线评估无法替代的。\n\n\n\n# 参考\n\n- 《机器学习》 -- 周志华\n- 《推荐系统实战》-- 项亮\n- 《深度学习推荐系统》-- 王喆\n- https://www.jianshu.com/p/5df19746daf9\n- [https://www.zhihu.com/question/39840928](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F39840928)\n- [https://baike.baidu.com/item/AUC/19282953?fr=aladdin](https://links.jianshu.com/go?to=https%3A%2F%2Fbaike.baidu.com%2Fitem%2FAUC%2F19282953%3Ffr%3Daladdin)\n- [https://en.wikipedia.org/wiki/Precision_and_recall](https://links.jianshu.com/go?to=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FPrecision_and_recall)\n- [https://blog.csdn.net/bqw18744018044/article/details/81024520](https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fbqw18744018044%2Farticle%2Fdetails%2F81024520)\n- [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection](https://links.jianshu.com/go?to=https%3A%2F%2Fscikit-learn.org%2Fstable%2Fmodules%2Fclasses.html%23module-sklearn.model_selection)\n- [https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html](https://links.jianshu.com/go?to=https%3A%2F%2Fscikit-learn.org%2Fstable%2Fauto_examples%2Fmodel_selection%2Fplot_precision_recall.html)\n\n\n\n","tags":["评价指标"],"categories":["推荐系统"]}]