<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    <meta charset="utf8" />
    <meta name="viewport" content="initial-scale=1.0, width=device-width" />
    <title>
      
        自然语言处理BERT模型 | 思想放牧之地
      
    </title>
    <meta name="description" content="有情有颜 有才缺钱" />
    <meta name="keywords" content="" />
    
      <link rel="apple-touch-icon"
            sizes="180x180"
            href="/images/apple-touch-icon.png" />
    
    
      <link rel="icon"
            type="image/png"
            sizes="32x32"
            href="/images/favicon-32x32.png" />
    
    
      <link rel="icon"
            type="image/png"
            sizes="16x16"
            href="/images/favicon-16x16.png" />
    
    
      <link rel="mask-icon"
            href="/images/logo.svg"
            color="" />
    
    
    
      
  <style>
    @font-face {
        font-family:sourceHanSerif;
        src: url(/font/normal.ttf);
        font-weight: normal;
    }
  </style>

  <style>
    @font-face {
        font-family:sourceHanSerif;
        src: url(/font/bold.ttf);
        font-weight: bold;
    }
  </style>


    
    <link rel="stylesheet"
          type="text/css"
          href='/css/layout.css' />
    
    
  <link rel="stylesheet" type="text/css" href="/css/post.css"/>
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"/>

  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    
      <div id="search-mask" style="display:none">
  <div class="search-main" id="search-main">
    <div class="search__head">
      <div class="search-form">
        <svg t="1706347533072"
             class="icon"
             viewBox="0 0 1024 1024"
             version="1.1"
             xmlns="http://www.w3.org/2000/svg"
             p-id="7828"
             width="20"
             height="20">
          <path d="M685.6 660.336l155.152 155.168a16 16 0 0 1 0 22.624l-11.312 11.328a16 16 0 0 1-22.624 0l-158.528-158.544a289.792 289.792 0 0 1-165.152 51.36C322.336 742.256 192 611.904 192 451.12 192 290.336 322.336 160 483.136 160c160.784 0 291.12 130.336 291.12 291.136 0 82.112-33.984 156.272-88.672 209.2z m-202.464 33.92c134.272 0 243.12-108.848 243.12-243.12C726.256 316.848 617.408 208 483.136 208 348.848 208 240 316.848 240 451.136c0 134.272 108.848 243.12 243.136 243.12z" fill="#000000" p-id="7829">
          </path>
        </svg>
        <input id="search-input" placeholder="搜索文章">
        <svg t="1706361500528"
             id="search-clear"
             class="icon"
             viewBox="0 0 1024 1024"
             version="1.1"
             xmlns="http://www.w3.org/2000/svg"
             p-id="4351"
             width="20"
             height="20">
          <path d="M512 562.688l-264.2944 264.2944-50.688-50.688L461.312 512 197.0176 247.7056l50.688-50.688L512 461.312l264.2944-264.2944 50.688 50.688L562.688 512l264.2944 264.2944-50.688 50.688L512 562.688z" fill="#00" p-id="4352">
          </path>
        </svg>
      </div>
    </div>
    <div class="search__body" id="search-result"></div>
    <div class="search__foot"></div>
  </div>
</div>

    
    
    <div class=head--sticky>
      <div class="nav">
        <a href='/' class="nav-logo">
          <img alt="logo" height="60px" width="60px" src="/images/logo.svg" />
        </a>
        <input id="navBtn" type="checkbox" />
        <div class="nav-right">
          
            <div class="search-outer">
  <div class="search" id="search-btn">
    <svg t="1706347533072"
         class="icon"
         viewBox="0 0 1024 1024"
         version="1.1"
         xmlns="http://www.w3.org/2000/svg"
         p-id="7828"
         width="20"
         height="20">
      <path d="M685.6 660.336l155.152 155.168a16 16 0 0 1 0 22.624l-11.312 11.328a16 16 0 0 1-22.624 0l-158.528-158.544a289.792 289.792 0 0 1-165.152 51.36C322.336 742.256 192 611.904 192 451.12 192 290.336 322.336 160 483.136 160c160.784 0 291.12 130.336 291.12 291.136 0 82.112-33.984 156.272-88.672 209.2z m-202.464 33.92c134.272 0 243.12-108.848 243.12-243.12C726.256 316.848 617.408 208 483.136 208 348.848 208 240 316.848 240 451.136c0 134.272 108.848 243.12 243.136 243.12z" fill="#000000" p-id="7829">
      </path>
    </svg>
    <span>搜索</span>
    <span class="search-shortcut-key">Ctrl K</span>
  </div>
</div>

          
          <div class="nav-menu">
            
              
                <a class="nav-menu-item" href="/AI">AI大模型</a>
              
                <a class="nav-menu-item" href="/photograph">拍摄摄影</a>
              
                <a class="nav-menu-item" href="/live">生活随记</a>
              
                <a class="nav-menu-item" target="_blank" rel="noopener" href="https://bbs.hanlp.com/">社区维护</a>
              
            
            <a class="nav-menu-item" href='/cv/'>简历</a>
          </div>
        </div>
        <label class="nav-btn" for="navBtn"></label>
      </div>
    </div>
    <div class="body">
      
  <article class="post-content">
    <div class="post-inner--toc">
      <div class="post-content__head">
        <div class="post-title">自然语言处理BERT模型</div>
        <div class="post-info">
          
  <a href="/tags/BERT/" class="post-tag">#BERT</a><a href="/tags/NLP/" class="post-tag">#NLP</a>


          <span class="post-date">2022-07-19</span>
        </div>
      </div>
      
        <aside class="toc-outer">
          <div class="toc-title">目录</div>
          <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86bert%E6%A8%A1%E5%9E%8B"><span class="post-toc-text">自然语言处理BERT模型</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E8%AF%BE%E7%A8%8B%E5%AE%89%E6%8E%92"><span class="post-toc-text">课程安排</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#bert"><span class="post-toc-text">BERT</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#transformer"><span class="post-toc-text">Transformer</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="post-toc-text">整体架构</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#attention%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D%E5%91%A2"><span class="post-toc-text">Attention是什么意思呢？</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#self-attention%E6%98%AF%E4%BB%80%E4%B9%88"><span class="post-toc-text">Self-attention是什么？</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#self-attention%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97"><span class="post-toc-text">self-attention如何计算？</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E6%AF%8F%E4%B8%AA%E8%AF%8D%E7%9A%84attention%E8%AE%A1%E7%AE%97"><span class="post-toc-text">每个词的Attention计算</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#attention%E6%95%B4%E4%BD%93%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B"><span class="post-toc-text">Attention整体计算流程</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#multi-headed%E6%9C%BA%E5%88%B6"><span class="post-toc-text">Multi-headed机制</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#multi-headed%E7%BB%93%E6%9E%9C"><span class="post-toc-text">Multi-headed结果</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%A0%86%E5%8F%A0%E5%A4%9A%E5%B1%82"><span class="post-toc-text">堆叠多层</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E8%A1%A8%E8%BE%BE"><span class="post-toc-text">位置信息表达</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E5%91%A2"><span class="post-toc-text">什么是残差连接呢？</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#norm%E6%93%8D%E4%BD%9C"><span class="post-toc-text">Norm操作</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#decoder"><span class="post-toc-text">Decoder</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E6%9C%80%E7%BB%88%E7%9A%84%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C"><span class="post-toc-text">最终的输出结果</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E6%95%B4%E4%BD%93%E6%A2%B3%E7%90%86%E4%B8%80%E4%B8%8B"><span class="post-toc-text">整体梳理一下</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E6%BA%90%E7%A0%81"><span class="post-toc-text">源码</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#pre-trained-models"><span class="post-toc-text">Pre-trained models</span></a></li></ol></li></ol>
          <a href="#" class="toc-top">回到顶部</a>
        </aside>
      
      <div class="post-content__body">
        
          <div class="post-gallery">
            
          </div>
        
        <h1 id="自然语言处理bert模型">自然语言处理BERT模型</h1>
<h2 id="课程安排">课程安排</h2>
<ul>
<li>通俗讲解知识点，项目实战驱动</li>
<li>当下主流解决框架，一站式搞定NLP任务</li>
<li>环境配置：选一款IDE即可，基于谷歌开源项目</li>
<li>提供所有数据与代码，追随热点持续更新</li>
</ul>
<h2 id="bert">BERT</h2>
<ul>
<li>需要熟悉word2vec，RNN网络模型，了解词向量如何建模</li>
<li>重要在于Transformer网络架构，BERT训练方法，实际应用</li>
<li>开源项目，都是现成的，套用进去就ok</li>
<li>提供预训练模型，基本任务拿过来直接用都成</li>
</ul>
<h2 id="transformer">Transformer</h2>
<p>要做一件什么事呢？</p>
<ul>
<li>基本组成依旧是机器翻译模型中常见的Seq2Seq网络</li>
<li>输入输出都很直观，其核心架构就是中间的网络设计了</li>
</ul>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h884nz6dxkj30ng05yaa6.jpg" /></p>
<p>对比传统的RNN网络：</p>
<p>计算时有什么问题？</p>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h884pbn9wgj315d0dimy5.jpg" /></p>
<p>是依次串行处理的，现在处理的需要依赖上一步处理后的数据。</p>
<p>而Self-Attention机制来进行并行计算（得益于QKV的矩阵），在输入和输出都相同，输出结果是同时被计算出来的，现在基本已经取代RNN了。</p>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h884sbjayej30xm0l40tv.jpg" alt="image-20221117145723293" style="zoom:50%;" /></p>
<p>传统的word2vec</p>
<ul>
<li>表示向量时有什么问题？</li>
</ul>
<p>训练好的词向量是固定的，但是应用到不同的情景中原有的词含义并不合适</p>
<ul>
<li>如果‘干哈那’是一个词？</li>
</ul>
<p>不同语义中不同的含义</p>
<ul>
<li>不同语境中相同的词该如何表达</li>
<li>预训练好的向量就永久不变了</li>
</ul>
<h2 id="整体架构">整体架构</h2>
<ul>
<li>输入如何编码？</li>
<li>输出结果是什么？</li>
<li>Attention的目的？</li>
<li>怎样组合在一起？</li>
</ul>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8859gwec0j315z0n1djr.jpg" /></p>
<h3 id="attention是什么意思呢">Attention是什么意思呢？</h3>
<p>对于输入的数据，所关注的点是什么？不同的句子有不同的关注点。</p>
<p>那么如何才能让计算机关注到这些有用的信息呢？</p>
<h3 id="self-attention是什么">Self-attention是什么？</h3>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h885yjha1cj30n103jaaj.jpg" /></p>
<p>在上面的句子中，不同的情景下it所代指的是不一样的。</p>
<p>self-attention实质上就是当前词自己self，融合了该词所在句子中其他单词的含义，只不过对不同的词添加了不同的权重。</p>
<blockquote>
<p>所谓的各种模型，实际上都是在进行加权平均</p>
</blockquote>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h885ued8naj30fj0endge.jpg" /></p>
<h3 id="self-attention如何计算">self-attention如何计算？</h3>
<p>考虑如何获得一个句子中不同的单词对该词自身selt的影响，考虑添加一个query、key、value矩阵来，在开始的时候初始化，不断的训练中会改变，其实质上就是一个权重参数矩阵。</p>
<ol type="1">
<li>输入经过编码后得到向量</li>
<li>想得到当前词语上下文的关系，可以当作是是加权</li>
<li>构建三个矩阵分别来查询当前词跟其他词的关系，以及特征向量的表达。</li>
</ol>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8862zxd7yj31320ncdgw.jpg" alt="image-20221117154213948" style="zoom:50%;" /></p>
<p>三个需要训练的矩阵：</p>
<p>Q：query ，要去查询的</p>
<p>K：key，等着被查询的</p>
<p>V：value，实际的特征信息</p>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dwrlsu6hj30kb0mj74y.jpg" alt="image-20221122145315126" style="zoom: 50%;" /></p>
<p>首先：</p>
<p>q与k的内积表示有多匹配，向量垂直，内积为0、相近的话内积接近于1。</p>
<p>输入了两个向量，得到一个分值。q、k。得到的是两个矩阵查询出来的结果。</p>
<p>其中k代表了等着被查的，v代表了q*k后的实际的特征信息。</p>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dwwx0j59j30ss0ggq3m.jpg" alt="image-20221122145824269" style="zoom:50%;" /></p>
<p>在Q *
K转制后也就是得到了一个实际的内积值，因为多个计算结果不同的值之间方差比较大，所以我们去除以了根号下k的维度，然后做了一个softmax，结果可能是：</p>
<p>【0.2、0.12、0.34、0.37 。。】</p>
<p>这实际上对应的是当前词和和不同词之间的相关度（这里使用概率值来表示）然后和V矩阵相乘后得到z矩阵，也就是当前词根据上下文信息后得到的综合向量embedding。</p>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dx3rw1q2j30ob0c274i.jpg" alt="image-20221122150458880" style="zoom:33%;" /></p>
<p>最终的得分值经过softmax就是最终上下文结果</p>
<p>除以根号的原因在于，不能让其分值随着向量维度的增大而增加，所以这里做了一个缩小除法。</p>
<p>softmax公式回忆：</p>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dx66pirbj30s609z0tp.jpg" alt="image-20221122150719368" style="zoom:33%;" /></p>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dx76lquij308f02o0sm.jpg" /></p>
<p>把一些输入映射为0-1之间的实数，并且归一化保证和为1。</p>
<h3 id="每个词的attention计算">每个词的Attention计算</h3>
<ul>
<li>每个词的Q会跟整个序列中每一个K计算得分，然后基于得分再分配特征。<img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dxb16vpvj30mi09imxm.jpg" /></li>
</ul>
<h3 id="attention整体计算流程">Attention整体计算流程</h3>
<ul>
<li>每个词的Q会跟每一个K计算得分</li>
<li>Softmax后（和V矩阵相乘后）就得到整个加权结果</li>
<li>此时每个词看的不只是它前面的序列而是整个输入序列</li>
<li>会在同一时间计算出所有词的表示结果</li>
</ul>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dxf3ytzxj30pj0pzt9t.jpg" alt="image-20221122151553690" style="zoom:50%;" /></p>
<p>首先是输入文本，然后转换成embedding，然后新建Q、K、V三个矩阵并随机初始化，然后q1*k1、q2*k2等等得到一个相关内积，然后除以k矩阵的维度，再经过softmax后得到概率值，然后和v矩阵相乘，就得到了如：</p>
<p>Z1 = 0.88v1 + 0.12 v2 这样的表示方式。</p>
<h3 id="multi-headed机制">Multi-headed机制</h3>
<ul>
<li>一组q、k、v得到了一组当前词的特征表达。</li>
<li>我们设想类似cnn中的filter，能不能提取多种特征呢？</li>
<li>多头注意力机制类似于我们从不同的视角去分析问题，对于selt-attention的信息抽取同样是包含这种思想在里面的。</li>
</ul>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dy5np243j30j10cc74m.jpg" alt="image-20221122154124438" style="zoom: 67%;" /></p>
<p>实质上就是借鉴了：</p>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dyh4hhm5j30we07y3zv.jpg" alt="image-20221122155225292" style="zoom: 33%;" /></p>
<p>通过不同的head得到了多个特征表示
，然后将所有特征拼接在一起，然后再进经过一层全链接来降维：</p>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dyijc607j30u00wb0tz.jpg" alt="image-20221122155346655" style="zoom: 33%;" /></p>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dyiwz6mmj31yi0oyjuw.jpg" /></p>
<h3 id="multi-headed结果">Multi-headed结果</h3>
<blockquote>
<p>不同注意力的结果，得到的特征向量表达也不相同。</p>
</blockquote>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dyjyx3pqj30yu0u0afl.jpg" alt="image-20221122155509768" style="zoom: 50%;" /></p>
<h3 id="堆叠多层">堆叠多层</h3>
<p>一层不够，需要堆叠多层。计算方法都是相同的。</p>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dym8v26wj30wr0k5gmz.jpg" alt="image-20221122155721316" style="zoom: 50%;" /></p>
<h3 id="位置信息表达">位置信息表达</h3>
<p>在上文描述的self-attention中每个词都会考虑整个序列的加权，所以其出现位置对结果并不会产生什么影响，相当于放在哪里都是无所谓的，但是这就跟实际有些不相符了，所以希望模型对位置信息有一些额外的认识，于是引入了位置变量。</p>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8dyqf7xxoj30n0088mxf.jpg" /></p>
<p>在Encoder层和Decoder层中都用到了Add&amp;Norm操作，即残差连接和层归一化操作。</p>
<h3 id="什么是残差连接呢">什么是残差连接呢？</h3>
<blockquote>
<p>残差连接就是把网络的输入和输出相加，即网络的输出为F(x) + x
，在网络结构比较深的时候，网络梯度反向传播更新参数时，容易造成梯度消失的问题，但是如果每层的输出都加上一个x的时候，就会变成F（x）+x，对x求导结果为1，所以就相当于每一层求导时都加上了一个常数项1，有效的解决了梯度消失的问题。</p>
<p>加入未学习的原向量使得到的结果的效果至少不弱于原来的结果</p>
</blockquote>
<h3 id="norm操作">Norm操作</h3>
<p>Norm操作
首先要明白Norm做了一件什么事，从刚开始接触Transformer开始，我认为所谓的Norm就是BatchNorm，但是有一天我看到了这篇文章，才明白了Norm是什么。</p>
<p>假设我们输入的词向量的形状是（2，3，4），2为批次（batch），3为句子长度，4为词向量的维度，生成以下数据：</p>
<p>[[w11, w12, w13, w14], [w21, w22, w23, w24], [w31, w32, w33, w34]
[w41, w42, w43, w44], [w51, w52, w53, w54], [w61, w62, w63, w64]] 1 2
如果是在做BatchNorm（BN）的话，其计算过程如下：BN1=(w11+w12+w13+w14+w41+
w42+w43+w44)/8，同理会得到BN2和BN3，最终得到[BN1,BN2,BN3] 3个mean</p>
<p>如果是在做LayerNorm（LN）的话，则会进如下计算：LN1=(w11+w12+w13+w14+w21+
w22+w23+w24+w31+w32+w33+w34)/12，同理会得到LN2，最终得到[LN1,LN2]两个mean</p>
<p>如果是在做InstanceNorm（IN）的话，则会进如下计算：IN1=(w11+w12+w13+w14)/4，同理会得到IN2，IN3，IN4，IN5，IN6，六个mean，[[IN1，IN2，IN3],[IN4，IN5，IN6]]
下图完美的揭示了，这几种Norm</p>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8e24bu8drj311t0ahacj.jpg" /></p>
<ol type="1">
<li>通过解决ICS（<a
target="_blank" rel="noopener" href="http://www.baidu.com/link?url=NzFNnRmBgJpmycgNhEUB2zE2KdI-YD7F5gT7Q-8mKmZvRU9ojcu3ioiHRRRUovkT">内部协变量偏移</a>）的问题，使得每一层神经网络的输入分布稳定，在这个基础上可以使用较大的学习率，加速了模型的训练速度</li>
<li>起到一定的正则作用，进而减少了dropout的使用。当我们通过BN规整数据的分布以后，就可以尽量避免一些极端值造成的overfitting的问题</li>
<li>使得数据不落入饱和性激活函数（如sigmoid，tanh等）饱和区间，避免梯度消失的问题。</li>
</ol>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8e2k7r6lsj31q50u0448.jpg" /></p>
<hr />
<h3 id="decoder">Decoder</h3>
<p>Attention计算不同</p>
<p>并且加入了MASK机制</p>
<h3 id="最终的输出结果">最终的输出结果</h3>
<p>得出最终预测结果</p>
<p>损失函数使用cross-entropy即可</p>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8e2z0jgvtj30hb0c3dg7.jpg" alt="image-20221122182800177" style="zoom: 67%;" /></p>
<h3 id="整体梳理一下">整体梳理一下</h3>
<p><img src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8ewkskjmcj30ge0oqq4t.jpg" alt="img" style="zoom:67%;" /></p>
<p>首先输入Embedding和positional
Encoding，然后输入到Self-attention，并且使用multi-head多层堆叠，位置编码，然后使用Add
&amp; Norm 残差连接和归一化，并且使用多个transformer块进行训练。</p>
<p>在decoder方面，为了使得中间隐层向量的解码带有注意力，而不是一视同仁地统一解码，所以需要某种计算机制来生成一种记录训练过程中语义倾向的值。</p>
<p>encoder-decoder attention 用到的策略的名称叫做encoder-decoder
attention，
用原博中的两个gif就能解释，首先通过最末尾的一个encoder保留KV，初始化Q得到decoder的第一个输出</p>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8ewkmb050j30n30d23z9.jpg" /></p>
<p>图示中的I就是decoder的第一个输出，
接下来这个输出当做下一个decoder的Q，然后再获取之前的encoder的KV，继续做self
attention 如下图所示：</p>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8evrbaucnj30oz0d3wfh.jpg" /></p>
<p>这样一来就能够通过transformer得到对应的一个不带位置编码的输出
一句话总结就是： <strong>encoder-decoder
attention中，Q来自于decoder的上一个输出，KV来自于encoder的输出</strong>。</p>
<h2 id="源码">源码</h2>
<h2 id="pre-trained-models">Pre-trained models</h2>
<p>预训练的模型包含如下：</p>
<p><img
src="https://image.baidu.com/search/down?url=https://image.baidu.com/search/down?url=https://tva1.sinaimg.cn/large/008vxvgGly1h8f1va35f6j310h07aabq.jpg" /></p>
<p>README中介绍.zip包含3大块：</p>
<ul>
<li>A TensorFlow checkpoint (<code>bert_model.ckpt</code>) containing
the pre-trained weights (which is actually 3 files).
<strong>权重参数和偏置</strong></li>
<li>A vocab file (<code>vocab.txt</code>) to map WordPiece to word id.
<strong>词表</strong></li>
<li>A config file (<code>bert_config.json</code>) which specifies the
hyperparameters of the model.<strong>超参数</strong></li>
</ul>
<table>
<thead>
<tr>
<th>这种表格</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

      </div>
    </div>
    
      <script src='https://unpkg.com/mermaid@latest/dist/mermaid.min.js'></script>
      <script>
        if (window.mermaid) {
          mermaid.initialize({"startOnload":true});
        }
      </script>
    
  </article>
  <div class="post__foot">
    
      <div class="like-author">
  <input type="checkbox" id="likeCode" />
  <div class="author-face">
    <img height="100px"
         width="100px"
         id="front-face"
         alt="author face"
         src="/images/author-face.jpg" />
    <img height="100px"
         width="100px"
         id="back-face"
         alt="like code"
         src="/images/pay-code.jpg" />
  </div>
  <div class="like-text">“给作者倒杯卡布奇诺”</div>
  <label for="likeCode" class="like-btn">
    <svg viewBox="0 0 1024 1024"
         width="20px"
         style="margin-right: 10px"
         height="20px">
      <path d="M466.88 908.96L113.824 563.296a270.08 270.08 0 0 1 0-387.392c108.8-106.56 284.896-106.56 393.696 0 1.504 1.472 2.976 2.944 4.448 4.48 1.472-1.536 2.944-3.008 4.448-4.48 108.8-106.56 284.896-106.56 393.696 0a269.952 269.952 0 0 1 34.016 347.072l-387.392 385.6a64 64 0 0 1-89.92 0.384z" p-id="13650" fill="#ee4242" />
    </svg>
    喜欢作者
  </label>
</div>

    
    <div class="post-nav">
  
    <a class="post-nav-item-left" href="/2022/09/26/HanLP%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%EF%BC%9A%E5%95%86%E5%93%81%E5%9B%BE%E8%87%AA%E5%8A%A8%E6%8E%A8%E8%8D%90%E5%8A%9F%E8%83%BD%E7%9A%84%E5%BA%94%E7%94%A8/">
      <div class="text-align">
        <svg t="1670570876164"
             class="icon"
             viewBox="0 0 1024 1024"
             width="16"
             height="16">
          <path d="M384 512L731.733333 202.666667c17.066667-14.933333 19.2-42.666667 4.266667-59.733334-14.933333-17.066667-42.666667-19.2-59.733333-4.266666l-384 341.333333c-10.666667 8.533333-14.933333 19.2-14.933334 32s4.266667 23.466667 14.933334 32l384 341.333333c8.533333 6.4 19.2 10.666667 27.733333 10.666667 12.8 0 23.466667-4.266667 32-14.933333 14.933333-17.066667 14.933333-44.8-4.266667-59.733334L384 512z" p-id="14596" />
        </svg>
        <span class="text-small">上一篇</span>
      </div>
      <div>HanLP分词工具应用案例：商品图自动推荐功能的应用</div>
    </a>
  
  <div class="vhr"></div>
  
    <a class="post-nav-item-right" href="/2022/06/13/%E5%8D%9A%E5%A3%AB%E8%BF%99%E4%BA%94%E5%B9%B4-%E6%9D%8E%E6%B2%90/">
      <div class="text-align">
        <span class="text-small">下一篇</span>
        <svg t="1670570876164"
             class="icon"
             viewBox="0 0 1024 1024"
             transform="scale(-1,-1)"
             width="16"
             height="16">
          <path d="M384 512L731.733333 202.666667c17.066667-14.933333 19.2-42.666667 4.266667-59.733334-14.933333-17.066667-42.666667-19.2-59.733333-4.266666l-384 341.333333c-10.666667 8.533333-14.933333 19.2-14.933334 32s4.266667 23.466667 14.933334 32l384 341.333333c8.533333 6.4 19.2 10.666667 27.733333 10.666667 12.8 0 23.466667-4.266667 32-14.933333 14.933333-17.066667 14.933333-44.8-4.266667-59.733334L384 512z" p-id="14596" />
        </svg>
      </div>
      博士这五年-李沐
    </a>
  
</div>

    
    
      <div id="gitalk-container"></div>
    
  </div>

    </div>
    <div class="foot">
  <div class="foot-inner">
    <div class="foot__head">
      
        <div class="foot-line">
          <div class="matts">今</div><div class="matts">夜</div><div class="matts">无</div><div class="matts">眠</div>
        </div>
      
        <div class="foot-line">
          <div class="matts">睡</div><div class="matts">意</div><div class="matts">全</div><div class="matts">无</div>
        </div>
      
    </div>
    <div class="foot__body">
      
        <div class="foot-item">
          <div class="foot-item__head">朋友</div>
          <div class="foot-item__body">
            
            


  
  
    <div class="foot-link-group">
      
        
        
          <div class="text">
            <img alt="link"
                 height="20px"
                 width="20px"
                 src="/images/icon/icon-link.svg" />
            <a class="foot-link" target="_blank" rel="noopener" href="https://moyanxinxu.github.io/unlock-hf/">unlock-hf</a>
          </div>
        
      
        
        
          <div class="text">
            <img alt="link"
                 height="20px"
                 width="20px"
                 src="/images/icon/icon-link+.svg" />
            <a class="foot-link" href="mailto:873101411@qq.com?subject=申请http://example.com的友链">申请友链</a>
          </div>
        
      
        
        
      
        
        
      
    </div>
  


          </div>
        </div>
      
      
        <div class="foot-item">
          <div class="foot-item__head">账号</div>
          <div class="foot-item__body">
            


  
  
    <div class="foot-link-group">
      
        
        
          <div class="text">
            <img alt="link"
                 height="20px"
                 width="20px"
                 src="/images/logo-github.svg" />
            <a class="foot-link" target="_blank" rel="noopener" href="https://github.com/viserion-nlper">viserion-nlper</a>
          </div>
        
      
        
        
          <div class="text">
            <img alt="link"
                 height="20px"
                 width="20px"
                 src="/images/logo-wx.svg" />
            <a class="foot-link" target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Fq9ZIUwZvMNx2kSahz8zEw">HanLP.com</a>
          </div>
        
      
        
        
          <div class="text">
            <img alt="link"
                 height="20px"
                 width="20px"
                 src="/images/logo-zh.svg" />
            <a class="foot-link" target="_blank" rel="noopener" href="https://www.zhihu.com/people/shuo-hao-jin-ye-bu-dian-yan">点烟侠</a>
          </div>
        
      
        
        
      
    </div>
  


          </div>
        </div>
      
      <div class="foot-item">
        <div class="foot-item__head">联系</div>
        <div class="foot-item__body">
          


  
  
    <div class="foot-link-group">
      
        
        
          <div class="text">
            <img alt="link"
                 height="20px"
                 width="20px"
                 src="/images/icon/icon-email.svg" />
            <a class="foot-link" href="mailto:873101411@qq.com">873101411@qq.com</a>
          </div>
        
      
        
        
      
        
        
      
        
        
      
    </div>
  


        </div>
      </div>
    </div>
    <div class="copyright">
      <a href="http://example.com">思想放牧之地</a> &nbsp;|&nbsp;由&nbsp;<a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>&nbsp;及&nbsp;
      <svg width="20" height="20" viewBox="0 0 725 725">
        <path fill-rule="evenodd" fill="rgb(221, 221, 221)" d="M145.870,236.632 L396.955,103.578 L431.292,419.44 L156.600,522.53 L145.870,236.632 Z" />
        <path fill-rule="evenodd" fill="rgb(159, 159, 159)" d="M396.955,103.578 L564.345,234.486 L611.558,513.469 L431.292,419.44 L396.955,103.578 Z" />
        <path fill-rule="evenodd" fill="rgb(0, 0, 0)" d="M431.292,419.44 L611.558,513.469 L358.327,595.18 L156.600,522.53 L431.292,419.44 Z" />
      </svg>
      <a target="_blank" rel="noopener" href="https://github.com/hooozen/hexo-theme-tranquility">致远</a>&nbsp;驱动
    </div>
  </div>
</div>

    
    
      <script src="/js/search.js"></script>
      <script>searchInitialize("/search.json")</script>
    
    <script src="/js/copy-code.js"></script>
    
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script type="text/javascript">
  const param = JSON.parse('{"enable":true,"owner":"Viserion-nlper","admin":"Viserion-nlper","repo":"Aurora-gitalk","clientID":"383d8abfdbdb7adac4cc","clientSecret":"753a2f01ea61c8eadec056ab8a209cacb6a0a4e9","distractionFreeMode":false,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN"}')
  let title = location.pathname.substr(0, 50); 
  param.id = title
  const gitalk = new Gitalk(param)
  gitalk.render('gitalk-container')
</script>

  

  </body>
</html>
